{
    "name": "MetaMathQA",
    "size": "400,000 samples, 200 MB.",
    "download": "https://huggingface.co/datasets/meta-math/MetaMathQA",
    "companion": "https://arxiv.org/abs/2309.12284",
    "notes": "OpenMathInstruct is a more recent iteration of the same idea.",
    "summary": "Datasets collected by 'bootstrapping' gsm8k and Math"
}

Here's the completed dataset card for **MetaMathQA**:

- **name**: MetaMathQA

- **summary**: MetaMathQA is a dataset comprising approximately 395,000 mathematical problems, each paired with detailed solutions. The dataset was constructed by augmenting the training sets of GSM8K and MATH through a process called "question bootstrapping," which involves generating new questions by rephrasing existing ones and creating forward and backward reasoning paths. This approach aims to enhance the diversity and quality of mathematical problems available for training large language models (LLMs). citeturn0search1

- **size**: Approximately 395,000 samples, totaling around 200 MB. citeturn0search0

- **download**: The dataset is available for download at [https://huggingface.co/datasets/meta-math/MetaMathQA](https://huggingface.co/datasets/meta-math/MetaMathQA). citeturn0search0

- **companion**: The dataset is introduced in the paper "MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models," accessible at [https://arxiv.org/abs/2309.12284](https://arxiv.org/abs/2309.12284). citeturn0search1

- **notes**: MetaMathQA was developed to improve the mathematical reasoning capabilities of LLMs by providing a diverse set of problems with varying reasoning paths. The dataset has been utilized to fine-tune models such as MetaMath-7B, which demonstrated significant performance improvements on benchmarks like GSM8K and MATH. citeturn0search1

- **tags**: "synthetic", "QA", "compilation", "answer:verifiable", "question:synthetic", "answer:synthetic", "mathematics"

This card provides a comprehensive overview of the MetaMathQA dataset, highlighting its structure, content, and relevance to advancing mathematical problem-solving in large language models. 