aime.json
```json
{
    "name": "AIME",
    "size": "15 questions per year",
    "download": "Various formatted datasets based on historical AIME questions are available, e.g., [on HuggingFace](https://huggingface.co/datasets/di-zhang-fdu/AIME_1983_2024)",
    "notes": "Since AIME is an annual examination, new data becomes available each year. However, designers may select questions already present on the internet, which could be included in pretraining corpora.",
    "summary": "US high-school competition, American Invitational Mathematics Examination.",
    "tags": ["atomic", "benchmark", "QA", "question:human", "answer:human"]
}
```

algebraic_stack.json
```json
{
    "name": "Algebraic Stack",
    "size": "3,000,000, 11MB.",
    "download": "https://huggingface.co/datasets/EleutherAI/proof-pile-2",
    "companion": "https://arxiv.org/abs/2310.10631",
    "summary": "Subset of ProofPile-2. Obtained by filtering GitHub for Coq, Isabelle, Lean and Matlab, extracting data from Mathlib 4 (the Lean library), building a dataset of Isabelle proofs, and filtering the Stack."
}
```

amc.json
```json
{
    "name": "AMC",
    "size": "3 (levels) times 25 multi-choice questions per year.",
    "download": "Various formatted datasets based on historical AMC questions are available, e.g. https://huggingface.co/datasets/AI-MO/aimo-validation-amc/",
    "summary": "American Math competition for various high-school students, which acts as a pre-selection to AIME."
}
```

apps.json
```json
{
    "name": "APPS",
    "size": "10,000 problems",
    "download": "https://huggingface.co/datasets/codeparrot/apps",
    "companion": "https://arxiv.org/pdf/2105.09938",
    "notes": "AlphaCode mentions that test coverage was insufficient, leading to false positive.",
    "summary": "10,000 programming problems with python solutions and test cases for correctness.  Curated from Codewars, AtCoder, Kattis, and Codeforces."
}
```

aqua.json
```json
{
    "name": "AQuA (Algebra Question Answering)",
    "size": "98,000 samples; 52 MB",
    "download": "https://huggingface.co/datasets/deepmind/aqua_rat",
    "companion": "https://arxiv.org/pdf/1705.04146",
    "summary": "DeepMind Dataset built by extracting 34,000 questions from undergrad, and grad student admission test (GMAT and GRE), with answer and rational scrapped on the web. Plus crowdsourcing to provide similar questions."
}
```

arxiv.json
```json
{
    "name": "ArXiv",
    "size": "29B tokens",
    "download": "https://huggingface.co/datasets/EleutherAI/proof-pile-2 contains data scrapped from the ArXiv website, according to a snapshot taken in 2023 by RedPajama.",
    "notes": "The dataset is valuable for training models on advanced mathematical concepts and research-level problems.",
    "summary": ""
}
```

camel.json
```json
{
    "name": "Camel",
    "companion": "https://arxiv.org/abs/2303.17760",
    "notes": "Comes with no verification of correctness",
    "summary": "Exercise textbooks synthetically generated by GPT-4"
}
```

codecontests.json
```json
{
    "name": "CodeContests",
    "size": "13,000 samples, 2GB",
    "download": "https://huggingface.co/datasets/deepmind/code_contests",
    "companion": "https://arxiv.org/abs/2203.07814",
    "notes": "Includes correct and incorrect solutions. Rich data for training/debugging models.",
    "summary": "Dataset used for AlphaCode. Competitive programming problems from Aizu, AtCoder, CodeChef, Codeforces and HackerEarth."
}
```

codeforces.json
```json
{
    "name": "CodeForces",
    "size": "700,000 samples, 1.7 GB",
    "download": "https://huggingface.co/datasets/MatrixStudio/Codeforces-Python-Submissions",
    "notes": "Quite extensive datasets with unit test",
    "summary": "Website with competitive programming puzzles."
}
```

coqgym.json
```json
{
    "name": "CoqGym",
    "size": "70,000 proof steps",
    "download": "https://github.com/princeton-vl/CoqGym",
    "companion": "https://arxiv.org/abs/1905.09381",
    "summary": "Large-scale dataset compiled from various 71,000 Coq projects."
}
```

dclm.json
```json
{
    "name": "DataComp-LM (DCLM)",
    "size": "4T token",
    "download": "https://huggingface.co/datasets/mlfoundations/dclm-baseline-1.0",
    "companion": "https://arxiv.org/abs/2406.11794",
    "summary": "Filtering of Common Crawl based on heuristic cleaning and filtering (see RefinedWeb), deduplication (through Bloom), filtering with fastText classifier to match the reddit channel ExplainLikeImFive, and an instruct model."
}
```

deepscaler.json
```json
{
    "name": "DeepScaleR",
    "summary": "Compiled from Aime, AMC, Omni-Math and Still Compilation of data from AIME, AMC, Omni-Math, and Still. Used to reproduce R1."
}
```

deepseek_math.json
```json
{
    "name": "DeepSeek Math Corpus",
    "companion": "https://arxiv.org/pdf/2402.03300",
    "summary": "Filtering of Common Crawl based on a FastText classifier based on OpenWebMath as initial positive examples, and additional heuristics. Datasets not available."
}
```

deepseek_prover.json
```json
{
    "name": "DeepSeek-Prover-V1",
    "size": "27,000 samples, 6 MB",
    "download": "https://huggingface.co/datasets/deepseek-ai/DeepSeek-Prover-V1",
    "companion": "https://arxiv.org/abs/2405.14333",
    "summary": "Synthetic dataset of Lean proofs generated by DeepSeek, solving half of miniF2F."
}
```

eurus_rl.json
```json
{
    "name": "Eurus-RL",
    "size": "500,000 samples, 2 GB",
    "download": "https://huggingface.co/datasets/PRIME-RL/Eurus-2-RL-Data",
    "notes": "Includes multiple datasets with corresponding training recipes.",
    "summary": "Collection of question with verifiable answer extracted from Numina, Apps, CodeContests, Taco and Codeforces. https://arxiv.org/abs/2502.01456"
}
```

finemath.json
```json
{
    "name": "FineMath",
    "download": "https://huggingface.co/datasets/HuggingFaceTB/finemath",
    "companion": "https://arxiv.org/abs/2502.02737",
    "summary": "Filtered from CommonCrawl by HuggingFace for SmolLM focused on Math domain."
}
```

fineweb.json
```json
{
    "name": "FineWeb",
    "download": "https://huggingface.co/datasets/HuggingFaceFW/fineweb",
    "companion": "https://arxiv.org/abs/2406.17557",
    "summary": "Filtered from CommonCrawl by HuggingFace, seems to be slightly worse than DCLM."
}
```

fineweb_edu.json
```json
{
    "name": "FineWeb-Edu",
    "download": "https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu",
    "companion": "https://arxiv.org/abs/2406.17557",
    "summary": "Filtered from FineWeb to focus on sample with educational value"
}
```

glaiveai.json
```json
{
    "name": "GlaiveAI",
    "size": "20 million examples, 87 GB",
    "download": "https://huggingface.co/datasets/glaiveai/reasoning-v1-20m",
    "notes": "Comes with no verification of correctness",
    "summary": "Traces from DeepSeek R1"
}
```

gsm8k.json
```json
{
    "name": "GSM8k",
    "size": "8,500 problems, 5 MB",
    "download": "https://github.com/openai/grade-school-math",
    "companion": "https://arxiv.org/abs/2110.14168",
    "summary": "Grade School Math (GSM) benchmark, created by human annotator for OpenAI."
}
```

imo_steps.json
```json
{
    "name": "IMO-Steps",
    "size": "20 samples, 6 kB",
    "download": "https://huggingface.co/datasets/roozbeh-yz/IMO-Steps",
    "companion": "https://arxiv.org/abs/2411.18872",
    "summary": "DOWNLOADED IN `/checkpoint/amaia/explore/datasets/reasoning/raw` 20 Lean proofs of IMO problems"
}
```

isabelle_premise_selection.json
```json
{
    "name": "Isabelle Premise Selection",
    "size": "4,000,000 samples",
    "download": "https://huggingface.co/datasets/Simontwice/premise_selection_in_isabelle",
    "companion": "none, but the datasets was key to https://arxiv.org/abs/2303.04488",
    "summary": "Datasets of proofs in Isabelle collected from the Archive of Formal Proofs (https://www.isa-afp.org/). Useful to study premise selection (i.e. selecting potential lemmas to apply mid-proof)."
}
```

lean_workbook.json
```json
{
    "name": "Lean-Workbook",
    "size": "25,000 samples, 5MB",
    "download": "https://huggingface.co/datasets/internlm/Lean-Workbook",
    "companion": "https://arxiv.org/abs/2411.18872",
    "summary": "DOWNLOADED IN `/checkpoint/amaia/explore/datasets/reasoning/raw` Tens of thousands of math problems formalized in Lean4"
}
```

leetcode.json
```json
{
    "name": "LeetCode",
    "size": "2,000 samples, 7 MB (for the small version I found on HuggingFace)",
    "download": "https://huggingface.co/datasets/greengerong/leetcode",
    "notes": "Unclear how the dataset was collected, comes with no unit tests",
    "summary": "Website with programming puzzles, typically used to prepare coding interviews."
}
```

lila.json
```json
{
    "name": "Lila",
    "download": "https://huggingface.co/datasets/allenai/lila",
    "companion": "https://arxiv.org/abs/2210.17517",
    "notes": "The datasets were collected for evaluation, it seems small and outdated.",
    "summary": "Compilation of many datasets including addsub, amps, apps, asdiv, conala, mathematics, dolphin, draw, gsm8k, math, mathqa, mbpp, mctaco, multiarith, numersense, numglus, simuleq, singleop, singleq, svamp."
}
```

math.json
```json
{
    "name": "MATH",
    "size": "12,500 samples",
    "download": "https://github.com/hendrycks/math (was taken down from HuggingFace due to copyright issue filled by the art of problem solving).",
    "companion": "https://arxiv.org/abs/2103.03874",
    "summary": "Classical evaluation datasets, created by Dan Hendrycks, by collecting various US high-school competition problems with solution."
}
```

mathinstruct.json
```json
{
    "name": "MathInstruct",
    "size": "262,000 samples, 200 MB",
    "download": "https://huggingface.co/datasets/TIGER-Lab/MathInstruct",
    "companion": "https://arxiv.org/pdf/2309.05653",
    "summary": "A compilation of datasets (gsm8k, aqua,camel) with rationale (some of them being generated by LLMs) used to train MAmmoTH"
}
```

mathpile.json
```json
{
    "name": "MathPile",
    "size": "Approximately 9.5 billion tokens.",
    "download": "https://huggingface.co/datasets/GAIR/MathPile",
    "companion": "https://arxiv.org/abs/2312.17120",
    "summary": "Dataset collected by GAIR (the lab behind LIMO) by compiling textbook, arXiv, ProofWiki, and filtering common crawl."
}
```

mbpp.json
```json
{
    "name": "MBPP (Mostly Basic Python Problems)",
    "size": "1,000 samples",
    "download": "https://github.com/google-research/google-research/tree/master/mbpp",
    "companion": "https://arxiv.org/abs/2108.07732",
    "summary": "Crowd-sourced datasets of small Python problems A dataset consisting of 1,000 Python programming problems aimed at entry-level programmers."
}
```

metamathqa.json
```json
{
    "name": "MetaMathQA",
    "size": "400,000 samples, 200 MB.",
    "download": "https://huggingface.co/datasets/meta-math/MetaMathQA",
    "companion": "https://arxiv.org/abs/2309.12284",
    "notes": "OpenMathInstruct is a more recent iteration of the same idea.",
    "summary": "Datasets collected by 'bootstrapping' gsm8k and Math"
}
```

minif2f.json
```json
{
    "name": "MiniF2F",
    "size": "500 examples with Lean formal statements and informal statements and proofs.",
    "download": "https://github.com/facebookresearch/miniF2F",
    "companion": "https://arxiv.org/abs/2109.00110",
    "summary": "MiniF2F is a benchmark for formal mathematics, created by Kunhao, consisting of 500 Olympiad-level mathematics problems from competitions like AIME, AMC, and IMO. Each problem is provided with both informal and formal statements. # Compilation"
}
```

natural_reasoning.json
```json
{
    "name": "Natural Reasoning",
    "download": "https://huggingface.co/datasets/facebook/natural_reasoning",
    "companion": "https://arxiv.org/abs/2502.13124",
    "summary": "From Fair RAM group, questions filtered from DCLM and FineMath, with answers provided by Llama."
}
```

nemotron.json
```json
{
    "name": "Nemotron Post-Training Dataset v1",
    "size": "15,000,000 samples, 6 GB.",
    "download": "https://huggingface.co/datasets/nvidia/Llama-Nemotron-Post-Training-Dataset-v1",
    "companion": "https://arxiv.org/abs/2502.00203",
    "notes": "Developed to fine-tune models for improved accuracy and reliability.",
    "summary": "We do not have too many details on this datasets, it seems to have been curated from Llama, Qwen and DeepSeek answers."
}
```

numina_tool.json
```json
{
    "name": "Numina-Tool",
    "size": "70,000 samples, 150 MB",
    "download": "https://huggingface.co/datasets/AI-MO/NuminaMath-TIR",
    "companion": "https://arxiv.org/pdf/2309.17452",
    "summary": "Subset numina problems solved with tool-use."
}
```

numinamath.json
```json
{
    "name": "NuminaMath",
    "size": "Approximately 900,000 samples, 531 MB.",
    "download": "https://huggingface.co/datasets/AI-MO/NuminaMath-1.5",
    "summary": "Mix of math problems solved with rationale. Sources: aops_forum (https://artofproblemsolving.com/), amc_aime, Chinese k12, gsm8k, math, Olympiads, Orca-math (https://arxiv.org/abs/2402.14830), synthetic_amc, synthetic_math"
}
```

olympiadbench.json
```json
{
    "name": "OlympiadBench",
    "size": "8,000 questions.",
    "download": "https://huggingface.co/datasets/Hothan/OlympiadBench",
    "companion": "https://arxiv.org/pdf/2402.14008",
    "summary": "Datasets collected from Olympiads with figures (multi-modal), rationale (derived by humans), various level of difficulty."
}
```

omnimath.json
```json
{
    "name": "OmniMath",
    "size": "4,000, 7 MB.",
    "download": "https://huggingface.co/datasets/KbsdJames/Omni-MATH",
    "companion": "https://arxiv.org/abs/2410.07985",
    "summary": "Data obtained from regional to international Olympiads, from the art-of-problem solving."
}
```

openmathinstruct_v2.json
```json
{
    "name": "OpenMathInstruct-v2",
    "size": "22,000,000 samples, 12 GB",
    "download": "https://huggingface.co/datasets/nvidia/OpenMathInstruct-2",
    "companion": "https://arxiv.org/abs/2410.01560",
    "summary": "Augmentation of MATH and GSM8K from LLM (rephrase questions, provide answers)."
}
```

openr1.json
```json
{
    "name": "Open-R1-220k",
    "size": "225,000 samples, 8 GB.",
    "download": "https://huggingface.co/datasets/open-r1/OpenR1-Math-220k",
    "notes": "They have other datasets, see https://huggingface.co/open-r1",
    "summary": "This part of an ongoing effort by HuggingFace to reproduce DeepSeek R1. Consists of DeepSeek R1 traces answering problems from NuminaMath, verified with Math Verify, a HuggingFace parser to check numerical equality."
}
```

openwebmath.json
```json
{
    "name": "OpenWebMath",
    "size": "6,300,000 samples, 27 GB.",
    "download": "https://huggingface.co/datasets/open-web-math/open-web-math",
    "companion": "https://arxiv.org/abs/2310.06786",
    "summary": "Filtering of CommonCrawl done in 2023, that is considered good quality. Not donwloaded, as FineMath is more recent."
}
```

prm800k.json
```json
{
    "name": "PRM800K",
    "size": "800,000 samples,",
    "download": "https://github.com/openai/prm800k",
    "companion": "https://arxiv.org/abs/2305.20050",
    "notes": "Some annotations were reported to be incorrect",
    "summary": "Datasets created by OpenAI by using LLMs to answer question from the MATH datasets, with rationale graded by humnas."
}
```

proofpile_2.json
```json
{
    "name": "Proof-Pile-2",
    "size": "60B tokens,",
    "download": "https://huggingface.co/datasets/EleutherAI/proof-pile-2",
    "companion": "https://arxiv.org/abs/2310.10631",
    "summary": "Massive pretraining corpus of formal mathematics and related documents (Lean, Coq, math papers)."
}
```

stack_edu.json
```json
{
    "name": "Stack-Edu",
    "size": "167,000,000 samples, ~1 TB",
    "download": "https://huggingface.co/datasets/HuggingFaceTB/stack-edu",
    "companion": "https://arxiv.org/abs/2502.02737",
    "notes": "Need to be download with S3, still really big in terms of number of tokens.",
    "summary": "Filtering version of the Stack V2. # LLM Augmented Source Data being synthesized with LLMs. The LLM can be used to synthesize texts or questions (usually providing example of questions from existing datasets). It can also be used to synthesize a rationale to answer questions. The answer may be verified, either with parser checking for numerical equality (i.e. `\\frac13` = `0.333`), or using LLM as a judge. Synthesized questions (bootstrapped from existing one), synthesized, eventually verified, answers."
}
```

stack_v2.json
```json
{
    "name": "Stack V2",
    "size": "5,500,000,000 samples, 67 TB",
    "summary": "Scrapping of the Software Heritage archive, which contains software source code, in September 2023."
}
```

still.json
```json
{
    "name": "Still",
    "size": "30,000 samples, 10 MB",
    "download": "https://huggingface.co/datasets/RUC-AIBOX/STILL-3-Preview-RL-Data",
    "companion": "https://arxiv.org/abs/2412.09413",
    "summary": "Collection of question and verifiable answer extracted from Math, Numina, and Aime. # Additional notes (for additional assets) MIT-8 benchmark Proof-Pile - ArXiv.math (10GB) - Open-source math textbooks (50MB) - Formal mathematics libraries (500MB) - Lean mathlib and other Lean repositories - Isabelle AFP - Coq mathematical components and other Coq repositories - HOL Light - set.mm - Mizar Mathematical Library - Math Overflow and Math Stack Exchange (2.5GB) - Wiki-style sources (50MB) - ProofWiki - Wikipedia math articles - MATH dataset (6MB) https://huggingface.co/datasets/HuggingFaceTB/cosmopedia https://huggingface.co/datasets/open-web-math/open-web-math https://github.com/LiveCodeBench/LiveCodeBench https://huggingface.co/datasets/LLM360/MegaMath https://huggingface.co/datasets/math-ai/AutoMathText https://arxiv.org/abs/2402.07625 https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k AMPS (Khan + Mathematica) Dataset OCR: https://arxiv.org/pdf/2502.18443"
}
```

still_long_format.json
```json
{
    "name": "Still long format",
    "size": "5,000 samples, 20 MB.",
    "download": "https://huggingface.co/datasets/RUC-AIBOX/long_form_thought_data_5k",
    "companion": "https://arxiv.org/abs/2412.09413",
    "summary": "Similar to Still, with a focus on long answer, questions come from NuminaMath, Aime, Leetcode, OpenCoder, Camel, Gaokao (Chinese A-level) and RiddleSense. # Formal Math"
}
```

swe_bench.json
```json
{
    "name": "SWE-bench",
    "size": "2,300 samples",
    "download": "https://github.com/swe-bench/SWE-bench",
    "companion": "https://arxiv.org/abs/2310.06770",
    "summary": "Datasets of codebase, issues and unit tests. The goal is to fix the codebase that currently yield the issue resulting in failing unit tests. The data was collected from popular Github repository."
}
```

taco.json
```json
{
    "name": "TACO",
    "size": "26,000 problems with 1.5M solutions, .",
    "download": "https://huggingface.co/datasets/BAAI/TACO",
    "companion": "https://arxiv.org/abs/2312.14852",
    "summary": "Algorithmic problems collected from various platforms such as CodeChef, CodeForces, HackerRank, and GeeksforGeeks, as well as existing datasets such as APPS, CodeContest, and Description2code. Each problems come with unit tests that allows to test for correctness. # Filtered Sources Data that come from filtering a bigger dataset."
}
```

various_websites.json
```json
{
    "name": "Various Websites",
    "summary": "- The art of problem solving (https://artofproblemsolving.com/): a website that prepare students to various STEM competition, and is often used by researchers to craft datasets with rationale. - ProofWiki (https://proofwiki.org/):  a website that aim at collecting math proofs. - StackExchange - Wikipedia # Semi-Native Sources Data that was built by leveraging existing assets with substantial data scrapping work."
}
```

