{
    "name": "FineMath",
    "download": "https://huggingface.co/datasets/HuggingFaceTB/finemath",
    "companion": "https://arxiv.org/abs/2502.02737",
    "summary": "Filtered from CommonCrawl by HuggingFace for SmolLM focused on Math domain."
}


Here's the completed dataset card for **FineMath**:

- **name**: FineMath

- **summary**: FineMath is a curated dataset comprising high-quality mathematical educational content filtered from Common Crawl. Developed to enhance the training of language models in mathematical reasoning and problem-solving, FineMath emphasizes clear explanations and step-by-step solutions, utilizing Markdown and LaTeX formatting for clarity. The dataset is structured into two main subsets:
  - **FineMath-3+**: Contains 34 billion tokens across 21.4 million documents.
  - **FineMath-4+**: A higher-quality subset with 9.6 billion tokens in 6.7 million documents, focusing on detailed explanations.

- **size**: Approximately 34 billion tokens (FineMath-3+) and 9.6 billion tokens (FineMath-4+).

- **download**: The dataset is available for download at [https://huggingface.co/datasets/HuggingFaceTB/finemath](https://huggingface.co/datasets/HuggingFaceTB/finemath).

- **companion**: The dataset is introduced in the paper "SmolLM2: When Smol Goes Big -- Data-Centric Training of a Small Language Model," accessible at [https://arxiv.org/abs/2502.02737](https://arxiv.org/abs/2502.02737).

- **notes**: FineMath was developed by training a mathematical content classifier using annotations generated by Llama-3.1-70B-Instruct. This classifier filtered Common Crawl data to retain only the most educational mathematical content, prioritizing clear explanations and step-by-step problem-solving over advanced academic papers. The dataset supports research in mathematical reasoning and problem-solving, providing a valuable resource for training models in these areas.

- **tags**: "web", "compilation", "filtration", "CommonCrawl", "mathematics", "education"

This card provides an overview of the FineMath dataset, highlighting its structure, content, and significance in advancing the training of language models in mathematical reasoning and problem-solving. 