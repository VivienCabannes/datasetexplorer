{
    "name": "FineWeb-Edu",
    "download": "https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu",
    "companion": "https://arxiv.org/abs/2406.17557",
    "summary": "Filtered from FineWeb to focus on sample with educational value"
}

Here's the completed dataset card for **FineWeb-Edu**:

- **name**: FineWeb-Edu

- **summary**: FineWeb-Edu is a curated dataset comprising approximately 1.3 trillion tokens of high-quality educational content, extracted from the larger FineWeb dataset. Utilizing an educational quality classifier trained on annotations generated by Llama 3, this dataset focuses on texts with significant educational value. Models pretrained on FineWeb-Edu have demonstrated enhanced performance on knowledge-intensive benchmarks, such as MMLU and ARC, compared to those trained on broader datasets like FineWeb. 

- **size**: Approximately 1.3 trillion tokens. 

- **download**: The dataset is available for download at [https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu). 

- **companion**: The dataset is introduced in the paper "The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale," accessible at [https://arxiv.org/abs/2406.17557](https://arxiv.org/abs/2406.17557). 

- **notes**: FineWeb-Edu was developed by filtering the FineWeb dataset using an educational quality classifier. This classifier was trained on 450,000 web samples annotated by Llama 3, scoring content on a scale from 0 to 5 based on educational value. The resulting dataset emphasizes high-scoring educational texts, aiming to enhance the training of language models in educational and knowledge-intensive tasks. 

- **tags**: "web", "compilation", "filtration", "CommonCrawl", "LLM training", "educational content"

This card provides an overview of the FineWeb-Edu dataset, highlighting its structure, content, and role in advancing the training of language models, particularly in educational contexts. 