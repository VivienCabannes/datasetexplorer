{
    "name": "APPS",
    "size": "10,000 problems",
    "download": "https://huggingface.co/datasets/codeparrot/apps",
    "companion": "https://arxiv.org/pdf/2105.09938",
    "notes": "AlphaCode mentions that test coverage was insufficient, leading to false positive.",
    "summary": "10,000 programming problems with python solutions and test cases for correctness.  Curated from Codewars, AtCoder, Kattis, and Codeforces."
}

Here's the completed dataset card for **APPS (Automated Programming Progress Standard)**:

- **name**: APPS (Automated Programming Progress Standard)

- **summary**: APPS is a comprehensive benchmark dataset designed to evaluate the code generation capabilities of language models. It comprises 10,000 Python programming problems sourced from platforms such as Codewars, AtCoder, Kattis, and Codeforces. Each problem includes a natural language description, multiple ground-truth solutions, and corresponding test cases to assess solution correctness. The dataset spans various difficulty levels, from introductory to competition-level challenges, aiming to mirror the evaluation process of human programmers by emphasizing both coding proficiency and problem-solving skills. 

- **size**: 10,000 problems with associated solutions and test cases. 

- **download**: The dataset is available for download at [https://huggingface.co/datasets/codeparrot/apps](https://huggingface.co/datasets/codeparrot/apps). 

- **companion**: The dataset is introduced in the paper "Measuring Coding Challenge Competence With APPS," accessible at [https://arxiv.org/pdf/2105.09938](https://arxiv.org/pdf/2105.09938). 

- **notes**: While APPS provides a substantial collection of programming problems with corresponding test cases, it has been noted that the test coverage may be insufficient in some instances. This limitation can lead to false positives, where incorrect solutions pass the available test cases. For example, the AlphaCode system reported a false positive rate of up to 60% when evaluated with an average of 20 unit tests per problem. Therefore, users should exercise caution and consider augmenting the test cases or employing additional verification methods when utilizing this dataset for model evaluation or training purposes. 

- **tags**: "code generation", "programming challenges", "Python", "benchmark", "test cases", "Codewars", "AtCoder", "Kattis", "Codeforces"

This card provides an overview of the APPS dataset, highlighting its structure, content, and considerations for use in evaluating and training language models on programming tasks. 