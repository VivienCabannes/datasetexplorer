{
    "name": "Cosmepedia",
    "dataset": "https://huggingface.co/datasets/HuggingFaceTB/cosmopedia"
}

Here's the completed dataset card for **Cosmepedia**:

- **name**: Cosmepedia

- **summary**: Cosmepedia is a synthetic dataset comprising over 30 million documents, totaling approximately 25 billion tokens. The dataset includes various forms of content such as textbooks, blog posts, stories, and WikiHow articles, all generated by the Mixtral-8x7B-Instruct-v0.1 model. The primary objective of Cosmepedia is to facilitate the pre-training of language models by providing a diverse and extensive collection of synthetic data. The dataset encompasses a wide range of topics, aiming to replicate the breadth of knowledge found in web datasets like RefinedWeb and RedPajama. It is organized into eight distinct subsets, each corresponding to different source materials used as seed data for generation:
  - **web_samples_v1**: Derived from web data similar to RefinedWeb.
  - **web_samples_v2**: Similar to v1 with refined prompts for enhanced depth.
  - **stanford**: Based on course outlines from Stanford University.
  - **stories**: Generated stories to enhance commonsense knowledge.
  - **wikihow**: Articles generated from scraped WikiHow titles.
  - **openstax**: Content from OpenStax course outlines.
  - **khanacademy**: Based on course outlines from Khan Academy.
  - **auto_math_text**: Utilizes samples from the AutoMathText dataset to bolster scientific knowledge.

  Each subset is designed to contribute uniquely to the dataset's overall diversity and educational value. 

- **size**: Over 30 million documents, totaling approximately 25 billion tokens. 

- **download**: The dataset is available for download at [https://huggingface.co/datasets/HuggingFaceTB/cosmopedia](https://huggingface.co/datasets/HuggingFaceTB/cosmopedia). 

- **companion**: Detailed information about the dataset and its creation process is discussed in the blog post "Cosmopedia: how to create large-scale synthetic data for pre-training," accessible at [https://huggingface.co/blog/cosmopedia](https://huggingface.co/blog/cosmopedia). 

- **notes**: Cosmepedia was developed to address the challenges of creating large-scale synthetic datasets for language model pre-training. The dataset's creation involved meticulous prompt engineering to ensure diversity and minimize duplication. The synthetic data generation process aimed to cover a broad spectrum of topics, providing a comprehensive resource for training language models. Additionally, subsets like **stories** were included to enhance commonsense and day-to-day knowledge aspects within the dataset. 

- **tags**: "synthetic", "textbooks", "blog posts", "stories", "WikiHow", "language model pre-training"

This card provides an overview of the Cosmepedia dataset, highlighting its structure, content, and significance in advancing the pre-training of language models through synthetic data. 