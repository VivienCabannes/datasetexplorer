{
    "name": "ArXiv",
    "size": "29B tokens",
    "download": "https://huggingface.co/datasets/EleutherAI/proof-pile-2 contains data scrapped from the ArXiv website, according to a snapshot taken in 2023 by RedPajama.",
    "notes": "The dataset is valuable for training models on advanced mathematical concepts and research-level problems.",
    "summary": ""
}

Here's the completed dataset card for the **ArXiv Subset of Proof-Pile-2**:

- **name**: ArXiv Subset of Proof-Pile-2

- **summary**: The ArXiv Subset is a component of the Proof-Pile-2 dataset, comprising approximately 29 billion tokens of scientific and mathematical documents sourced from arXiv. This subset was curated to support the training of large language models, particularly in advanced mathematical and scientific reasoning. It serves as a valuable resource for developing models capable of understanding and generating research-level content. 

- **size**: Approximately 29 billion tokens. 

- **download**: The dataset is available as part of the Proof-Pile-2 collection at [https://huggingface.co/datasets/EleutherAI/proof-pile-2](https://huggingface.co/datasets/EleutherAI/proof-pile-2). To load only the ArXiv subset, use the following code:

  
```python
  from datasets import load_dataset
  ds_arxiv = load_dataset("EleutherAI/proof-pile-2", "arxiv")
  ```


  

- **notes**: The ArXiv Subset is instrumental for training models on advanced mathematical concepts and research-level problems. It was utilized in training the Llemma 7B and Llemma 34B models, which are designed for mathematical reasoning tasks. 

- **tags**: "arXiv", "Proof-Pile-2", "mathematics", "scientific research", "language model training"

This card provides an overview of the ArXiv Subset within the Proof-Pile-2 dataset, highlighting its composition, access instructions, and relevance in training language models for scientific and mathematical applications. 