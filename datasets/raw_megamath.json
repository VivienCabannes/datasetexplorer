{
  "name": "MegaMath",
  "summary": "_âš  Use card with caution_ <br>An extensive open math pretraining dataset with over 300 billion tokens, curated to enhance mathematical reasoning in language models.",
  "size": "Approximately 215 million samples totaling 371.6 billion tokens.",
  "date": "2025-04-03",
  "download": "https://huggingface.co/datasets/LLM360/MegaMath",
  "companion": "https://arxiv.org/abs/2504.02807",
  "notes": "MegaMath surpasses previous open math pretraining datasets, such as DeepSeekMath, by over 30% in token count. Extensive experiments during development led to optimized practices for text extraction, deduplication, and fastText training, ensuring high data quality. Training language models on MegaMath has demonstrated a 15% to 20% performance boost on ten downstream benchmarks, underscoring its efficacy.",
  "tags": [
    "synthetic",
    "compilation",
    "answer:verifiable",
    "question:synthetic",
    "answer:synthetic",
    "mathematics",
    "programming"
  ]
}
