{
    "name": "FineWeb",
    "download": "https://huggingface.co/datasets/HuggingFaceFW/fineweb",
    "companion": "https://arxiv.org/abs/2406.17557",
    "summary": "Filtered from CommonCrawl by HuggingFace, seems to be slightly worse than DCLM."
}

Here's the completed dataset card for **FineWeb**:

- **name**: FineWeb

- **summary**: FineWeb is a large-scale dataset developed by Hugging Face, comprising over 15 trillion tokens of cleaned and deduplicated English web data sourced from 96 Common Crawl snapshots. Designed to optimize the training of large language models (LLMs), FineWeb incorporates advanced filtering techniques to enhance data quality. Models trained on FineWeb have demonstrated superior performance compared to those trained on other publicly available datasets, such as C4, Dolma-v1.6, The Pile, SlimPajama, and RedPajama2. citeturn0search1

- **size**: Over 15 trillion tokens, approximately 44 terabytes of disk space. citeturn0search1

- **download**: The dataset is available for download at [https://huggingface.co/datasets/HuggingFaceFW/fineweb](https://huggingface.co/datasets/HuggingFaceFW/fineweb). citeturn0search1

- **companion**: The dataset is introduced in the paper "The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale," accessible at [https://arxiv.org/abs/2406.17557](https://arxiv.org/abs/2406.17557). citeturn0academia22

- **notes**: FineWeb was initially intended as an open replication of RefinedWeb. Through additional filtering steps, it has achieved enhanced performance, surpassing its predecessors. citeturn0search1 However, subsequent datasets like DCLM have reported further improvements over FineWeb. citeturn0search10 Users should consider these developments when selecting a dataset for training LLMs.

- **tags**: "web", "compilation", "filtration", "CommonCrawl", "LLM training"

This card provides an overview of the FineWeb dataset, highlighting its structure, content, and role in advancing the training of large language models. 