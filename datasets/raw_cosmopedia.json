{
  "name": "Cosmopedia",
  "summary": "_âš  Use card with caution_ <br>A synthetic dataset of over 30 million documents generated by Mixtral-8x7B-Instruct-v0.1, including textbooks, blog posts, stories, and WikiHow articles.",
  "size": "Over 30 million documents totaling approximately 25 billion tokens.",
  "date": "2024-03-20",
  "download": "https://huggingface.co/datasets/HuggingFaceTB/cosmopedia",
  "companion": "https://huggingface.co/blog/cosmopedia",
  "notes": "Cosmopedia is organized into eight subsets based on the source materials used as seed data for generation: web_samples_v1, web_samples_v2, stanford, stories, wikihow, openstax, khanacademy, and auto_math_text. The dataset was developed to facilitate language model pre-training by providing diverse synthetic data across a broad range of topics.",
  "tags": [
    "synthetic",
    "monolithic",
    "compilation",
    "nlp"
  ]
}
