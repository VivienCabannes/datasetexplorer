{
    "name": "Wizard of Wikipedia",
    "summary": "A knowledge-grounded open-domain dialogue dataset where one speaker (the \"Wizard\") has access to Wikipedia knowledge and the other (\"Apprentice\") does not. They converse on an open-ended topic. The wizard provides knowledgeable responses with relevant retrieved sentences. There are 22k dialogues with ~201k turns, each grounded in a Wikipedia sentence&#8203;:contentReference[oaicite:95]{index=95}.",
    "size": "~22,311 dialogs, ~201,000 utterances total. (Train 18,430, Dev 1,948, Test 1,933 dialogs). Each turn by Wizard is annotated with the Wiki sentence used.",
    "download": "Available via ParlAI (Wizard of Wikipedia task) and on HuggingFace `wizard_of_wikipedia`.",
    "companion": "https://arxiv.org/abs/1811.01241 (Dinan et al. 2019, Wizard of Wikipedia paper)",
    "notes": "This dataset is used to train conversational agents that can incorporate factual knowledge smoothly. The Wizard was asked to naturally steer the conversation and inform the Apprentice using retrieved knowledge. It\u2019s a multi-turn knowledge-QA hybrid (the apprentice often asks questions, wizard answers with facts). Evaluations often consider knowledge F1 (overlapping facts) and fluency.",
    "tags": [
        "crowd-sourced",
        "dialog",
        "tool use",
        "question:crowd-sourced",
        "answer:crowd-sourced",
        "domain:nlp"
    ]
}
