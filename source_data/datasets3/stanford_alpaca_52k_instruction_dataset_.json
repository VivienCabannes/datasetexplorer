{
    "name": "Stanford Alpaca (52k Instruction Dataset)",
    "summary": "A synthetic instruction-following dataset created by Stanford by prompting OpenAI\u2019s text-davinci-003 (GPT) with 175 human-written seed tasks. It contains 52,000 diverse instructions and GPT-generated responses, covering tasks like writing, transformation, closed QA, code, etc. It aims to replicate the behavior of InstructGPT in an open dataset.",
    "size": "52,000 instruction-response pairs. (No official train/test split; it\u2019s mainly for fine-tuning). ~80 MB JSON.",
    "download": "Released on GitHub (stanford_alpaca) with data in JSON. Also available on HuggingFace (`tatsu-lab/alpaca`).",
    "companion": "https://crfm.stanford.edu/2023/03/13/alpaca.html (Stanford CRFM blog announcing Alpaca 2023)",
    "notes": "Source prompts were based on Self-Instruct techniques. The data covers many domains and formats (e.g., brainstorming, Q&A, classification). This dataset is widely used to fine-tune LLaMA and other models to be chatty and follow user instructions. It is synthetic (GPT outputs as answers) so quality is high but it may contain subtle biases from the base model.",
    "tags": [
        "synthetic",
        "monolithic",
        "compilation",
        "question:synthetic",
        "answer:synthetic",
        "domain:nlp"
    ]
}
