{
    "name": "MMLU (Massive Multitask Language Understanding)",
    "summary": "A benchmark of 57 academic and professional subjects, each as a set of multiple-choice questions (4 options). Totaling 14,000+ questions, covering elementary math, US history, college chemistry, law, etc. It is designed to evaluate broad knowledge and reasoning of models&#8203;:contentReference[oaicite:76]{index=76}.",
    "size": "57 subjects, each ~100-500 questions (14,000+ questions total). All are 4-choice multiple-choice. Provided as a test set mostly (with a small dev for few-shot use).",
    "download": "The dataset can be obtained from Hendrycks et al.\u2019s repository (ETH-PUBLIC/MMLU) and on HuggingFace `cais/mmlu`.",
    "companion": "https://arxiv.org/abs/2002.03786 (Hendrycks et al. 2021, MMLU paper)",
    "notes": "No training set is provided \u2013 meant as a zero or few-shot eval. Questions were sourced from exams like AP tests, college exams, etc., so they require real-world knowledge and problem-solving. Evaluation is simply accuracy. It's become a standard test for large language models\u2019 breadth of knowledge.",
    "tags": [
        "compilation",
        "QA",
        "benchmark",
        "question:human",
        "answer:human",
        "domain:nlp"
    ]
}
