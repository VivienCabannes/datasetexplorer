{
  "name": "HumanEval",
  "summary": "HumanEval is a benchmark dataset developed by OpenAI to evaluate the functional correctness of code generated by language models. It comprises 164 hand-crafted Python programming problems, each designed to assess a model's ability to synthesize programs from natural language descriptions.",
  "size": "164 programming problems",
  "download": "https://github.com/openai/human-eval",
  "companion": "https://arxiv.org/abs/2107.03374",
  "notes": "Each problem in the HumanEval dataset includes a function signature, a docstring specifying the intended functionality, and a set of unit tests to verify correctness. The dataset emphasizes functional correctness over mere syntactic accuracy, making it a robust benchmark for code generation models. Notably, it has been used to evaluate models like OpenAI's Codex, which achieved a 28.8% pass rate on these problems.",
  "tags": [
    "human",
    "QA",
    "benchmark",
    "answer:verifiable",
    "question:human",
    "answer:human",
    "programming"
  ]
}
