{"name": "APPS (Automated Programming Progress Standard)", "summary": "A large dataset of coding problems, including competitive programming challenges. It contains 10,000 problems of varying difficulty, each with a problem description and public test cases&#8203;:contentReference[oaicite:87]{index=87}. Solutions (in Python) are provided for the training set. Models are evaluated by writing code that passes the test cases.", "size": "10,000 problems (5k training with solutions, 5k test)&#8203;:contentReference[oaicite:88]{index=88}. Total ~131k test cases across all problems&#8203;:contentReference[oaicite:89]{index=89}; ~232k human solutions for train problems&#8203;:contentReference[oaicite:90]{index=90}.", "download": "APPS dataset on the authors\u2019 GitHub (Measuring Coding Challenge Competence). Also on HuggingFace `codeparrot/apps`.", "companion": "https://arxiv.org/abs/2105.09938 (Hendrycks et al. 2021, APPS NeurIPS paper)", "notes": "Problem difficulty ranges from easy (100-line solutions) to very hard (algorithmic puzzles). The test set is further divided by difficulty for evaluation. APPS is used to benchmark code generation at scale. Models are typically evaluated on the test set by execution-based metrics (% of tests passed). It's significantly larger and more challenging than MBPP or HumanEval.", "tags": ["compilation", "QA", "benchmark", "tool use", "question:human", "answer:human", "domain:programming"]}