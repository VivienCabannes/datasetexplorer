#### Prompt to build dataset cards

I'm building a dataset explorer with a structured tag system. I’ve already collected several datasets, but many are missing key details. For each dataset, I want to complete a card containing the following fields:

- **name**: the name of the dataset  
- **summary**: a brief executive summary of the dataset  
- **size**: the dataset’s size, both in terms of the number of samples and its storage footprint (in bytes)  
- **download**: the download link or location  
- **companion**: a companion paper, if one was published alongside the dataset  
- **notes**: additional details that could support an extended summary  
- **tags**: all relevant tags based on my tagging system

The tag system is organized into the following groups:

```json
[
  {
    "group": "Source",
    "tags": {
      "human": "data generated by humans without AI in mind",
      "crowd-sourced": "data collected via human crowd-sourcing platforms to train AI models",
      "synthetic": "data generated by machines"
    }
  },
  {
    "group": "Type",
    "tags": {
      "monolithic": "continuous, unstructured blocks of text useful for pretraining",
      "dialog": "data that can be segmented into individual turns (e.g., `user, llm, python-tool, llm, user, llm, user, llm`)",
      "QA": "question–answer data",
      "tool use": "data illustrating the integration of external tools"
    }
  },
  {
    "group": "Collection process",
    "tags": {
      "filtration": "data obtained by filtering a large chunk of the internet",
      "compilation": "a mixture of sources compiled for training purposes",
      "atomic": "data from a single, consistent source (e.g., standardized exam sets)",
      "benchmark": "datasets originally created for evaluation or benchmarking"
    }
  },
  {
    "group": "QA specific",
    "tags": {
      "answer:verifiable": "answers can be independently verified (e.g., factual or numerical)",
      "with rationale": "answers that include supporting justifications",
      "question:synthetic": "questions generated by machines",
      "question:human": "questions written by humans",
      "question:crowd-sourced": "questions collected via crowd-sourcing",
      "answer:synthetic": "answers generated by machines",
      "answer:human": "answers written by humans",
      "answer:crowd-sourced": "answers collected via crowd-sourcing",
      "rationale:synthetic": "rationales generated by machines",
      "rationale:human": "rationales written by humans",
      "rationale:crowd-sourced": "rationales collected via crowd-sourcing"
    }
  },
  {
    "group": "Domain",
    "tags": {
      "mathematics": "focused on math problems and reasoning",
      "physics": "focused on physics content",
      "programming": "focused on coding and software tasks",
      "nlp": "focused on natural language processing"
    }
  }
]
```

Each dataset should be tagged with the relevant inner-level tags (e.g., `"human"`, `"QA"`, `"benchmark"`).

Your role is to help me complete these dataset cards. I’ll provide partially filled cards, and you’ll research and complete any missing information, while also improving the quality of what’s already there.