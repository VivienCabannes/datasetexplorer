Task: You are my assistant for editing dataset cards.
Your job is to polish and standardize draft dataset card entries according to the guidelines provided below.

### Overview

I am developing a dataset explorer based on dataset cards. I have drafted several cards using the following template:

```json
{
  "name": "XXX",
  "summary": "XXX",
  "date": "XXX",
  "download": "XXX",
  "companion": "XXX",
  "notes": "XXX",
  "tags": [
    "XYZ",
    "ABC",
    "..."
  ]
}
```

I have created draft cards but need to polish them to:
- **Add Date:** By default, the date is taken from the first companion paper on ArXiv. (Please verify this date on the internet before finalizing.)
- **Trim Text:** Ensure that all descriptions are succinct and to the point. In particular, the summary should be a one-liner.
- **Correct Tags:** Make sure that each tag corresponds to one of the final tags defined in the following file:

```json
[
  {
    "group": "Source",
    "tags": {
      "human": "data generated by humans without AI in mind",
      "crowd-sourced": "data collected via human crowd-sourcing platforms to train AI models",
      "synthetic": "data generated by machines"
    }
  },
  {
    "group": "Type",
    "tags": {
      "monolithic": "continuous, unstructured blocks of text useful for pretraining",
      "dialog": "data that can be segmented into individual turns (e.g., `user, llm, python-tool, llm, user, llm, user, llm`)",
      "QA": "question–answer data",
      "tool use": "data illustrating the integration of external tools"
    }
  },
  {
    "group": "Collection process",
    "tags": {
      "filtration": "data obtained by filtering a large chunk of the internet",
      "compilation": "a mixture of sources compiled for training purposes",
      "atomic": "data from a single, consistent source (e.g., standardized exam sets)",
      "benchmark": "datasets originally created for evaluation or benchmarking"
    }
  },
  {
    "group": "QA specific",
    "tags": {
      "answer:verifiable": "answers can be independently verified (e.g., factual or numerical)",
      "with rationale": "answers that include supporting justifications",
      "question:synthetic": "questions generated by machines",
      "question:human": "questions written by humans",
      "question:crowd-sourced": "questions collected via crowd-sourcing",
      "answer:synthetic": "answers generated by machines",
      "answer:human": "answers written by humans",
      "answer:crowd-sourced": "answers collected via crowd-sourcing",
      "rationale:synthetic": "rationales generated by machines",
      "rationale:human": "rationales written by humans",
      "rationale:crowd-sourced": "rationales collected via crowd-sourcing"
    }
  },
  {
    "group": "Domain",
    "tags": {
      "mathematics": "focused on math problems and reasoning",
      "physics": "focused on physics content",
      "programming": "focused on coding and software tasks",
      "nlp": "focused on natural language processing"
    }
  }
]
```

---

### Example 1: AIME Card

**Draft 1:**

```json
{
  "name": "AIME",
  "size": "15 questions per year",
  "download": "Various formatted datasets based on historical AIME questions are available, e.g., [on HuggingFace](https://huggingface.co/datasets/di-zhang-fdu/AIME_1983_2024)",
  "notes": "Since AIME is an annual examination, new data becomes available each year. However, designers may select questions already present on the internet, which could be included in pretraining corpora.",
  "summary": "US high-school competition, American Invitational Mathematics Examination.",
  "tags": ["atomic", "benchmark", "QA", "question:human", "answer:human"]
}
```

**Draft 2:**

```json
{
  "name": "AIME (American Invitational Mathematics Examination)",
  "summary": "The AIME is a prestigious annual mathematics competition in the United States that bridges the American Mathematics Competitions (AMC) and the USA Mathematical Olympiad (USAMO). It comprises 15 challenging questions solved within a 3-hour time limit.",
  "size": "Each exam features 15 questions. Given its annual occurrence since 1983, the dataset covers many years of problems.",
  "download": "For example, view the [di-zhang-fdu/AIME_1983_2024](https://huggingface.co/datasets/di-zhang-fdu/AIME_1983_2024) dataset on Hugging Face.",
  "notes": "New problems are introduced each year. Some questions might overlap with those already available online, possibly appearing in pretraining corpora.",
  "tags": [
    "mathematics",
    "competition",
    "high school",
    "problem-solving",
    "benchmark",
    "QA",
    "question:human",
    "answer:human"
  ]
}
```

**Final Polished Version:**

```json
{
  "name": "AIME (American Invitational Mathematics Examination)",
  "summary": "Annual US high-school competition, more challenging than the AMC and less so than the USAMO.",
  "size": "15 questions per year.",
  "download": "https://huggingface.co/datasets/di-zhang-fdu/AIME_1983_2024",
  "notes": "The exam lasts 3 hours and expects a numerical answer between 0 and 999.",
  "tags": [
    "human",
    "QA",
    "dialog",
    "atomic",
    "benchmark",
    "answer:human",
    "answer:verifiable",
    "question:human",
    "mathematics"
  ]
}
```

---

### Example 2: Algebraic Stack Card

**Draft 1:**

```json
{
  "name": "Algebraic Stack",
  "summary": "Code data coming from filtering the Stack, as well as extracting formal proofs.",
  "size": "Approximately 11 billion tokens (~40 MB).",
  "date": "2023-10-16",
  "download": "Available as part of the [Proof-Pile-2 mix](https://huggingface.co/datasets/EleutherAI/proof-pile-2).",
  "companion": "Introduced in the data mix to the train Llema: https://arxiv.org/abs/2310.10631",
  "notes": "The dataset is dominated by Python (65%), Isabelle (10%), and C++ (9%). Among others, formal proofs are extracted from the Lean mathlib library and the Isabelle Archive of Formal Proofs.",
  "tags": [
    "human",
    "monolithic",
    "filtration",
    "programming"
  ]
}
```

**Draft 2:**

```json
{
  "name": "Algebraic Stack",
  "summary": "A specialized subset of the Proof-Pile-2 dataset, comprising approximately 11 billion tokens of mathematical code curated to support language model training in mathematical domains. It includes source code from 17 programming languages and extracted proof states from Lean's Mathlib 4 and Isabelle proofs.",
  "size": "Approximately 11 billion tokens.",
  "download": "https://huggingface.co/datasets/EleutherAI/proof-pile-2",
  "companion": "https://arxiv.org/abs/2310.10631",
  "notes": "Algebraic Stack was instrumental in training the Llemma models. The dataset predominantly features Python (65%), with Isabelle (10%) and C++ (9%) also represented. Formal proofs are extracted from the Lean mathlib library and the Isabelle Archive of Formal Proofs.",
  "tags": [
    "human",
    "monolithic",
    "filtration",
    "programming"
  ]
}
```

**Final Polished Version:**

```json
{
  "name": "Algebraic Stack",
  "summary": "Code data obtained by filtering the Stack, combined with extracted formal proofs.",
  "size": "Approximately 11 billion tokens (~40 MB).",
  "date": "2023-10-16",
  "download": "https://huggingface.co/datasets/EleutherAI/proof-pile-2",
  "companion": "https://arxiv.org/abs/2310.10631",
  "notes": "The dataset is dominated by Python (65%), Isabelle (10%), and C++ (9%). Formal proofs are extracted from the Lean mathlib library and the Isabelle Archive of Formal Proofs.",
  "tags": [
    "human",
    "monolithic",
    "filtration",
    "programming"
  ]
}
```

---

### Polishing Guidelines

- **Date Field:**  
  Verify the date by searching for the publication date of the first companion paper on ArXiv and update the field as needed.

- **Download & Companion Fields:**  
  These should generally contain only the URL links without any additional descriptive text. For example:
  - **Download:** `https://example.com/download`
  - **Companion:** `https://arxiv.org/abs/XXXX.XXXXX`

- **Summary:**  
  Keep the summary brief and direct—ideally one sentence that captures the essence of the dataset.

- **Tags:**  
  Ensure that each tag is drawn from the final tag list. Cross-check each tag’s group (e.g., Source, Type, Collection process, QA specific, Domain) to maintain consistency.

I will provide you with drafted cards.
Your task is to return a corresponding polished JSON card for each one.
