{
  "name": "MATH-500",
  "summary": "MATH-500 is a curated subset of 500 problems from the original MATH benchmark, designed to evaluate mathematical problem-solving abilities of language models across various topics such as algebra, geometry, and calculus.",
  "size": "500 problems",
  "download": "https://huggingface.co/datasets/HuggingFaceH4/MATH-500",
  "companion": "https://arxiv.org/abs/2103.03874",
  "notes": "The MATH-500 dataset is utilized to assess and benchmark the performance of language models in mathematical reasoning and problem-solving. It includes problems of varying difficulty levels, providing a comprehensive evaluation framework. Notably, models like DeepSeek R1 and OpenAI's o3-mini have achieved high accuracy scores on this benchmark, indicating its role in advancing mathematical reasoning capabilities in AI systems.",
  "tags": [
    "human",
    "QA",
    "benchmark",
    "answer:verifiable",
    "question:human",
    "answer:human",
    "mathematics"
  ]
}