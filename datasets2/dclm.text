Here's the completed dataset card for **DataComp-LM (DCLM)**:

- **name**: DataComp-LM (DCLM)

- **summary**: DataComp-LM (DCLM) is a comprehensive benchmark designed to facilitate controlled experiments in dataset curation for training large language models (LLMs). It provides a standardized corpus of 240 trillion tokens extracted from Common Crawl, effective pretraining recipes based on the OpenLM framework, and a broad suite of 53 downstream evaluations. Researchers can experiment with data curation strategies such as deduplication, filtering, and data mixing across model scales ranging from 412 million to 7 billion parameters. Notably, model-based filtering has been identified as key to assembling high-quality training sets. The resulting dataset, DCLM-Baseline, enables training a 7B parameter language model from scratch to achieve 64% 5-shot accuracy on MMLU with 2.6 trillion training tokens, outperforming previous state-of-the-art open-data language models while using 40% less compute. 

- **size**: 240 trillion tokens. 

- **download**: The dataset is available for download at [https://huggingface.co/datasets/mlfoundations/dclm-baseline-1.0](https://huggingface.co/datasets/mlfoundations/dclm-baseline-1.0). 

- **companion**: The dataset is introduced in the paper "DataComp-LM: In search of the next generation of training sets for language models," accessible at [https://arxiv.org/abs/2406.11794](https://arxiv.org/abs/2406.11794). 

- **notes**: DCLM emphasizes the importance of dataset design in training language models and offers a starting point for further research on data curation. The benchmark consists of multiple scales, with various candidate pool sizes and associated compute budgets, facilitating the study of scaling trends and making the benchmark accessible to researchers with varying resources. 

- **tags**: "benchmark", "dataset curation", "language models", "Common Crawl", "data filtering", "data deduplication"

This card provides an overview of the DataComp-LM (DCLM) dataset, highlighting its structure, content, and significance in advancing research on dataset curation for training large language models.
