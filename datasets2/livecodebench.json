{
  "name": "LiveCodeBench",
  "summary": "LiveCodeBench is a comprehensive and contamination-free benchmark designed to evaluate the coding capabilities of large language models (LLMs). It continuously collects new problems from coding competition platforms such as LeetCode, AtCoder, and CodeForces, ensuring an up-to-date and challenging problem set. Beyond code generation, LiveCodeBench assesses a broader range of code-related capabilities, including self-repair, code execution, and test output prediction.",
  "size": "As of January 2025, LiveCodeBench hosts 880 high-quality coding problems that were published between May 2023 and January 2025.",
  "download": "The dataset is available for download at [https://github.com/LiveCodeBench/LiveCodeBench](https://github.com/LiveCodeBench/LiveCodeBench).",
  "companion": "The dataset is introduced in the paper \"LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code,\" accessible at [https://arxiv.org/abs/2403.07974](https://arxiv.org/abs/2403.07974). citeturn0academia20",
  "notes": "LiveCodeBench addresses the issue of test set contamination by continuously updating its problem set with new challenges from recent coding competitions. This approach ensures that the benchmark remains relevant and that models are evaluated on problems they have not been exposed to during training. citeturn0academia20",
  "tags": [
    "human",
    "QA",
    "benchmark",
    "answer:verifiable",
    "question:human",
    "answer:human",
    "programming"
  ]
}