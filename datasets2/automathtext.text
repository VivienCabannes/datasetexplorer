Here's the completed dataset card for **AutoMathText**:

- **name**: AutoMathText

- **summary**: AutoMathText is a meticulously curated dataset encompassing approximately 200 GB of mathematical texts. It aggregates content from diverse platforms, including websites, arXiv, and GitHub repositories such as OpenWebMath, RedPajama, and Algebraic Stack. The dataset was autonomously selected and labeled by the Qwen-72B language model, assigning each piece of content a score (`lm_q1q2_score`) ranging from 0 to 1, reflecting its relevance, quality, and educational value in mathematical intelligence. 

- **size**: Approximately 200 GB of text data. 

- **download**: The dataset is available for download at [https://huggingface.co/datasets/math-ai/AutoMathText](https://huggingface.co/datasets/math-ai/AutoMathText). 

- **companion**: The dataset is introduced in the paper "Autonomous Data Selection with Zero-shot Generative Classifiers for Mathematical Texts," accessible at [https://arxiv.org/abs/2402.07625](https://arxiv.org/abs/2402.07625). 

- **notes**: AutoMathText was developed to enhance language models' proficiency in mathematical reasoning through continual pretraining. The dataset's autonomous selection process leverages base language models as zero-shot generative classifiers to evaluate and select high-quality mathematical content without human annotations. Empirical evaluations demonstrated that language models continually pretrained on AutoMathText achieved substantial improvements on mathematical benchmarks, underscoring the dataset's efficacy in enhancing mathematical reasoning capabilities. 

- **tags**: "mathematics", "language model pretraining", "autonomous data selection", "Qwen-72B", "OpenWebMath", "RedPajama", "Algebraic Stack"

This card provides an overview of the AutoMathText dataset, highlighting its structure, content, and significance in advancing mathematical reasoning in language models.
