{
  "name": "Līla",
  "summary": "Līla is a comprehensive benchmark for mathematical reasoning, encompassing over 140,000 natural language questions annotated with Python programs and natural language instructions. The benchmark integrates 23 diverse tasks across four dimensions:",
  "size": "Over 140,000 questions across 23 tasks.",
  "download": "The dataset is available for download at [https://huggingface.co/datasets/allenai/lila](https://huggingface.co/datasets/allenai/lila).",
  "companion": "The dataset is introduced in the paper \"Līla: A Unified Benchmark for Mathematical Reasoning,\" accessible at [https://arxiv.org/abs/2210.17517](https://arxiv.org/abs/2210.17517).",
  "notes": "Līla includes multiple splits to evaluate models under different conditions:",
  "Līla-IID": "In-distribution split with train, dev, and test sets.",
  "Līla-OOD": "Out-of-distribution split to assess generalization.",
  "Līla-Robust": "Split designed to test robustness to language perturbations.",
  "tags": [
    "compilation",
    "QA",
    "benchmark",
    "answer:verifiable",
    "question:human",
    "answer:human",
    "mathematics"
  ]
}