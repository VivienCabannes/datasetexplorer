Draft 1:
```json

```

Draft 2:
```json
{
  "name": "MMLU",
  "summary": "The Massive Multitask Language Understanding (MMLU) dataset is a benchmark designed to evaluate the knowledge and problem-solving abilities of language models across 57 diverse subjects, including mathematics, history, law, and medicine.",
  "size": "Approximately 16,000 multiple-choice questions",
  "download": "https://github.com/hendrycks/test",
  "companion": "https://arxiv.org/abs/2009.03300",
  "notes": "MMLU serves as a comprehensive test for language models, assessing both world knowledge and reasoning skills. The dataset's broad subject coverage and varying difficulty levels make it a robust tool for identifying a model's strengths and weaknesses across different domains.",
  "tags": [
    "human",
    "QA",
    "benchmark",
    "answer:verifiable",
    "question:human",
    "answer:human"
  ]
}
```

Draft 3:
```json
{
    "name": "MMLU (Massive Multitask Language Understanding)",
    "summary": "A benchmark of 57 academic and professional subjects, each as a set of multiple-choice questions (4 options). Totaling 14,000+ questions, covering elementary math, US history, college chemistry, law, etc. It is designed to evaluate broad knowledge and reasoning of models&#8203;:contentReference[oaicite:76]{index=76}.",
    "size": "57 subjects, each ~100-500 questions (14,000+ questions total). All are 4-choice multiple-choice. Provided as a test set mostly (with a small dev for few-shot use).",
    "download": "The dataset can be obtained from Hendrycks et al.\u2019s repository (ETH-PUBLIC/MMLU) and on HuggingFace `cais/mmlu`.",
    "companion": "https://arxiv.org/abs/2002.03786 (Hendrycks et al. 2021, MMLU paper)",
    "notes": "No training set is provided \u2013 meant as a zero or few-shot eval. Questions were sourced from exams like AP tests, college exams, etc., so they require real-world knowledge and problem-solving. Evaluation is simply accuracy. It's become a standard test for large language models\u2019 breadth of knowledge.",
    "tags": [
        "compilation",
        "QA",
        "benchmark",
        "question:human",
        "answer:human",
        "domain:nlp"
    ]
}
```