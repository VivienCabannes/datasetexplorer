{"name": "HellaSwag", "summary": "A difficult dataset for commonsense NLI, focusing on picking the most plausible continuation of a given situation description. It contains 70k multiple-choice questions&#8203;:contentReference[oaicite:64]{index=64}. Each question is a partial description (from video caption or wikiHow), and the task is to choose the correct ending out of four, where distractors are adversarially generated to be challenging.", "size": "~70,000 problems (Train ~70k, Dev 10k, Test 10k) with 4 answer choices each&#8203;:contentReference[oaicite:65]{index=65}.", "download": "Available from the authors\u2019 website (rowanzellers.com/hellaswag) and on HuggingFace `hellaswag`.", "companion": "https://arxiv.org/abs/1905.07830 (Zellers et al. 2019, HellaSwag paper)", "notes": "Built upon SWAG dataset but made harder via adversarial filtering. The contexts come from video descriptions or procedural text, and endings require physical and contextual commonsense. Human performance is near 95%, while GPT-2 struggled ~41% on it, showing the challenge. It\u2019s often used to test common sense and coherence in LMs.", "tags": ["synthetic", "QA", "filtration", "question:human", "answer:human", "domain:nlp"]}