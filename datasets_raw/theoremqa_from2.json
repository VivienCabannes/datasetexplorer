{
  "name": "TheoremQA",
  "summary": "TheoremQA is a benchmark dataset designed to evaluate AI models' abilities to apply STEM theorems to solve complex, university-level problems across mathematics, physics, electrical engineering and computer science (EE&CS), and finance.",
  "size": "800 high-quality questions covering over 350 theorems",
  "download": "https://huggingface.co/datasets/TIGER-Lab/TheoremQA",
  "companion": "https://arxiv.org/abs/2305.12524",
  "notes": "Curated by domain experts, TheoremQA encompasses a diverse range of theorems, such as Taylor's theorem, Lagrange's theorem, Huffman coding, and Quantum Theorem. The dataset serves as a rigorous benchmark to assess large language models' capabilities in applying theoretical knowledge to solve challenging science problems. Evaluations have shown that models like GPT-4 achieve up to 51% accuracy with Program-of-Thoughts Prompting, while other open-source models perform below 15%, indicating the dataset's challenging nature.",
  "tags": [
    "human",
    "QA",
    "benchmark",
    "answer:verifiable",
    "question:human",
    "answer:human",
    "mathematics",
    "physics",
    "programming"
  ]
}
  