{
  "name": "APPS: Automated Programming Progress Standard",
  "summary": "APPS is a comprehensive benchmark dataset designed to evaluate the code generation capabilities of language models. It comprises 10,000 Python programming problems sourced from platforms such as Codewars, AtCoder, Kattis, and Codeforces. Each problem includes a natural language description, multiple ground-truth solutions, and corresponding test cases to assess solution correctness. The dataset spans various difficulty levels, from introductory to competition-level challenges, aiming to mirror the evaluation process of human programmers by emphasizing both coding proficiency and problem-solving skills.",
  "size": "10,000 problems with associated solutions and test cases.",
  "date": "2021-05-20",
  "download": "https://huggingface.co/datasets/codeparrot/apps",
  "companion": "https://arxiv.org/abs/2105.09938",
  "notes": "While APPS provides a substantial collection of programming problems with corresponding test cases, it has been noted that the test coverage may be insufficient in some instances. This limitation can lead to false positives, where incorrect solutions pass the available test cases. For example, the AlphaCode system reported a false positive rate of up to 60% when evaluated with an average of 20 unit tests per problem. Therefore, users should exercise caution and consider augmenting the test cases or employing additional verification methods when utilizing this dataset for model evaluation or training purposes.",
  "tags": [
    "code generation",
    "programming challenges",
    "Python",
    "benchmark",
    "test cases",
    "Codewars",
    "AtCoder",
    "Kattis",
    "Codeforces"
  ]
}