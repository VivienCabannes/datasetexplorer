{"name": "ELI5 (Explain Like I'm Five)", "summary": "A dataset of complex questions and long-form answers from the Reddit \"Explain Like I\u2019m Five\" forum. It contains diverse open-ended questions (often starting with \"Why/How\") and answers averaging around a few paragraphs, written to be simple and clear. The task is open-ended answer generation rather than span selection.", "size": "~270k question-answer pairs (in the open-domain version, including subquestions). A filtered high-quality subset has ~30k QA. Answers are paragraphs of length ~100 tokens.", "download": "Originally scraped from Reddit (2019). Processed versions on HuggingFace: `eli5` dataset (with train/val/test splits).", "companion": "https://arxiv.org/abs/1907.09190 (Fan et al. 2019 used ELI5 in their long-form QA paper, although dataset existed from Reddit).", "notes": "Questions often require multi-sentence explanatory answers, using general and commonsense knowledge. No single correct answer \u2013 the dataset provides one or more top-voted human answers. It's used for long-form QA evaluation. Not limited to a fixed context; often paired with a retrieval step to fetch supporting info (as in the KILT benchmark).", "tags": ["human", "QA", "atomic", "answer:human", "domain:nlp"]}