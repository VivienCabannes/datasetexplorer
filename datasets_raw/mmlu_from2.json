{
  "name": "MMLU",
  "summary": "The Massive Multitask Language Understanding (MMLU) dataset is a benchmark designed to evaluate the knowledge and problem-solving abilities of language models across 57 diverse subjects, including mathematics, history, law, and medicine.",
  "size": "Approximately 16,000 multiple-choice questions",
  "download": "https://github.com/hendrycks/test",
  "companion": "https://arxiv.org/abs/2009.03300",
  "notes": "MMLU serves as a comprehensive test for language models, assessing both world knowledge and reasoning skills. The dataset's broad subject coverage and varying difficulty levels make it a robust tool for identifying a model's strengths and weaknesses across different domains.",
  "tags": [
    "human",
    "QA",
    "benchmark",
    "answer:verifiable",
    "question:human",
    "answer:human"
  ]
}
  