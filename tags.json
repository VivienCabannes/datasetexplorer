{
    "Data Source Tags": {
      "human": "data generated by humans without AI in mind",
      "crowd-sourced": "data collected via human crowd-sourcing platforms to train AI models",
      "synthetic": "data generated by machines"
    },
    "Data Type Tags": {
      "monolithic": "continuous, unstructured blocks of text useful for pretraining",
      "dialog": "data that can be segmented into individual turns (e.g., `user, llm, python-tool, llm, user, llm, user, llm`)",
      "QA": "questionâ€“answer data",
      "tool use": "data illustrating the integration of external tools"
    },
    "Collection Process Tags": {
      "filtration": "data obtained by filtering to a large chunk of the internet",
      "compilation": "data mix that was used for training purposes",
      "atomic": "data sourced uniformly from a single, consistent origin (e.g., standardized exam sets)",
      "benchmark": "datasets originally curated for benchmarking purposes"
    },
    "QA specific tags": {
      "answer:verifiable": "answers can be verified (e.g. numerical, factual)",
      "with rationale": "answers including justifications",
      "question:synthetic": "questions generated by machineis",
      "question:human": "questions generated by humans",
      "question:crowd-sourced": "questions collected via crowd-sourcing",
      "answer:synthetic": "answers generated by machineis",
      "answer:human": "answers generated by humans",
      "answer:crowd-sourced": "answers collected via crowd-sourcing",
      "rationale:synthetic": "rationales generated by machineis",
      "rationale:human": "rationales generated by humans",
      "rationale:crowd-sourced": "rationales collected via crowd-sourcing"
    },
    "Domain Tags": {
      "mathematics": "data centered on mathematics",
      "physics": "data centered on physics",
      "programming": "data centered on programming",
      "nlp": "natural language processing data"
    }
}