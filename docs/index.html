<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>Dataset Explorer</title>
  <link rel="stylesheet" type="text/css" href="static/style.css">

  <!-- Use Katex to render math if needed -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.css" integrity="sha384-zh0CIslj+VczCZtlzBcjt5ppRcsAmDnRem7ESsYwWwg3m/OaJ2l4x7YBZl9Kxxib" crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.js" integrity="sha384-Rma6DA2IPUwhNxmrB/7S3Tno0YY7sFu9WSYMCuulLhIqYSGZ2gKCJWIqhBWqMQfh" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/contrib/auto-render.min.js" integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh" crossorigin="anonymous"
      onload="renderMathInElement(document.body);"></script>
</head>
<body>
  <div class="header">
    <h1>Dataset Explorer</h1>
    <div class="top-buttons">
      <button id="help-button" class="round-button" title="click for help">?</button>
      <button id="theme-toggle-button" class="round-button" title="toggle theme">üåô</button>
    </div>
  </div>
  
  <!-- Help text block, initially hidden -->
  <div id="help-text" style="display:none; margin: 10px 0;">
    <p>
      ‚ö†Ô∏è <strong>Under Construction:</strong> the dataset card needs to be completed.<br/><br/>

      <strong>Welcome to the Dataset Explorer!</strong><br/>
      Use the filters to narrow your dataset choices, type in the search bar to find datasets by name or summary, and sort them as needed. 
      Click on a dataset to see further details.
    </p>
  </div>
  
  <!-- Tags rendered below the help button -->
  <div id="tags-container">
    <!-- Tags will be dynamically inserted here -->
  </div>
  
  <!-- Controls row: Search bar (left) & Order menu (right) -->
  <div class="controls-row">
    <div class="search-bar">
      <input type="text" id="search-input" placeholder="Search datasets...">
    </div>
    <div class="sort-bar">
      <label for="sort-select">Sort by:</label>
      <select id="sort-select">
        <option value="name">Name (A-Z)</option>
        <option value="date_latest">Date (Latest)</option>
        <option value="date_earliest">Date (Earliest)</option>
      </select>
    </div>
  </div>
  
  <!-- Container for displaying datasets -->
  <div id="datasets-container">
    <!-- Matching datasets will be displayed here -->
  </div>

  <!-- Render mardown -->
  <script id="markdown-it" src="static/markdownit.js"></script>
  <script id="data" type="application/json">[{"companion": "https://arxiv.org/abs/1811.00937 (Talmor et al. 2019, CommonsenseQA paper)", "download": "https://www.tau-nlp.org/commonsenseqa (requires registration). Also on HuggingFace `commonsense_qa`.", "name": "CommonsenseQA", "notes": "Workers were given a concept from ConceptNet and asked to formulate a question that involves that concept in a commonsense way, and provide a correct answer and tricky distractors\u0026#8203;:contentReference[oaicite:58]{index=58}. Questions often require implicit everyday knowledge or reasoning about likely events. It\u0027s part of the GLUE-style leaderboards for commonsense reasoning.", "size": "12,247 questions, each with 5 answer choices\u0026#8203;:contentReference[oaicite:57]{index=57} (Train 9,741, Dev 1,221, Test 1,285).", "summary": "A multiple-choice question set targeting commonsense knowledge. It contains 12,247 questions that were crowd-sourced based on ConceptNet triples\u0026#8203;:contentReference[oaicite:56]{index=56}. Each question comes with 5 answer options, challenging a system\u2019s ability to use commonsense beyond text matching.", "tags": ["crowd-sourced", "QA", "atomic", "question:crowd-sourced", "answer:crowd-sourced", "domain:nlp"]}, {"companion": "The dataset is introduced in the paper \"Let\u0027s Verify Step by Step,\" accessible at [https://arxiv.org/abs/2305.20050](https://arxiv.org/abs/2305.20050).", "download": "The dataset is available for download at [https://github.com/openai/prm800k](https://github.com/openai/prm800k).", "name": "PRM800K", "notes": "PRM800K was created to support research in training more reliable models for complex multi-step reasoning tasks. It provides step-level human feedback, enabling the development of models that can assess the correctness of each reasoning step. However, some annotations have been reported to be incorrect, which may impact model training and evaluation.", "size": "The dataset comprises 800,000 annotated reasoning steps.", "summary": "PRM800K is a process supervision dataset developed by OpenAI, containing 800,000 step-level correctness labels for model-generated solutions to problems from the MATH dataset. Each solution is annotated with human feedback at each reasoning step, facilitating the training of process reward models (PRMs) that evaluate intermediate steps in problem-solving.", "tags": ["human", "QA", "compilation", "answer:verifiable", "with rationale", "question:synthetic", "answer:synthetic", "rationale:human", "mathematics"]}, {"companion": "https://arxiv.org/abs/1808.07042 (Reddy et al. 2019, CoQA paper)", "download": "Stanford CS224N GitHub (coqa-train-v1.0.json, coqa-dev-v1.0.json) or HuggingFace `coqa`.", "name": "CoQA", "notes": "Domains include children\u2019s stories, literature, middle/high school textbooks, etc. Answers can be yes/no or free text, not necessarily extractive. A highlighted evidence text is provided for each answer. CoQA evaluates F1 of answer text and a conversational Exact Match.", "size": "8,000 conversations, 127,000+ Q\u0026A pairs\u0026#8203;:contentReference[oaicite:30]{index=30}. (Train ~108k Q, Dev 7.2k Q, Test ~7.3k Q).", "summary": "Conversational Question Answering challenge with 127k questions across 8k dialogues spanning multiple domains\u0026#8203;:contentReference[oaicite:29]{index=29}. Each question is answered in free-form text (often a short phrase) along with an evidence span from the passage as rationale.", "tags": ["crowd-sourced", "dialog", "atomic", "with rationale", "answer:verifiable", "question:crowd-sourced", "answer:crowd-sourced", "rationale:human", "domain:nlp"]}, {"companion": "The dataset is accompanied by the paper \"StarCoder2 and The Stack v2: The Next Generation,\" accessible at [https://arxiv.org/abs/2402.19173](https://arxiv.org/abs/2402.19173).", "download": "Access to The Stack v2 requires an agreement with Software Heritage and INRIA. Interested parties should contact datasets@softwareheritage.org for more information.", "name": "The Stack v2", "notes": "The Stack v2 is an evolution of the original Stack dataset, significantly expanding its scope and size. It includes a diverse range of programming languages and has been curated to support the development of code-focused LLMs. The dataset is structured to facilitate research in code generation, understanding, and related tasks.", "size": "The dataset comprises approximately 5.5 billion samples, totaling around 67.5 terabytes of data.", "summary": "The Stack v2 is a comprehensive dataset of software source code, compiled by the BigCode project in collaboration with Software Heritage. It encompasses over 3 billion files across more than 600 programming and markup languages, sourced from the Software Heritage archive as of September 2023. This dataset serves as a substantial resource for training and evaluating large language models (LLMs) for code generation and understanding.", "tags": ["human", "monolithic", "compilation", "programming"]}, {"companion": "https://arxiv.org/abs/1606.05250 (Rajpurkar et al. 2016, SQuAD v1.1 paper)", "download": "https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json (training data) and dev-v1.1.json/dev set; also on HuggingFace `squad`", "name": "SQuAD1.1", "notes": "Answers are spans from the passage. Primarily used for extractive QA evaluation with exact-match and F1 scoring. No unanswerable questions in v1.1.", "size": "107,785 question-answer pairs on 536 articles\u0026#8203;:contentReference[oaicite:1]{index=1} (train: ~87k, dev: ~10k, test: ~10k); 43 MB JSON", "summary": "The Stanford Question Answering Dataset v1.1 is a reading comprehension benchmark of 100,000+ crowd-sourced questions on 536 Wikipedia articles, where each answer is a text span from the corresponding passage\u0026#8203;:contentReference[oaicite:0]{index=0}. It tests extractive QA under closed-book conditions.", "tags": ["crowd-sourced", "QA", "atomic", "answer:verifiable", "question:crowd-sourced", "answer:human", "domain:nlp"]}, {"companion": "https://arxiv.org/abs/2309.12284", "download": "https://huggingface.co/datasets/meta-math/MetaMathQA", "name": "MetaMathQA", "notes": "OpenMathInstruct is a more recent iteration of the same idea.", "size": "400,000 samples, 200 MB.", "summary": "Datasets collected by \u0027bootstrapping\u0027 gsm8k and Math"}, {"companion": "https://arxiv.org/abs/2410.07985", "download": "https://huggingface.co/datasets/KbsdJames/Omni-MATH", "name": "OmniMath", "size": "4,000, 7 MB.", "summary": "Data obtained from regional to international Olympiads, from the art-of-problem solving."}, {"companion": "none, but the datasets was key to https://arxiv.org/abs/2303.04488", "download": "https://huggingface.co/datasets/Simontwice/premise_selection_in_isabelle", "name": "Isabelle Premise Selection", "size": "4,000,000 samples", "summary": "Datasets of proofs in Isabelle collected from the Archive of Formal Proofs (https://www.isa-afp.org/). Useful to study premise selection (i.e. selecting potential lemmas to apply mid-proof)."}, {"companion": "https://arxiv.org/abs/2402.07625", "download": "https://huggingface.co/datasets/math-ai/AutoMathText", "name": "AutoMathText"}, {"companion": "https://aclanthology.org/D17-1193 (Welbl et al. 2017, SciQ paper)", "download": "AI2 Science Questions repository (SciQ) or on HuggingFace `sciq`.", "name": "SciQ", "notes": "Since workers had a reference text, the questions are often answerable with a single sentence from a science fact, and an explanation sentence is provided. It\u0027s simpler than ARC and OpenBookQA but useful for studying how explanations help. Sometimes used for data augmentation or as a source of scientific QA with rationales.", "size": "13,679 questions\u0026#8203;:contentReference[oaicite:75]{index=75}, each with 4 answer choices (one correct) and a brief explanation or supporting fact.", "summary": "A collection of 13,679 crowd-sourced multiple-choice science questions (covering physics, chemistry, biology, etc.)\u0026#8203;:contentReference[oaicite:74]{index=74}. Each question has 4 options and includes a short explanation. The questions were written by crowdworkers who were shown a factoid or science paragraph and asked to create a question.", "tags": ["crowd-sourced", "QA", "atomic", "with rationale", "question:crowd-sourced", "answer:crowd-sourced", "rationale:crowd-sourced", "domain:physics"]}, {"companion": "The dataset is introduced in the paper \"Training Verifiers to Solve Math Word Problems,\" accessible at [https://arxiv.org/abs/2110.14168](https://arxiv.org/abs/2110.14168).", "download": "The dataset is available for download at [https://github.com/openai/grade-school-math](https://github.com/openai/grade-school-math).", "name": "GSM8K (Grade School Math 8K)", "notes": "GSM8K was collaboratively developed by OpenAI and Surge AI, with human problem writers crafting each question to ensure linguistic diversity and adherence to grade-school mathematical concepts. The dataset includes both questions and detailed solutions, formatted to facilitate the training and evaluation of language models in mathematical reasoning.", "size": "8,500 problems, with 7,500 designated for training and 1,000 for testing.", "summary": "GSM8K is a dataset comprising 8,500 high-quality, linguistically diverse grade school math word problems, designed to evaluate and enhance the mathematical reasoning abilities of language models. Each problem requires between 2 to 8 steps to solve, primarily involving basic arithmetic operations such as addition, subtraction, multiplication, and division. The dataset is structured to challenge models with multi-step problem-solving, reflecting tasks that a proficient middle school student should be able to solve.", "tags": ["human", "QA", "benchmark", "answer:verifiable", "question:human", "answer:human", "mathematics"]}, {"companion": "https://arxiv.org/abs/1712.07040 (Kocisky et al. 2018, NarrativeQA paper)", "download": "Publicly available via DeepMind (narrativeqa.org) and on HuggingFace `narrative_qa`.", "name": "NarrativeQA", "notes": "Two modes: summaries and full stories. Each story has a human-written summary. Questions were authored based on the summaries, then answers were written given the full story. Tasks include answering from summary (abstractive RC) or from full story (challenging long context comprehension).", "size": "1567 documents (book or script) and 46,765 QA pairs\u0026#8203;:contentReference[oaicite:33]{index=33}. (Train 38,000, Dev 4,500, Test 4,500).", "summary": "A reading comprehension dataset focusing on long narratives (books and movie scripts). It has ~46,765 QA pairs\u0026#8203;:contentReference[oaicite:31]{index=31} derived from 1,567 stories (books or scripts)\u0026#8203;:contentReference[oaicite:32]{index=32}. Questions require understanding entire narratives; answers are free-form summaries rather than exact spans.", "tags": ["crowd-sourced", "QA", "atomic", "question:crowd-sourced", "answer:crowd-sourced", "answer:human", "domain:nlp"]}, {"companion": "https://arxiv.org/pdf/2309.17452", "download": "https://huggingface.co/datasets/AI-MO/NuminaMath-TIR", "name": "Numina-Tool", "size": "70,000 samples, 150 MB", "summary": "Subset numina problems solved with tool-use."}, {"companion": "https://arxiv.org/pdf/2105.09938", "download": "https://huggingface.co/datasets/codeparrot/apps", "name": "APPS", "notes": "AlphaCode mentions that test coverage was insufficient, leading to false positive.", "size": "10,000 problems", "summary": "10,000 programming problems with python solutions and test cases for correctness.  Curated from Codewars, AtCoder, Kattis, and Codeforces."}, {"combined_text": "The model\u0027s response, divided into \u0027thought\u0027 (the reasoning process) and \u0027solution\u0027 (the final answer).", "companion": "The dataset is accompanied by the paper \"Imitate, Explore, and Self-Improve: A Reproduction Report on Slow-thinking Reasoning Systems,\" accessible at [https://arxiv.org/abs/2412.09413](https://arxiv.org/abs/2412.09413).", "domain": "The category of the question, including fields such as mathematics, physics, chemistry, biology, coding, and puzzles.", "download": "The dataset is available for download at [https://huggingface.co/datasets/RUC-AIBOX/long_form_thought_data_5k](https://huggingface.co/datasets/RUC-AIBOX/long_form_thought_data_5k).", "name": "STILL Long Format", "notes": "STILL Long Format is part of the STILL framework, which focuses on slow-thinking reasoning systems that engage in extended reasoning processes before arriving at solutions. The dataset is structured in JSONL format, with each entry containing:", "question": "The problem statement.", "size": "The dataset contains 5,000 samples with a total storage footprint of approximately 20 MB.", "summary": "STILL Long Format is a dataset comprising 5,000 complex problems emphasizing long-form answers. The questions are sourced from diverse platforms, including NuminaMath, AIME, Leetcode, OpenCoder, Camel, Gaokao (Chinese A-level), and RiddleSense. Each problem is designed to elicit detailed, structured responses, facilitating the evaluation of models\u0027 capabilities in generating comprehensive solutions.", "tags": ["human", "QA", "compilation", "answer:verifiable", "with rationale", "question:human", "answer:human", "rationale:human", "mathematics", "programming"]}, {"companion": "https://arxiv.org/abs/2109.07958", "download": "https://github.com/sylinrl/TruthfulQA", "name": "TruthfulQA", "notes": "The dataset comprises questions crafted to test whether language models produce imitative falsehoods\u2014false answers that mimic popular misconceptions. Evaluations have shown that larger language models tend to generate more such falsehoods, highlighting challenges in ensuring model truthfulness. TruthfulQA serves as a critical tool for measuring and improving the reliability of AI-generated information.", "size": "817 questions across 38 categories", "summary": "TruthfulQA is a benchmark designed to evaluate the truthfulness of language models by assessing their ability to generate accurate answers to questions that commonly elicit false beliefs or misconceptions.", "tags": ["human", "QA", "benchmark", "answer:verifiable", "question:human", "answer:human"]}, {"companion": "https://arxiv.org/abs/1905.10044 (Clark et al. 2019, BoolQ paper)", "download": "Part of the SuperGLUE benchmark (https://super.gluebenchmark.com/tasks). Also on HuggingFace as `boolq`. License: CC BY-SA 3.0 (Wikipedia content).", "name": "BoolQ", "notes": "Each instance is a question (from search logs), a paragraph from Wikipedia, and a yes/no answer. The task requires reading comprehension and often commonsense or subtle inference. It\u2019s included in SuperGLUE. Annotators labeled the answer after reading the passage.", "size": "15,942 Q-passage examples\u0026#8203;:contentReference[oaicite:43]{index=43} (Train ~9,427, Dev ~3,270, Test ~3,245).", "summary": "Boolean Questions is a QA dataset of naturally occurring yes/no questions. It contains 15,942 examples of real Google queries (often long questions) paired with a relevant Wikipedia passage and a yes or no answer\u0026#8203;:contentReference[oaicite:42]{index=42}. The questions were not originally posed as yes/no, making them challenging.", "tags": ["human", "QA", "atomic", "answer:verifiable", "question:human", "answer:human", "domain:nlp"]}, {"companion": "https://arxiv.org/abs/2007.00398", "download": "https://rrc.cvc.uab.es/?ch=17", "name": "DocVQA", "notes": "DocVQA focuses on assessing models\u0027 abilities to understand and extract information from various document types, including forms, tables, and figures. The dataset presents a significant challenge, with a notable performance gap between existing models and human accuracy, highlighting the need for improved document understanding in VQA systems.", "size": "50,000 questions on 12,000+ document images", "summary": "DocVQA is a dataset designed to evaluate Visual Question Answering (VQA) models on document images, comprising 50,000 questions over 12,000+ document images from the UCSF Industry Documents Library.", "tags": ["human", "QA", "benchmark", "answer:verifiable", "question:human", "answer:human", "nlp"]}, {"companion": "The dataset is introduced in the paper \"Omni-MATH: A Universal Olympiad Level Mathematic Benchmark For Large Language Models,\" accessible at [https://arxiv.org/abs/2410.07985](https://arxiv.org/abs/2410.07985).", "download": "The dataset is available for download at [https://huggingface.co/datasets/KbsdJames/Omni-MATH](https://huggingface.co/datasets/KbsdJames/Omni-MATH).", "name": "Omni-MATH", "notes": "Omni-MATH was developed to address the limitations of existing mathematical benchmarks, which have become less challenging for advanced LLMs. By focusing on Olympiad-level problems, the dataset provides a rigorous assessment tool for evaluating and improving the mathematical reasoning abilities of LLMs. Experimental results indicate that even state-of-the-art models struggle with these problems, highlighting significant challenges in this domain.", "size": "4,428 problems, approximately 7 MB.", "summary": "Omni-MATH is a comprehensive and challenging benchmark designed to assess large language models\u0027 (LLMs) mathematical reasoning capabilities at the Olympiad level. The dataset comprises 4,428 competition-level problems sourced from various regional to international Olympiads, curated from the Art of Problem Solving platform. It focuses exclusively on mathematics, encompassing over 33 sub-domains with diverse difficulty levels, aiming to push the boundaries of LLMs in complex mathematical reasoning.", "tags": ["human", "QA", "atomic", "answer:verifiable", "question:human", "answer:human", "mathematics"]}, {"companion": "The dataset is introduced in the paper \"CAMEL: Communicative Agents for \u0027Mind\u0027 Exploration of Large Scale Language Model Society,\" accessible at [https://arxiv.org/abs/2303.17760](https://arxiv.org/abs/2303.17760).", "download": "The dataset is available for download at [https://huggingface.co/datasets/camel-ai/physics](https://huggingface.co/datasets/camel-ai/physics).", "name": "Camel", "notes": "The problems and solutions in the Camel dataset are synthetically generated by GPT-4 and have not undergone verification for correctness. Users should exercise caution and independently verify the accuracy of the content before utilizing it for research or educational purposes.", "size": "20,000 problem-solution pairs.", "summary": "Camel is a synthetic dataset comprising 20,000 physics problem-solution pairs generated using GPT-4. The dataset covers 25 physics topics, each with 25 subtopics, and includes 32 problems for each topic-subtopic pair. It was developed to facilitate research in automated problem-solving and educational applications within the field of physics.", "tags": ["synthetic", "physics", "problem-solving", "GPT-4", "education"]}, {"companion": "https://arxiv.org/abs/2112.09332 (WebGPT paper by Nakano et al. 2021)", "download": "OpenAI has not fully open-sourced it, but a portion is available via the WebGPT paper\u2019s data release (or OpenAI Evals). Not on HuggingFace officially due to web content licenses.", "name": "WebGPT QA dataset", "notes": "Questions are long-form and often open-ended. The provided answers include citations to specific web articles, aiming to be factually grounded. The dataset\u2019s goal is to encourage truthful, evidence-backed answers. It\u2019s used in evaluating how models can browse and cite. Since the data is limited, it\u2019s primarily an evaluation set for research.", "size": "Approximately 4,398 Q\u0026A with references (WebGPT\u2019s high-quality human reference answers for validation). The exact number includes 1,473 train, 419 test questions from ELI5 and similar distributions.", "summary": "A dataset of question-answer pairs with supporting evidence, derived from the WebGPT project (OpenAI, 2021). Questions are taken from Reddit (ELI5 and others) and answered by human demonstrators who use a web browser to find information. Each answer is a paragraph with references (extracted quotes). This was used to train WebGPT and as a testbed for truthful QA with sources.", "tags": ["human", "QA", "tool use", "with rationale", "answer:verifiable", "question:human", "answer:human", "rationale:human", "domain:nlp"]}, {"companion": "https://arxiv.org/abs/2412.09413", "download": "https://huggingface.co/datasets/RUC-AIBOX/long_form_thought_data_5k", "name": "Still long format", "size": "5,000 samples, 20 MB.", "summary": "Similar to Still, with a focus on long answer, questions come from NuminaMath, Aime, Leetcode, OpenCoder, Camel, Gaokao (Chinese A-level) and RiddleSense. # Formal Math"}, {"companion": "The dataset is introduced in the paper \"Measuring Mathematical Problem Solving With the MATH Dataset,\" accessible at [https://arxiv.org/abs/2103.03874](https://arxiv.org/abs/2103.03874).", "download": "The AMPS dataset is available for download through the MATH dataset GitHub repository. The repository provides dataset loaders and evaluation code, facilitating access to both the MATH and AMPS datasets.", "name": "AMPS (Auxiliary Mathematics Problems and Solutions)", "notes": "AMPS serves as a valuable resource for training intelligent tutoring systems (ITS) and machine learning models in mathematical reasoning. The diverse coverage of mathematical topics allows for the development of models capable of addressing a wide range of problem types and difficulty levels. The inclusion of step-by-step solutions aids in teaching models not only to arrive at correct answers but also to generate explanatory reasoning. Researchers and educators can utilize this dataset to enhance the mathematical problem-solving capabilities of AI systems and to develop educational tools for personalized learning experiences. ", "size": "Over 100,000 Khan Academy problems and approximately 5 million Mathematica-generated problems, totaling over 23 GB of data.", "summary": "AMPS is a comprehensive dataset designed to support the pretraining of models in mathematical problem-solving. It comprises over 100,000 problems sourced from Khan Academy, each accompanied by step-by-step solutions formatted in LaTeX. Additionally, the dataset includes approximately 5 million problems generated through manually designed Mathematica scripts, covering a wide array of mathematical topics such as algebra, calculus, geometry, linear algebra, and number theory. The Khan Academy subset encompasses 693 exercise types, ranging from elementary mathematics to advanced topics like multivariable calculus, reflecting the curriculum used to teach K-12 students. The Mathematica-generated problems are based on 100 hand-designed modules, producing around 50,000 exercises per script, resulting in a total of over 23 GB of mathematical problems and solutions.", "tags": ["mathematics", "Khan Academy", "Mathematica", "problem-solving", "step-by-step solutions", "algebra", "calculus", "geometry", "linear algebra", "number theory"]}, {"companion": "The dataset is introduced in the paper \"OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text,\" accessible at [https://arxiv.org/abs/2310.06786](https://arxiv.org/abs/2310.06786).", "download": "The dataset is available for download at [https://huggingface.co/datasets/open-web-math/open-web-math](https://huggingface.co/datasets/open-web-math/open-web-math).", "name": "OpenWebMath", "notes": "OpenWebMath was curated by filtering the Common Crawl dataset to include only English documents containing mathematical content of high quality. The filtering process involved prefiltering HTML documents, extracting text (including LaTeX content), classifying and filtering content, deduplicating using SimHash, and manual inspection to remove low-quality pages.", "size": "The dataset comprises 6.3 million samples, totaling approximately 27 GB.", "summary": "OpenWebMath is a dataset containing 6.3 million documents of high-quality mathematical text extracted from over 200 billion HTML files in the Common Crawl dataset. It encompasses approximately 14.7 billion tokens and is intended for use in pretraining and finetuning large language models.", "tags": ["human", "monolithic", "filtration", "mathematics"]}, {"companion": "The dataset is introduced and utilized in the paper \"SmolLM2: When Smol Goes Big -- Data-Centric Training of a Small Language Model,\" accessible at [https://arxiv.org/abs/2502.02737](https://arxiv.org/abs/2502.02737).", "download": "The dataset is available for download at [https://huggingface.co/datasets/HuggingFaceTB/stack-edu](https://huggingface.co/datasets/HuggingFaceTB/stack-edu).", "name": "Stack-Edu", "notes": "Due to its substantial size, downloading Stack-Edu requires the use of Amazon S3. Despite its size, the dataset offers a significant number of tokens, making it valuable for training language models. The creation of Stack-Edu involved fine-tuning classifiers on code files annotated by Llama3.1-70B-Instruct to assess their educational quality. Each classifier was trained on a specific programming language to ensure precise evaluation.", "size": "The dataset comprises approximately 167 million samples, totaling around 1 terabyte of data.", "summary": "Stack-Edu is a curated subset of The Stack v2 dataset, focusing on code files with high educational value across 15 programming languages. It was developed to enhance the training of small language models, particularly SmolLM2, by providing high-quality, educational code examples. The dataset was created using classifiers fine-tuned on code files annotated by Llama3.1-70B-Instruct, scoring the educational value of each file.", "tags": ["human", "monolithic", "filtration", "programming"]}, {"companion": "https://arxiv.org/abs/2203.10244", "download": "https://github.com/vis-nlp/ChartQA", "name": "ChartQA", "notes": "ChartQA addresses the need for complex reasoning over chart data by including questions that require arithmetic operations, logical inference, and visual feature interpretation. The dataset is sourced from various online platforms to ensure diversity in chart styles and topics. It serves as a challenging benchmark for developing models capable of understanding and reasoning about chart-based information.", "size": "32,719 questions across 20,882 charts", "summary": "ChartQA is a large-scale benchmark dataset designed to evaluate question-answering systems on charts, emphasizing both visual and logical reasoning. It comprises 9,608 human-written questions and 23,111 questions generated from human-written chart summaries, covering a diverse set of 20,882 real-world charts.", "tags": ["human", "QA", "benchmark", "answer:verifiable", "question:human", "answer:human", "mathematics"]}, {"companion": "The dataset is introduced in the paper \"Measuring Mathematical Problem Solving With the MATH Dataset,\" accessible at [https://arxiv.org/abs/2103.03874](https://arxiv.org/abs/2103.03874). ", "download": "The dataset was previously available at [https://github.com/hendrycks/math](https://github.com/hendrycks/math). However, it has been removed from certain platforms, including Hugging Face, due to copyright issues raised by the Art of Problem Solving. ", "name": "MATH", "notes": "MATH serves as a benchmark for evaluating the mathematical problem-solving abilities of machine learning models. Despite advancements in language models, performance on this dataset remains relatively low, indicating the complexity of the problems and the need for further research in mathematical reasoning. ", "size": "12,500 problems with detailed solutions. ", "summary": "MATH is a dataset comprising 12,500 challenging mathematics competition problems, primarily sourced from American high school contests such as the American Mathematics Competitions (AMC 10 and AMC 12) and the American Invitational Mathematics Examination (AIME). Each problem is accompanied by a detailed step-by-step solution, facilitating the training and evaluation of models in mathematical reasoning and problem-solving. The problems span various topics, including algebra, geometry, number theory, and calculus, and are categorized by difficulty levels ranging from 1 to 5. ", "tags": ["human", "QA", "benchmark", "answer:verifiable", "question:human", "answer:human", "mathematics"]}, {"companion": "https://arxiv.org/abs/2406.11794", "download": "https://huggingface.co/datasets/mlfoundations/dclm-baseline-1.0", "name": "DataComp-LM (DCLM)", "size": "4T token", "summary": "Filtering of Common Crawl based on heuristic cleaning and filtering (see RefinedWeb), deduplication (through Bloom), filtering with fastText classifier to match the reddit channel ExplainLikeImFive, and an instruct model."}, {"companion": "The dataset is accompanied by the paper \"Imitate, Explore, and Self-Improve: A Reproduction Report on Slow-thinking Reasoning Systems,\" accessible at [https://arxiv.org/abs/2412.09413](https://arxiv.org/abs/2412.09413).", "download": "The dataset is available for download at [https://huggingface.co/datasets/RUC-AIBOX/STILL-3-Preview-RL-Data](https://huggingface.co/datasets/RUC-AIBOX/STILL-3-Preview-RL-Data). ", "name": "STILL-3 Preview RL Data", "notes": "STILL-3 serves as a resource for training and evaluating models in mathematical reasoning tasks. It includes problems from various reputable sources, ensuring a diverse range of mathematical challenges. The dataset is part of the STILL framework, which focuses on slow-thinking reasoning systems that engage in extended reasoning processes before arriving at solutions.", "size": "The dataset contains 30,000 samples with a total storage footprint of approximately 10 MB.", "summary": "STILL-3 is a dataset comprising 30,000 mathematical problems sourced from collections such as Math, Numina, and AIME. Each problem is paired with verifiable answers, facilitating the evaluation of reasoning models. This dataset is designed to support reinforcement learning approaches in mathematical problem-solving. ", "tags": ["human", "QA", "compilation", "answer:verifiable", "question:human", "answer:human", "mathematics"]}, {"companion": "https://arxiv.org/abs/1811.00207 (Rashkin et al. 2019, EmpatheticDialogues paper)", "download": "Facebook AI released it; available via ParlAI and on HuggingFace `empathetic_dialogues`.", "name": "EmpatheticDialogues", "notes": "Each dialog is relatively short (one person shares, the other responds, maybe a few back-and-forths). The dataset includes the emotion label (e.g., sad, angry) for the situation and the conversation. It\u2019s used to train chatbots that better recognize and respond to user emotions. Not QA; model quality is measured by how well it aligns with human empathetic responses.", "size": "24,850 dialogues, ~50k utterances (each dialogue has 2 speakers and about 2 turns each on average). 32 emotion labels overall.", "summary": "A dataset of 24,850 one-on-one open-domain conversations grounded in emotional situations\u0026#8203;:contentReference[oaicite:94]{index=94}. Each conversation starts with a speaker describing a personal situation that evokes a specific emotion, then the conversation continues with an empathetic listener. The goal is to teach models to respond with empathy.", "tags": ["crowd-sourced", "dialog", "atomic", "domain:nlp"]}, {"companion": "The dataset is introduced in the paper \"The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale,\" accessible at [https://arxiv.org/abs/2406.17557](https://arxiv.org/abs/2406.17557).", "download": "The dataset is available for download at [https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu).", "name": "FineWeb-Edu", "notes": "FineWeb-Edu was developed by filtering the FineWeb dataset using an educational quality classifier. This classifier was trained on 450,000 web samples annotated by Llama 3, scoring content on a scale from 0 to 5 based on educational value. The resulting dataset emphasizes high-scoring educational texts, aiming to enhance the training of language models in educational and knowledge-intensive tasks.", "size": "Approximately 1.3 trillion tokens.", "summary": "FineWeb-Edu is a curated dataset comprising approximately 1.3 trillion tokens of high-quality educational content, extracted from the larger FineWeb dataset. Utilizing an educational quality classifier trained on annotations generated by Llama 3, this dataset focuses on texts with significant educational value. Models pretrained on FineWeb-Edu have demonstrated enhanced performance on knowledge-intensive benchmarks, such as MMLU and ARC, compared to those trained on broader datasets like FineWeb.", "tags": ["web", "compilation", "filtration", "CommonCrawl", "LLM training", "educational content"]}, {"companion": "https://arxiv.org/abs/1904.09728 (Sap et al. 2019, SocialIQA paper)", "download": "Data available via AI2 (social-iqa dataset on GitHub) and on HuggingFace `social_i_qa`.", "name": "SocialIQA", "notes": "Built on ATOMIC knowledge graph events. Question types include \"What will X do next?\", \"Why did X do that?\", etc. It tests understanding of social dynamics and emotions. Models need to infer beyond the literal text. It\u2019s commonly used to evaluate commonsense reasoning in language models.", "size": "37,000+ questions with 3-choice answers\u0026#8203;:contentReference[oaicite:60]{index=60} (Train ~33k, Dev 1.1k, Test 1.2k). Also 1,924 unique narrative contexts.", "summary": "A benchmark for social commonsense reasoning. It consists of 38,000 multiple choice questions about everyday social interactions\u0026#8203;:contentReference[oaicite:59]{index=59}. Each question focuses on motivations, reactions, or feelings of people in a short context scenario, with 3 answer options.", "tags": ["crowd-sourced", "QA", "atomic", "question:crowd-sourced", "answer:crowd-sourced", "domain:nlp"]}, {"companion": "https://arxiv.org/abs/1611.09268 (Nguyen et al. 2016, MS MARCO paper)", "download": "https://microsoft.github.io/msmarco/ (registration required); also available on the MS MARCO leaderboard site and on HuggingFace (`ms_marco` for passages and QA).", "name": "MS MARCO QA", "notes": "Answers are often a segment from a passage or a human-written summary of multiple passages. Used for passage ranking and reading comprehension. It includes yes/no and unanswerable questions as well.", "size": "~1,010,916 query-passages pairs (approx 8.8M passages)\u0026#8203;:contentReference[oaicite:16]{index=16}; 182,669 questions with manually generated answers\u0026#8203;:contentReference[oaicite:17]{index=17}. The v2.1 training set has ~808k queries.", "summary": "A large-scale machine reading comprehension dataset from Bing search. It contains 1,010,916 real anonymized queries, each with a set of retrieved passages and human-generated answers\u0026#8203;:contentReference[oaicite:13]{index=13}. About 182k questions have well-formed answers, the rest have extracted answers\u0026#8203;:contentReference[oaicite:14]{index=14}\u0026#8203;:contentReference[oaicite:15]{index=15}.", "tags": ["human", "QA", "compilation", "answer:verifiable", "question:human", "answer:crowd-sourced", "domain:nlp"]}, {"companion": "https://arxiv.org/abs/1611.09830 (Trischler et al. 2017, NewsQA paper)", "download": "https://github.com/Maluuba/NewsQA (contains the dataset in JSON, under license CC BY-NC-SA 3.0). Also on HuggingFace as `newsqa`.", "name": "NewsQA", "notes": "Answers are spans in the article (or occasionally unanswerable). The questions often require inference or combining information, aiming to be more challenging than surface word matching.", "size": "~119,633 questions collected, with ~100k answerable\u0026#8203;:contentReference[oaicite:20]{index=20} (train: 92k, dev: 5k, test: 5k). Data size ~200 MB.", "summary": "A QA dataset of questions on news articles from CNN, created via crowdworkers. It has ~120k question-answer pairs\u0026#8203;:contentReference[oaicite:18]{index=18}. Workers were shown an article\u2019s summary and asked to write questions, then another set found the answers in the article\u0026#8203;:contentReference[oaicite:19]{index=19}.", "tags": ["crowd-sourced", "QA", "atomic", "answer:verifiable", "question:crowd-sourced", "answer:crowd-sourced", "domain:nlp"]}, {"companion": "https://www.aclweb.org/anthology/D15-1237 (Yang et al. 2015, WikiQA paper)", "download": "Microsoft Research release (WikiQA corpus on Microsoft site and Kaggle). Also on HuggingFace as `wiki_qa`.", "name": "WikiQA", "notes": "All questions are real user questions (mostly factoid) originally intended to be answered by Wikipedia. Roughly 40% of questions have no correct answer in the provided sentences (labeled none). Often used to evaluate answer sentence selection models. It\u2019s notably smaller than other QA sets.", "size": "3,047 questions, 29,258 sentences in total\u0026#8203;:contentReference[oaicite:55]{index=55}. On average ~9.6 candidate sentences per question; 1 or more marked correct answers (1.5 on avg for answerable questions).", "summary": "A small open-domain QA dataset of question and sentence pairs from Bing query logs (2015). It has 3,047 factoid questions, each with a set of candidate sentences from Wikipedia, among which ~1,473 are labeled as answer sentences\u0026#8203;:contentReference[oaicite:54]{index=54}. The task is usually answer sentence selection (find which sentence contains the answer).", "tags": ["human", "QA", "filtration", "answer:verifiable", "question:human", "answer:human", "domain:nlp"]}, {"companion": "The dataset is introduced in the paper \"A Lean Dataset for International Math Olympiad: Small Steps towards Writing Math Proofs for Hard Problems,\" accessible at [https://arxiv.org/abs/2411.18872](https://arxiv.org/abs/2411.18872).", "download": "The dataset is available for download at [https://huggingface.co/datasets/roozbeh-yz/IMO-Steps](https://huggingface.co/datasets/roozbeh-yz/IMO-Steps).", "name": "IMO-Steps", "notes": "In addition to the formal proofs, the dataset includes a decomposition of these proofs into 1,329 lemmas, totaling over 40,000 lines of Lean code. These lemmas serve as building blocks for the formal proofs, offering approachable challenges for AI models and facilitating the evaluation and diagnosis of their performance in automated theorem proving.", "size": "Approximately 17 formalized IMO problems, totaling around 5,880 lines of Lean code.", "summary": "IMO-Steps is a meticulously curated dataset comprising formal proofs of International Mathematical Olympiad (IMO) problems, formalized in Lean 4. This collection includes complete, original formal proofs for 14 IMO problems from the miniF2F benchmark that previously lacked formal proofs, along with 3 additional problems from IMO 2022 and 2023. The dataset encompasses approximately 5,880 lines of Lean code, providing a valuable resource for developing and evaluating AI models in automated theorem proving.", "tags": ["human", "compilation", "theorem proving", "Lean", "mathematics"]}, {"download": "The dataset is available for download at [https://huggingface.co/datasets/MatrixStudio/Codeforces-Python-Submissions](https://huggingface.co/datasets/MatrixStudio/Codeforces-Python-Submissions).", "name": "Codeforces-Python-Submissions", "notes": "The dataset is extensive and includes unit tests for many problems, enabling automated evaluation of code submissions. It is particularly useful for research in code synthesis, analysis of programming techniques, and the development of educational tools for programming instruction. Users should be aware of the licensing terms associated with the dataset, which are specified on the download page.", "size": "Approximately 690,000 samples, totaling around 1.7 GB.", "summary": "The Codeforces-Python-Submissions dataset is a comprehensive collection of Python programming solutions submitted to the Codeforces platform, a renowned competitive programming website. This dataset encompasses a wide array of problem-solving instances, including problem statements, user submissions, and corresponding metadata. It serves as a valuable resource for analyzing coding patterns, developing machine learning models for code generation and evaluation, and studying competitive programming methodologies. Notably, the dataset includes unit tests for many of the problems, facilitating automated assessment of solution correctness.", "tags": ["competitive programming", "Python", "Codeforces", "programming puzzles", "unit tests"]}, {"companion": "https://arxiv.org/abs/1810.12885 (Zhang et al. 2018, ReCoRD paper)", "download": "Part of the SuperGLUE benchmark (super.gluebenchmark.com). Also on HuggingFace `record`.", "name": "ReCoRD", "notes": "It\u2019s formatted as a blank in a sentence (cloze). The correct answer is an entity from the passage that makes the statement true. Many questions require using commonsense or implied relationships, not just text matching. Scoring is by token-level F1 and Exact Match. Derived from CNN/DailyMail articles but focuses on commonsense connections.", "size": "120,000+ queries from ~70,000 news articles\u0026#8203;:contentReference[oaicite:52]{index=52}. (Train ~101k, Dev 10k, Test 10k). Each query has a small set of answer candidates (entities from the passage).", "summary": "Reading Comprehension with Commonsense Reasoning Dataset is a cloze-style MRC task with over 120k queries from ~70k news articles\u0026#8203;:contentReference[oaicite:51]{index=51}. Each query is formed by masking out a noun phrase in a sentence from the article, and the model must select the correct entity from the passage to fill in, using both the passage and broader commonsense.", "tags": ["human", "QA", "filtration", "with rationale", "answer:verifiable", "question:synthetic", "answer:human", "domain:nlp"]}, {"companion": "https://arxiv.org/abs/2109.00110", "download": "https://github.com/facebookresearch/miniF2F", "name": "MiniF2F", "size": "500 examples with Lean formal statements and informal statements and proofs.", "summary": "MiniF2F is a benchmark for formal mathematics, created by Kunhao, consisting of 500 Olympiad-level mathematics problems from competitions like AIME, AMC, and IMO. Each problem is provided with both informal and formal statements. # Compilation"}, {"AlgebraicStack": "11B tokens of mathematical code, including numerical computing, computer algebra, and formal mathematics.", "OpenWebMath": "15B tokens from the OpenWebMath dataset, containing high-quality mathematical text from the internet.", "arXiv": "29B tokens from the ArXiv subset of RedPajama.", "companion": "The dataset is introduced in the paper \"Llemma: An Open Language Model For Mathematics,\" accessible at [https://arxiv.org/abs/2310.10631](https://arxiv.org/abs/2310.10631).", "download": "The dataset is available for download at [https://huggingface.co/datasets/EleutherAI/proof-pile-2](https://huggingface.co/datasets/EleutherAI/proof-pile-2).", "name": "Proof-Pile-2", "notes": "Proof-Pile-2 serves as a substantial resource for training language models in mathematical reasoning and related tasks. It includes diverse sources of mathematical content, ensuring a comprehensive representation of formal mathematics. The dataset is structured to facilitate research in mathematical language processing and theorem proving.", "size": "The dataset contains approximately 55 billion tokens.", "summary": "Proof-Pile-2 is a 55-billion-token dataset of mathematical and scientific documents. It was created to train models like Llemma 7B and Llemma 34B. The dataset comprises three subsets:", "tags": ["human", "monolithic", "compilation", "mathematics"]}, {"companion": "https://arxiv.org/abs/2310.06770", "download": "https://github.com/swe-bench/SWE-bench", "name": "SWE-bench", "size": "2,300 samples", "summary": "Datasets of codebase, issues and unit tests. The goal is to fix the codebase that currently yield the issue resulting in failing unit tests. The data was collected from popular Github repository."}, {"name": "Stack V2", "size": "5,500,000,000 samples, 67 TB", "summary": "Scrapping of the Software Heritage archive, which contains software source code, in September 2023."}, {"download": "https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k", "name": "OpenThought"}, {"companion": "https://arxiv.org/abs/2305.12524", "download": "https://huggingface.co/datasets/TIGER-Lab/TheoremQA", "name": "TheoremQA", "notes": "Curated by domain experts, TheoremQA encompasses a diverse range of theorems, such as Taylor\u0027s theorem, Lagrange\u0027s theorem, Huffman coding, and Quantum Theorem. The dataset serves as a rigorous benchmark to assess large language models\u0027 capabilities in applying theoretical knowledge to solve challenging science problems. Evaluations have shown that models like GPT-4 achieve up to 51% accuracy with Program-of-Thoughts Prompting, while other open-source models perform below 15%, indicating the dataset\u0027s challenging nature.", "size": "800 high-quality questions covering over 350 theorems", "summary": "TheoremQA is a benchmark dataset designed to evaluate AI models\u0027 abilities to apply STEM theorems to solve complex, university-level problems across mathematics, physics, electrical engineering and computer science (EE\u0026CS), and finance.", "tags": ["human", "QA", "benchmark", "answer:verifiable", "question:human", "answer:human", "mathematics", "physics", "programming"]}, {"companion": "https://arxiv.org/abs/1803.05457 (Clark et al. 2018, ARC paper)", "download": "AI2 ARC dataset on AI2 website or AllenAI/ARC on HuggingFace.", "name": "AI2 ARC (Easy \u0026 Challenge)", "notes": "Each question also comes with a corpus of science facts (14M sentences) for retrieval. The challenge set is especially difficult for NLP models. This dataset spurred research in text retrieval + inference for QA. It\u2019s included in the AI2 Leaderboard and used in many evaluations of model knowledge.", "size": "7,787 questions total (ARC-Easy 5,228; ARC-Challenge 2,559)\u0026#8203;:contentReference[oaicite:69]{index=69}.", "summary": "The AI2 Reasoning Challenge is a set of grade-school science exam questions. It includes 7,787 multiple-choice questions (4 options) covering science topics from elementary and middle school levels\u0026#8203;:contentReference[oaicite:68]{index=68}. Two subsets: ARC-Easy (simple questions answerable with a single fact) and ARC-Challenge (hard questions requiring inference or multiple facts).", "tags": ["compilation", "QA", "benchmark", "question:human", "answer:human", "domain:physics"]}, {"companion": "https://arxiv.org/abs/2502.13124", "download": "https://huggingface.co/datasets/facebook/natural_reasoning", "name": "Natural Reasoning", "summary": "From Fair RAM group, questions filtered from DCLM and FineMath, with answers provided by Llama."}, {"companion": "https://arxiv.org/abs/2210.17517", "download": "https://huggingface.co/datasets/allenai/lila", "name": "Lila", "notes": "The datasets were collected for evaluation, it seems small and outdated.", "summary": "Compilation of many datasets including addsub, amps, apps, asdiv, conala, mathematics, dolphin, draw, gsm8k, math, mathqa, mbpp, mctaco, multiarith, numersense, numglus, simuleq, singleop, singleq, svamp."}, {"companion": "The dataset is introduced in the paper \"Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems,\" accessible at [https://arxiv.org/pdf/1705.04146](https://arxiv.org/pdf/1705.04146).", "download": "The dataset is available for download at [https://huggingface.co/datasets/deepmind/aqua_rat](https://huggingface.co/datasets/deepmind/aqua_rat).", "name": "AQuA (Algebra Question Answering with Rationales)", "notes": "AQuA provides a rich resource for training and evaluating models on algebraic problem-solving and rationale generation. The inclusion of both correct answers and detailed rationales allows for the development of models that not only arrive at the correct solution but also provide explanatory reasoning. Researchers should note that while the dataset is extensive, the quality of web-sourced rationales may vary, and additional verification may be necessary for certain applications.", "size": "Approximately 100,000 samples, totaling around 52 MB.", "summary": "AQuA is a large-scale dataset developed by DeepMind, consisting of approximately 100,000 algebraic word problems. Each problem is presented as a multiple-choice question with five possible answers, accompanied by a natural language rationale explaining the solution. The dataset was constructed by extracting around 34,000 questions from undergraduate and graduate admission tests such as the GMAT and GRE, supplemented with answers and rationales sourced from the web. Additionally, crowdsourcing methods were employed to generate similar questions, enhancing the dataset\u0027s diversity and comprehensiveness.", "tags": ["algebra", "word problems", "rationale generation", "multiple-choice", "DeepMind", "GMAT", "GRE", "crowdsourcing"]}, {"companion": "The dataset is introduced in the paper \"Llemma: An Open Language Model For Mathematics,\" accessible at [https://arxiv.org/abs/2310.10631](https://arxiv.org/abs/2310.10631).", "download": "The dataset is available as part of the Proof-Pile-2 collection at [https://huggingface.co/datasets/EleutherAI/proof-pile-2](https://huggingface.co/datasets/EleutherAI/proof-pile-2).", "name": "Algebraic Stack", "notes": "Algebraic Stack was instrumental in training the Llemma models, which are designed to enhance mathematical reasoning capabilities in language models. The dataset\u0027s composition includes a significant portion of Python code (approximately 6.1 billion tokens), reflecting its prevalent use in mathematical computing. Additionally, it contains substantial amounts of code from languages like C++, Isabelle, Lean, and Julia, among others. Researchers and developers can utilize this dataset to advance the development of AI systems capable of engaging with complex mathematical concepts and formal proofs.", "size": "Approximately 11 billion tokens.", "summary": "Algebraic Stack is a specialized subset of the Proof-Pile-2 dataset, comprising approximately 11 billion tokens of mathematical code. It was curated to support the development and training of language models in mathematical and scientific domains. The dataset includes source code from 17 programming languages, focusing on numerical computing, computer algebra, and formal mathematics. Notably, it encompasses data from Coq, Isabelle, Lean, and MATLAB, as well as extracted proof states from Lean\u0027s Mathlib 4 library and a collection of Isabelle proofs. This diverse compilation aims to enhance language models\u0027 proficiency in understanding and generating mathematical code and proofs.", "tags": ["mathematical code", "formal mathematics", "Proof-Pile-2", "Llemma", "Coq", "Isabelle", "Lean", "MATLAB", "Python", "numerical computing", "computer algebra"]}, {"companion": "https://arxiv.org/abs/2311.12022", "download": "https://github.com/idavidrein/gpqa", "name": "GPQA", "notes": "GPQA\u0027s questions are crafted by domain experts to be exceptionally difficult, with experts achieving 65% accuracy and highly skilled non-experts only 34%, despite unrestricted web access and extensive time per question. The dataset is \u0027Google-proof,\u0027 making it a robust tool for assessing advanced reasoning and knowledge in language models. Notably, as of March 2024, Claude 3 Opus achieved approximately 60% accuracy on this benchmark, highlighting rapid advancements in AI capabilities.", "size": "448 multiple-choice questions", "summary": "The Graduate-Level Google-Proof Q\u0026A (GPQA) is a benchmark dataset designed to evaluate the capabilities of language models and scalable oversight mechanisms through 448 challenging multiple-choice questions in biology, physics, and chemistry.", "tags": ["human", "QA", "benchmark", "answer:verifiable", "question:human", "answer:human", "mathematics", "physics"]}, {"companion": "https://arxiv.org/abs/2406.01574", "download": "https://huggingface.co/datasets/TIGER-Lab/MMLU-Pro", "name": "MMLU-Pro", "notes": "MMLU-Pro introduces more challenging questions requiring deeper reasoning, expanding the answer choices per question to ten to reduce the likelihood of random guessing. This enhancement aims to provide a more discriminative benchmark, with evaluations showing a significant drop in accuracy compared to the original MMLU, thereby offering a more rigorous assessment of language models\u0027 capabilities.", "size": "Approximately 12,000 multiple-choice questions across 14 disciplines", "summary": "MMLU-Pro is an enhanced version of the Massive Multitask Language Understanding (MMLU) benchmark, designed to more rigorously evaluate large language models\u0027 comprehension and reasoning abilities across diverse subjects by incorporating more complex, reasoning-focused questions and increasing the number of answer choices from four to ten.", "tags": ["human", "QA", "benchmark", "answer:verifiable", "question:human", "answer:human", "mathematics", "physics", "programming", "nlp"]}, {"companion": "https://arxiv.org/abs/1704.05179 (Dunn et al. 2017, SearchQA paper)", "download": "Available on the authors\u2019 GitHub (Dunn et al. 2017) and on HuggingFace as `search_qa`.", "name": "SearchQA", "notes": "Questions are challenging trivia (from Jeopardy). The provided snippets (around 50 per question) come from a search engine and contain the answer, but often scattered, requiring multi-sentence comprehension or filtering out distractors.", "size": "Approximately 139,000 question-answer pairs\u0026#8203;:contentReference[oaicite:23]{index=23}. Each question has on average 49.6 retrieved snippets of supporting text\u0026#8203;:contentReference[oaicite:24]{index=24}.", "summary": "A dataset of 140k open-domain factoid questions sourced from Jeopardy! clues, each paired with text snippets retrieved via Google Search\u0026#8203;:contentReference[oaicite:21]{index=21}\u0026#8203;:contentReference[oaicite:22]{index=22}. The task is to find the answer from the provided snippets.", "tags": ["compilation", "QA", "filtration", "answer:verifiable", "question:human", "answer:human", "domain:nlp"]}, {"AIME (American Invitational Mathematics Examination)": "Problems spanning from 1984 to 2023.", "AMC (American Mathematics Competitions)": "Problems from years prior to 2023.", "Omni-MATH dataset": "A compilation of diverse mathematical problems.", "Still dataset": "A collection of mathematical problems from various competitions.", "companion": "The dataset and its application are discussed in the article \"DeepScaleR: Surpassing O1-Preview with a 1.5B Model by Scaling RL,\" accessible at [https://medium.com/data-science-in-your-pocket/deepseek-r1-5b-new-deepseek-r1-model-tops-openai-o1-in-math-18dacfd04f1f](https://medium.com/data-science-in-your-pocket/deepseek-r1-5b-new-deepseek-r1-model-tops-openai-o1-in-math-18dacfd04f1f).", "download": "The dataset is not publicly available as a standalone resource. However, it was utilized in training the DeepScaleR-1.5B-Preview model, whose details can be found at [https://huggingface.co/agentica-org/DeepScaleR-1.5B-Preview](https://huggingface.co/agentica-org/DeepScaleR-1.5B-Preview).", "name": "DeepScaleR", "notes": "The DeepScaleR dataset was meticulously compiled to support reinforcement learning approaches in training language models for mathematical reasoning tasks. The inclusion of problems from AIME, AMC, Omni-MATH, and Still datasets ensures a diverse and challenging set of mathematical questions, facilitating the development of models capable of advanced problem-solving.", "size": "Approximately 40,000 unique problem-answer pairs.", "summary": "DeepScaleR is a curated dataset comprising approximately 40,000 unique problem-answer pairs, specifically designed to enhance the mathematical reasoning capabilities of language models. The dataset aggregates problems from several reputable sources:", "tags": ["mathematics", "reasoning", "reinforcement learning", "AIME", "AMC", "Omni-MATH", "Still"]}, {"download": "https://github.com/LiveCodeBench/LiveCodeBench", "name": "LiveCodeBench"}, {"Coding": "Duplicates were removed to maintain uniqueness.", "Mathematics": "Advanced reasoning models like Qwen-QwQ were utilized to filter out unsolvable problems, unmatchable questions, or those with incorrect answers. Multiple-choice questions were reformatted into open-ended questions.", "companion": "The dataset is associated with the paper \"Process Reinforcement through Implicit Rewards,\" accessible at [https://arxiv.org/abs/2502.01456](https://arxiv.org/abs/2502.01456).", "download": "The dataset is available for download at [https://huggingface.co/datasets/PRIME-RL/Eurus-2-RL-Data](https://huggingface.co/datasets/PRIME-RL/Eurus-2-RL-Data).", "name": "Eurus-RL", "notes": "Eurus-RL includes multiple datasets with corresponding training recipes, facilitating research in reinforcement learning for complex reasoning tasks. The dataset\u0027s construction involved rigorous validation processes to ensure the correctness of solutions, making it suitable for training models that require verifiable outcomes.", "size": "Approximately 480,537 samples, totaling around 2 GB.", "summary": "Eurus-RL is a high-quality reinforcement learning (RL) training dataset comprising approximately 480,537 samples of mathematics and coding problems, each accompanied by verifiable outcomes. The dataset aggregates problems from several sources:", "tags": ["human", "compilation", "filtration", "mathematics", "coding", "reinforcement learning"]}, {"download": "Various formatted datasets based on historical AIME questions are available, e.g., [on HuggingFace](https://huggingface.co/datasets/di-zhang-fdu/AIME_1983_2024)", "name": "AIME", "notes": "Since AIME is an annual examination, new data becomes available each year. However, designers may select questions already present on the internet, which could be included in pretraining corpora.", "size": "15 questions per year", "summary": "US high-school competition, American Invitational Mathematics Examination.", "tags": ["atomic", "benchmark", "QA", "question:human", "answer:human"]}, {"companion": "https://arxiv.org/abs/2108.07732", "download": "https://github.com/google-research/google-research/tree/master/mbpp", "name": "MBPP (Mostly Basic Python Problems)", "size": "1,000 samples", "summary": "Crowd-sourced datasets of small Python problems A dataset consisting of 1,000 Python programming problems aimed at entry-level programmers."}, {"companion": "The dataset is accompanied by the paper \"TACO: Topics in Algorithmic COde generation dataset,\" accessible at [https://arxiv.org/abs/2312.14852](https://arxiv.org/abs/2312.14852).", "download": "The dataset is available for download at [https://huggingface.co/datasets/BAAI/TACO](https://huggingface.co/datasets/BAAI/TACO).", "name": "TACO: Topics in Algorithmic Code Generation", "notes": "TACO provides fine-grained labels for each problem, including task topics, algorithms, programming skills, and difficulty levels, offering precise references for training and evaluating code generation models. The dataset emphasizes Python 3 solutions due to Python\u0027s restrictive syntactic rules, leading to cleaner and more discernible code structures.", "size": "The dataset comprises 26,443 problems and 1,539,152 verified Python 3 code solutions. The training set contains 25,443 problems, while the test set includes 1,000 problems. On average, each problem features 58.21 correct code solutions.", "summary": "TACO is an open-source, large-scale code generation dataset focused on algorithmic problems. It aggregates 26,443 programming challenges from platforms like CodeChef, CodeForces, HackerRank, GeeksforGeeks, and existing datasets such as APPS, CodeContest, and Description2code. Each problem is accompanied by unit tests to ensure solution correctness.", "tags": ["human", "QA", "compilation", "answer:verifiable", "with rationale", "question:human", "answer:human", "rationale:human", "programming"]}, {"companion": "https://arxiv.org/abs/2502.00203", "download": "https://huggingface.co/datasets/nvidia/Llama-Nemotron-Post-Training-Dataset-v1", "name": "Nemotron Post-Training Dataset v1", "notes": "Developed to fine-tune models for improved accuracy and reliability.", "size": "15,000,000 samples, 6 GB.", "summary": "We do not have too many details on this datasets, it seems to have been curated from Llama, Qwen and DeepSeek answers."}, {"companion": "https://crfm.stanford.edu/2023/03/13/alpaca.html (Stanford CRFM blog announcing Alpaca 2023)", "download": "Released on GitHub (stanford_alpaca) with data in JSON. Also available on HuggingFace (`tatsu-lab/alpaca`).", "name": "Stanford Alpaca (52k Instruction Dataset)", "notes": "Source prompts were based on Self-Instruct techniques. The data covers many domains and formats (e.g., brainstorming, Q\u0026A, classification). This dataset is widely used to fine-tune LLaMA and other models to be chatty and follow user instructions. It is synthetic (GPT outputs as answers) so quality is high but it may contain subtle biases from the base model.", "size": "52,000 instruction-response pairs. (No official train/test split; it\u2019s mainly for fine-tuning). ~80 MB JSON.", "summary": "A synthetic instruction-following dataset created by Stanford by prompting OpenAI\u2019s text-davinci-003 (GPT) with 175 human-written seed tasks. It contains 52,000 diverse instructions and GPT-generated responses, covering tasks like writing, transformation, closed QA, code, etc. It aims to replicate the behavior of InstructGPT in an open dataset.", "tags": ["synthetic", "monolithic", "compilation", "question:synthetic", "answer:synthetic", "domain:nlp"]}, {"companion": "https://people.ict.usc.edu/~gordon/publications/Chambers-etal-IJCAI2011.pdf (COPA original paper, 2011)", "download": "Part of the SuperGLUE benchmark (available via SuperGLUE datasets). Also on HuggingFace `copa`.", "name": "COPA", "notes": "Each COPA item is either asking for a cause or an effect (question is like \"What was the CAUSE?\" or \"What happened as a RESULT?\"). This was a popular small test for commonsense and was included in SuperGLUE. Human performance is ~100%. The dataset is balanced to avoid superficial cues.", "size": "1,000 questions (500 development, 500 test)\u0026#8203;:contentReference[oaicite:67]{index=67}; each has 2 answer options (cause or effect).", "summary": "Choice of Plausible Alternatives is a focused evaluation of commonsense causal reasoning. 1,000 questions are presented as premises with two possible continuations: either a likely cause or effect of the premise, and the model chooses the more plausible one\u0026#8203;:contentReference[oaicite:66]{index=66}. For example: \"The man broke his toe. What was the cause? (A) He got a hole in his sock. (B) He dropped a hammer on his foot.\"", "tags": ["human", "QA", "atomic", "question:human", "answer:human", "domain:nlp"]}, {"companion": "https://arxiv.org/abs/2107.03374", "download": "https://github.com/openai/human-eval", "name": "HumanEval", "notes": "Each problem in the HumanEval dataset includes a function signature, a docstring specifying the intended functionality, and a set of unit tests to verify correctness. The dataset emphasizes functional correctness over mere syntactic accuracy, making it a robust benchmark for code generation models. Notably, it has been used to evaluate models like OpenAI\u0027s Codex, which achieved a 28.8% pass rate on these problems.", "size": "164 programming problems", "summary": "HumanEval is a benchmark dataset developed by OpenAI to evaluate the functional correctness of code generated by language models. It comprises 164 hand-crafted Python programming problems, each designed to assess a model\u0027s ability to synthesize programs from natural language descriptions.", "tags": ["human", "QA", "benchmark", "answer:verifiable", "question:human", "answer:human", "programming"]}, {"download": "https://huggingface.co/datasets/EleutherAI/proof-pile-2 contains data scrapped from the ArXiv website, according to a snapshot taken in 2023 by RedPajama.", "name": "ArXiv", "notes": "The dataset is valuable for training models on advanced mathematical concepts and research-level problems.", "size": "29B tokens", "summary": ""}, {"companion": "https://arxiv.org/abs/2305.01210", "download": "https://github.com/evalplus/evalplus", "name": "HumanEval+", "notes": "HumanEval+ addresses the limitations of the original HumanEval dataset by augmenting the number of test cases approximately 80-fold. This enhancement aims to reduce the occurrence of false positives in model evaluations, offering a more stringent assessment of functional correctness in generated code.", "size": "164 programming problems with over 1.3 million test cases", "summary": "An enhanced version of the original HumanEval dataset, HumanEval+ significantly increases test coverage to provide a more rigorous evaluation framework for code generation models.", "tags": ["code generation", "benchmark", "functional correctness", "test augmentation", "model evaluation"]}, {"chat": "Approximately 39,800 samples.", "code": "Approximately 1.56 million samples.", "companion": "The dataset is associated with NVIDIA\u0027s Llama Nemotron Collection, detailed at [https://huggingface.co/collections/nvidia/llama-nemotron-67d92346030a2691293f200b](https://huggingface.co/collections/nvidia/llama-nemotron-67d92346030a2691293f200b). ", "download": "The dataset is available for download at [https://huggingface.co/datasets/nvidia/Llama-Nemotron-Post-Training-Dataset-v1](https://huggingface.co/datasets/nvidia/Llama-Nemotron-Post-Training-Dataset-v1).", "math": "Approximately 13.1 million samples.", "name": "Llama-Nemotron-Post-Training-Dataset-v1", "notes": "The dataset encompasses multiple subsets categorized into five main splits:", "safety": "Approximately 31,400 samples.", "science": "Approximately 484,000 samples.", "size": "Approximately 15 million samples, totaling around 6 GB.", "summary": "Llama-Nemotron-Post-Training-Dataset-v1 is a comprehensive dataset developed by NVIDIA to enhance the post-training of large language models (LLMs). It comprises approximately 15 million samples across various domains, including code, mathematics, science, chat, and safety. The dataset is designed to improve the accuracy and reliability of LLMs by providing diverse and high-quality training data.", "tags": ["synthetic", "compilation", "programming", "mathematics", "physics", "nlp"]}, {"companion": "https://arxiv.org/abs/2103.03874", "download": "https://huggingface.co/datasets/HuggingFaceH4/MATH-500", "name": "MATH-500", "notes": "The MATH-500 dataset is utilized to assess and benchmark the performance of language models in mathematical reasoning and problem-solving. It includes problems of varying difficulty levels, providing a comprehensive evaluation framework. Notably, models like DeepSeek R1 and OpenAI\u0027s o3-mini have achieved high accuracy scores on this benchmark, indicating its role in advancing mathematical reasoning capabilities in AI systems.", "size": "500 problems", "summary": "MATH-500 is a curated subset of 500 problems from the original MATH benchmark, designed to evaluate mathematical problem-solving abilities of language models across various topics such as algebra, geometry, and calculus.", "tags": ["human", "QA", "benchmark", "answer:verifiable", "question:human", "answer:human", "mathematics"]}, {"companion": "https://arxiv.org/abs/2502.02737", "download": "https://huggingface.co/datasets/HuggingFaceTB/stack-edu", "name": "Stack-Edu", "notes": "Need to be download with S3, still really big in terms of number of tokens.", "size": "167,000,000 samples, ~1 TB", "summary": "Filtering version of the Stack V2. # LLM Augmented Source Data being synthesized with LLMs. The LLM can be used to synthesize texts or questions (usually providing example of questions from existing datasets). It can also be used to synthesize a rationale to answer questions. The answer may be verified, either with parser checking for numerical equality (i.e. `\\frac13` = `0.333`), or using LLM as a judge. Synthesized questions (bootstrapped from existing one), synthesized, eventually verified, answers."}, {"companion": "https://arxiv.org/abs/2101.02235 (Geva et al. 2021, StrategyQA paper)", "download": "The authors\u2019 GitHub (StrategyQA data) and on HuggingFace `strategy_qa`.", "name": "StrategyQA", "notes": "Unique in that the question itself doesn\u2019t reveal the needed decomposition \u2013 the reasoning steps are implicit. E.g., \"Could Brooke Shields have been friends with Marilyn Monroe?\" (Answer: No, explanation: Monroe died before Shields was born). It tests creative multi-hop reasoning and is often used in evaluating explanation-generation (the provided rationales).", "size": "2,290 questions (Train ~1,696, Dev 212, Test 382). All answers are Yes/No. Each comes with a reference explanation (a series of facts).", "summary": "A recent QA dataset of 2,290 examples for implicit multi-hop reasoning (\"strategy questions\"). Each question is a yes/no question that requires gathering and composing multiple pieces of information, and an explanation is provided for each answer\u0026#8203;:contentReference[oaicite:73]{index=73}.", "tags": ["crowd-sourced", "QA", "atomic", "with rationale", "answer:verifiable", "question:crowd-sourced", "answer:crowd-sourced", "rationale:human", "domain:nlp"]}, {"companion": "(No single paper; context from community projects like Vicuna: https://arxiv.org/abs/2304.08994 cites using ShareGPT data)", "download": "Unofficial community dumps on HuggingFace (`sharegpt-90k` as JSON). OpenAI policy restricts official distribution, but data was shared publicly by users.", "name": "ShareGPT Conversations", "notes": "Conversations include user prompts and ChatGPT responses. The source is real usage, so content is diverse. The quality of assistant responses is generally high (from ChatGPT), though there are inaccuracies. This data has been pivotal in training open-source chat assistants like Vicuna. Legal/ToS aspects are a gray area since the data comes from user-submitted logs.", "size": "~90,000 conversations (after cleaning), with an average of ~10-15 turns each (some much longer). The dataset size is roughly 100MB+ of text.", "summary": "A crowdsourced collection of dialogues between users and ChatGPT, contributed by users via ShareGPT. It contains tens of thousands of multi-turn chat transcripts covering various topics and user queries (from casual conversation to complex questions and tasks). This dataset is used to fine-tune chat models to better mimic ChatGPT\u2019s style and capabilities.", "tags": ["human", "dialog", "compilation", "question:human", "answer:synthetic", "domain:nlp"]}, {"AMC 10": "For students in grades 10 and below.", "AMC 12": "For students in grades 12 and below.", "AMC 8": "For students in grades 8 and below.", "download": "Various formatted datasets based on historical AMC questions are available. For instance, the [AI-MO/aimo-validation-amc](https://huggingface.co/datasets/AI-MO/aimo-validation-amc) dataset on Hugging Face provides a curated set of AMC problems, particularly from recent years. Additionally, the [MATH dataset](https://huggingface.co/datasets/qwedsacf/competition_math) includes problems from AMC 10, AMC 12, AIME, and other competitions, each accompanied by step-by-step solutions.", "name": "AMC (American Mathematics Competitions)", "notes": "The AMC serves as a pre-selection competition for the AIME, acting as a gateway for students aspiring to participate in higher-level mathematical olympiads. The problems are designed to promote deep mathematical thinking and problem-solving skills. Researchers and educators often utilize AMC problems to develop and evaluate mathematical models and to prepare students for advanced mathematical studies.", "size": "Each AMC competition level (AMC 10 and AMC 12) administers 25 multiple-choice questions per year. Over the years, this has resulted in a substantial collection of problems across various mathematical topics and difficulty levels.", "summary": "The American Mathematics Competitions (AMC) are a series of examinations and curriculum materials designed to strengthen the mathematical capabilities of students from middle school through high school. Serving as the first in a series of competitions that lead to the International Mathematical Olympiad (IMO), the AMC challenges students with engaging problems that promote the development of problem-solving skills and mathematical reasoning. The competitions are divided into different levels:", "tags": ["mathematics competitions", "high school mathematics", "problem-solving", "AMC 10", "AMC 12", "AIME", "USAMO", "mathematical reasoning"]}, {"amc_aime": "American Mathematics Competitions and AIME problems.", "aops_forum": "Problems from the Art of Problem Solving community.", "cn_k12": "Chinese high school math exercises.", "download": "The dataset is available for download at [https://huggingface.co/datasets/AI-MO/NuminaMath-1.5](https://huggingface.co/datasets/AI-MO/NuminaMath-1.5).", "gsm8k": "Grade School Math problems.", "math": "Various math problems.", "name": "NuminaMath 1.5", "notes": "NuminaMath 1.5 introduces several enhancements over its predecessor, including the addition of problem metadata for verifiability and the inclusion of manually curated data from various contests. Notably, synthetic datasets like `synthetic_amc` were removed due to their negative impact on performance.", "olympiads": "International mathematics olympiad competition problems.", "orca_math": "Problems from the Orca-Math dataset.", "size": "Approximately 900,000 samples, totaling 531 MB.", "summary": "NuminaMath 1.5 is the second iteration of the NuminaMath dataset, comprising approximately 900,000 competition-level math problems. Each problem is accompanied by a solution formatted in a Chain of Thought (CoT) manner. The dataset sources include:", "synthetic_math": "Synthetic math problems.", "tags": ["human", "QA", "compilation", "answer:verifiable", "with rationale", "question:human", "answer:human", "rationale:human", "mathematics"]}, {"companion": "The dataset is accompanied by the paper \"SWE-bench: Can Language Models Resolve Real-World GitHub Issues?\" accessible at [https://arxiv.org/abs/2310.06770](https://arxiv.org/abs/2310.06770).", "download": "The dataset is available for download at [https://github.com/swe-bench/SWE-bench](https://github.com/swe-bench/SWE-bench).", "name": "SWE-bench", "notes": "SWE-bench includes subsets such as SWE-bench Lite, a curated selection of 300 instances designed for more accessible evaluation, and SWE-bench Verified, a human-validated subset of 500 problems confirmed to be solvable by experienced software engineers. These subsets aim to facilitate more efficient and reliable assessments of LLMs in software engineering tasks.", "size": "The dataset contains 2,294 issue-pull request pairs from 12 popular Python repositories.", "summary": "SWE-bench is a benchmark designed to evaluate large language models\u0027 (LLMs) capabilities in resolving real-world software engineering problems. It comprises 2,294 issues and corresponding pull requests sourced from 12 popular open-source Python repositories on GitHub. Each issue is paired with the codebase state at the time of the issue and the associated pull request that resolved it. The benchmark challenges models to generate patches that address the described issues, with success determined by the ability to pass unit tests that failed prior to the patch and pass after its application.", "tags": ["human", "QA", "compilation", "answer:verifiable", "with rationale", "question:human", "answer:human", "rationale:human", "programming"]}, {"companion": "https://arxiv.org/abs/2107.03374 (Chen et al. 2021, Evaluating Large LM Code Generation - introduces HumanEval)", "download": "OpenAI\u2019s HumanEval is included in the openai/human-eval GitHub. Also on HuggingFace `openai_humaneval`.", "name": "OpenAI HumanEval", "notes": "Each task is relatively short (write a function to do X) but non-trivial. The primary metric is the percentage of problems passed (all tests pass). It has become a standard metric in code modeling research (often referenced as \"HumanEval score\"). Solutions require reasoning about algorithms, and sometimes simple math or string manipulation.", "size": "164 programming problems with associated unit tests\u0026#8203;:contentReference[oaicite:84]{index=84}. (No train/test split; all are for evaluation.)", "summary": "A benchmark of 164 handcrafted coding problems for evaluating code generation by language models\u0026#8203;:contentReference[oaicite:83]{index=83}. Each problem includes a function signature, a docstring describing the task, and some hidden tests. The model must produce a correct implementation in Python. It\u0027s used to measure functional correctness of generated code.", "tags": ["human", "QA", "benchmark", "tool use", "question:human", "answer:human", "domain:programming"]}, {"companion": "While there isn\u0027t a dedicated companion paper, the dataset was instrumental in the research presented in \"Magnushammer: A Transformer-based Approach to Premise Selection,\" accessible at [https://arxiv.org/abs/2303.04488](https://arxiv.org/abs/2303.04488).", "download": "The dataset is available for download at [https://huggingface.co/datasets/Simontwice/premise_selection_in_isabelle](https://huggingface.co/datasets/Simontwice/premise_selection_in_isabelle).", "name": "Isabelle Premise Selection", "notes": "The dataset includes premises from both manually crafted proofs and those generated via Isabelle\u0027s Sledgehammer tool, which employs symbolic methods for automatic premise selection. This amalgamation enhances the dataset\u0027s diversity, making it a valuable asset for training models in premise selection tasks.", "size": "Over 4 million samples.", "summary": "The Isabelle Premise Selection dataset is a comprehensive collection of over 4 million aligned pairs of proof contexts and relevant premises, meticulously extracted from the Archive of Formal Proofs (AFP) and Isabelle\u0027s Standard Library. Designed to facilitate research in premise selection\u2014a critical task in automated theorem proving\u2014this dataset aids in identifying the most pertinent lemmas or theorems applicable at various stages of a proof. By encompassing both original proofs and those automatically generated using Isabelle\u0027s Sledgehammer tool, the dataset offers a diverse and rich resource for developing and evaluating models in automated reasoning and information retrieval.", "tags": ["human", "compilation", "filtration", "theorem proving", "Isabelle"]}, {"companion": "The dataset is introduced in the paper \"Autonomous Data Selection with Zero-shot Generative Classifiers for Mathematical Texts,\" accessible at [https://arxiv.org/abs/2402.07625](https://arxiv.org/abs/2402.07625).", "download": "The dataset is available for download at [https://huggingface.co/datasets/math-ai/AutoMathText](https://huggingface.co/datasets/math-ai/AutoMathText).", "name": "AutoMathText", "notes": "AutoMathText was developed to enhance language models\u0027 proficiency in mathematical reasoning through continual pretraining. The dataset\u0027s autonomous selection process leverages base language models as zero-shot generative classifiers to evaluate and select high-quality mathematical content without human annotations. Empirical evaluations demonstrated that language models continually pretrained on AutoMathText achieved substantial improvements on mathematical benchmarks, underscoring the dataset\u0027s efficacy in enhancing mathematical reasoning capabilities.", "size": "Approximately 200 GB of text data.", "summary": "AutoMathText is a meticulously curated dataset encompassing approximately 200 GB of mathematical texts. It aggregates content from diverse platforms, including websites, arXiv, and GitHub repositories such as OpenWebMath, RedPajama, and Algebraic Stack. The dataset was autonomously selected and labeled by the Qwen-72B language model, assigning each piece of content a score (`lm_q1q2_score`) ranging from 0 to 1, reflecting its relevance, quality, and educational value in mathematical intelligence.", "tags": ["mathematics", "language model pretraining", "autonomous data selection", "Qwen-72B", "OpenWebMath", "RedPajama", "Algebraic Stack"]}, {"companion": "The dataset is introduced in the paper \"DeepSeek-Prover: Advancing Theorem Proving in LLMs through Large-Scale Synthetic Data,\" accessible at [https://arxiv.org/abs/2405.14333](https://arxiv.org/abs/2405.14333).", "download": "The dataset is available for download at [https://huggingface.co/datasets/deepseek-ai/DeepSeek-Prover-V1](https://huggingface.co/datasets/deepseek-ai/DeepSeek-Prover-V1).", "name": "DeepSeek-Prover-V1", "notes": "The dataset was generated through a process that involved translating natural language problems into formal statements, filtering out low-quality statements, and generating proofs to create synthetic data. After fine-tuning the DeepSeekMath 7B model on this dataset, the model achieved whole-proof generation accuracies of 46.3% with 64 samples and 52% cumulatively on the Lean 4 miniF2F test, surpassing the baseline GPT-4 at 23.0% with 64 samples and a tree search reinforcement learning method at 41.0%. Additionally, the model successfully proved 5 out of 148 problems in the Lean 4 Formalized International Mathematical Olympiad (FIMO) benchmark, while GPT-4 failed to prove any.", "size": "Approximately 8 million samples, totaling around 6 MB.", "summary": "DeepSeek-Prover-V1 is a synthetic dataset comprising approximately 8 million formal mathematical statements paired with their corresponding proofs, generated by DeepSeek. The dataset focuses on formalizing and proving problems from high-school and undergraduate-level mathematical competitions, including those found in the miniF2F benchmark. The formal proofs are constructed in Lean 4, a modern proof assistant language. This dataset aims to advance the capabilities of large language models (LLMs) in formal theorem proving by providing a substantial corpus of formalized mathematics.", "tags": ["synthetic", "theorem proving", "Lean 4", "mathematics", "formal proofs"]}, {"companion": "https://arxiv.org/abs/2103.03874 (Hendrycks et al. 2021, MATH dataset NeurIPS paper)", "download": "From the official GitHub (hendrycks/math) or on HuggingFace `hendrycks_math`.", "name": "MATH (Competition Math Problems)", "notes": "The solutions are often several paragraphs of LaTeX describing the reasoning. This dataset evaluates advanced mathematical problem solving. Performance is measured by exact match on the final answer. It\u2019s extremely challenging: GPT-3 and similar models have low accuracy, making it a key benchmark for program-of-thought or tool-use techniques.", "size": "12,500 problems (7,500 train, 500 val, 5,000 test)\u0026#8203;:contentReference[oaicite:78]{index=78}, each with a detailed solution and an answer.", "summary": "A dataset of 12,500 difficult high school mathematics competition problems with full step-by-step solutions\u0026#8203;:contentReference[oaicite:77]{index=77}. Problems range in topics (algebra, geometry, calculus) and require written mathematical reasoning. Each problem\u2019s final answer is typically a short number or expression, and a detailed solution (rationale) is provided.", "tags": ["compilation", "QA", "benchmark", "with rationale", "question:human", "answer:human", "rationale:human", "domain:mathematics"]}, {"companion": "The dataset is introduced in the paper \"LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code,\" accessible at [https://arxiv.org/abs/2403.07974](https://arxiv.org/abs/2403.07974). ", "download": "The dataset is available for download at [https://github.com/LiveCodeBench/LiveCodeBench](https://github.com/LiveCodeBench/LiveCodeBench).", "name": "LiveCodeBench", "notes": "LiveCodeBench addresses the issue of test set contamination by continuously updating its problem set with new challenges from recent coding competitions. This approach ensures that the benchmark remains relevant and that models are evaluated on problems they have not been exposed to during training. ", "size": "As of January 2025, LiveCodeBench hosts 880 high-quality coding problems that were published between May 2023 and January 2025.", "summary": "LiveCodeBench is a comprehensive and contamination-free benchmark designed to evaluate the coding capabilities of large language models (LLMs). It continuously collects new problems from coding competition platforms such as LeetCode, AtCoder, and CodeForces, ensuring an up-to-date and challenging problem set. Beyond code generation, LiveCodeBench assesses a broader range of code-related capabilities, including self-repair, code execution, and test output prediction.", "tags": ["human", "QA", "benchmark", "answer:verifiable", "question:human", "answer:human", "programming"]}, {"companion": "https://arxiv.org/abs/2311.12022", "download": "https://github.com/idavidrein/gpqa", "name": "GPQA Diamond", "notes": "The Diamond subset represents the most difficult questions within the GPQA benchmark, designed to be \u0027Google-proof\u0027 and requiring deep subject matter expertise. Expert validators achieved 65% accuracy, while highly skilled non-experts reached only 34%, despite unrestricted web access and extended time per question. This subset serves as a rigorous test for assessing advanced reasoning and knowledge in language models.", "size": "198 multiple-choice questions", "summary": "The GPQA Diamond subset is a collection of 198 exceptionally challenging multiple-choice questions from the Graduate-Level Google-Proof Q\u0026A (GPQA) benchmark, focusing on advanced topics in biology, physics, and chemistry.", "tags": ["human", "QA", "benchmark", "answer:verifiable", "question:human", "answer:human", "mathematics", "physics"]}, {"companion": "The dataset is introduced in the paper \"NaturalReasoning: Reasoning in the Wild with 2.8M Challenging Questions,\" accessible at [https://arxiv.org/abs/2502.13124](https://arxiv.org/abs/2502.13124).", "download": "The dataset is available for download at [https://huggingface.co/datasets/facebook/natural_reasoning](https://huggingface.co/datasets/facebook/natural_reasoning).", "name": "NaturalReasoning", "notes": "A 1.1 million subset of NaturalReasoning has been released to the research community to foster research on training robust large language model reasoners.  The dataset\u0027s construction involved back-translating questions from pretraining corpora, ensuring a diverse and challenging set of reasoning tasks.", "size": "Approximately 2.8 million questions.", "summary": "NaturalReasoning is a large-scale dataset developed by Meta\u0027s Fundamental AI Research (FAIR) group, comprising 2.8 million challenging reasoning questions across diverse domains, including STEM fields (e.g., Physics, Computer Science), Economics, and Social Sciences. The dataset was constructed by back-translating content from pretraining corpora such as DCLM and FineMath. To ensure quality, the questions were deduplicated and decontaminated from popular reasoning benchmarks, including MATH, GPQA, MMLU-Pro, and MMLU-STEM. Each question includes a reference final answer extracted from the original document when available, and a model-generated response from Llama3.3-70B-Instruct.", "tags": ["synthetic", "QA", "compilation", "answer:verifiable", "question:synthetic", "answer:synthetic", "mathematics", "physics", "economics", "social sciences"]}, {"companion": "https://arxiv.org/abs/2411.18872", "download": "https://huggingface.co/datasets/roozbeh-yz/IMO-Steps", "name": "IMO-Steps", "size": "20 samples, 6 kB", "summary": "DOWNLOADED IN `/checkpoint/amaia/explore/datasets/reasoning/raw` 20 Lean proofs of IMO problems"}, {"companion": "The dataset is introduced in the paper \"The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale,\" accessible at [https://arxiv.org/abs/2406.17557](https://arxiv.org/abs/2406.17557). ", "download": "The dataset is available for download at [https://huggingface.co/datasets/HuggingFaceFW/fineweb](https://huggingface.co/datasets/HuggingFaceFW/fineweb).", "name": "FineWeb", "notes": "FineWeb was initially intended as an open replication of RefinedWeb. Through additional filtering steps, it has achieved enhanced performance, surpassing its predecessors.  However, subsequent datasets like DCLM have reported further improvements over FineWeb.  Users should consider these developments when selecting a dataset for training LLMs.", "size": "Over 15 trillion tokens, approximately 44 terabytes of disk space.", "summary": "FineWeb is a large-scale dataset developed by Hugging Face, comprising over 15 trillion tokens of cleaned and deduplicated English web data sourced from 96 Common Crawl snapshots. Designed to optimize the training of large language models (LLMs), FineWeb incorporates advanced filtering techniques to enhance data quality. Models trained on FineWeb have demonstrated superior performance compared to those trained on other publicly available datasets, such as C4, Dolma-v1.6, The Pile, SlimPajama, and RedPajama2.", "tags": ["web", "compilation", "filtration", "CommonCrawl", "LLM training"]}, {"companion": "The dataset is introduced in the paper \"OpenMathInstruct-2: Accelerating AI for Math with Massive Open-Source Instruction Data,\" accessible at [https://arxiv.org/abs/2410.01560](https://arxiv.org/abs/2410.01560).", "download": "The dataset is available for download at [https://huggingface.co/datasets/nvidia/OpenMathInstruct-2](https://huggingface.co/datasets/nvidia/OpenMathInstruct-2).", "name": "OpenMathInstruct-2", "notes": "OpenMathInstruct-2 was developed to address the challenges in mathematical reasoning within large language models. Through careful ablation experiments on data synthesis using the Llama3.1 models, the dataset was constructed to improve solution formats, leverage strong teacher models, and ensure question diversity. Fine-tuning models with OpenMathInstruct-2 has shown significant performance improvements on benchmarks like MATH.", "size": "The dataset contains approximately 14 million samples, totaling around 12 GB.", "summary": "OpenMathInstruct-2 is a large-scale synthetic dataset comprising approximately 14 million question-solution pairs, derived from around 600,000 unique mathematical questions. The dataset was created by augmenting existing benchmarks, such as MATH and GSM8K, using the Llama3.1 family of models to generate solutions. This augmentation aimed to enhance the training of large language models in mathematical reasoning tasks.", "tags": ["synthetic", "QA", "compilation", "answer:verifiable", "with rationale", "question:synthetic", "answer:synthetic", "rationale:synthetic", "mathematics"]}, {"download": "https://huggingface.co/datasets/PRIME-RL/Eurus-2-RL-Data", "name": "Eurus-RL", "notes": "Includes multiple datasets with corresponding training recipes.", "size": "500,000 samples, 2 GB", "summary": "Collection of question with verifiable answer extracted from Numina, Apps, CodeContests, Taco and Codeforces. https://arxiv.org/abs/2502.01456"}, {"companion": "https://aclanthology.org/P19-1443 (Saeidi et al. 2018, ShARC paper)", "download": "Data on GitHub: https://sharc-data.github.io (ShARC v1.0 JSON). Also on HuggingFace as `sharc`.", "name": "ShARC", "notes": "Unique for its conversational logic: The system must infer if the user\u2019s scenario satisfies conditions in the rule text. The answer can be \"Yes\", \"No\", or a follow-up question to ask for missing info. Dialogues are tree-structured. Annotations include the underlying rule text (snippet) and scenario (user background info).", "size": "~32,000 QA instances (including follow-ups) from 948 dialog trees. Train ~22.5k, Dev ~2.5k, Test ~7k instances.", "summary": "Shaping Answers with Rules through Conversation is a conversational QA dataset (32k instances) about understanding rule texts (like government policies). It involves an initial question and possibly follow-up Q\u0026A turns to clarify conditions before giving a final yes/no/inquire answer. It has 948 distinct scenarios (snippet + user question) and ~32k QA turns\u0026#8203;:contentReference[oaicite:53]{index=53}.", "tags": ["crowd-sourced", "dialog", "atomic", "with rationale", "question:crowd-sourced", "answer:crowd-sourced", "rationale:human", "domain:nlp"]}, {"auto_math_text": "Utilizes samples from the AutoMathText dataset to bolster scientific knowledge.", "companion": "Detailed information about the dataset and its creation process is discussed in the blog post \"Cosmopedia: how to create large-scale synthetic data for pre-training,\" accessible at [https://huggingface.co/blog/cosmopedia](https://huggingface.co/blog/cosmopedia).", "download": "The dataset is available for download at [https://huggingface.co/datasets/HuggingFaceTB/cosmopedia](https://huggingface.co/datasets/HuggingFaceTB/cosmopedia).", "khanacademy": "Based on course outlines from Khan Academy.", "name": "Cosmepedia", "notes": "Cosmepedia was developed to address the challenges of creating large-scale synthetic datasets for language model pre-training. The dataset\u0027s creation involved meticulous prompt engineering to ensure diversity and minimize duplication. The synthetic data generation process aimed to cover a broad spectrum of topics, providing a comprehensive resource for training language models. Additionally, subsets like **stories** were included to enhance commonsense and day-to-day knowledge aspects within the dataset.", "openstax": "Content from OpenStax course outlines.", "size": "Over 30 million documents, totaling approximately 25 billion tokens.", "stanford": "Based on course outlines from Stanford University.", "stories": "Generated stories to enhance commonsense knowledge.", "summary": "Cosmepedia is a synthetic dataset comprising over 30 million documents, totaling approximately 25 billion tokens. The dataset includes various forms of content such as textbooks, blog posts, stories, and WikiHow articles, all generated by the Mixtral-8x7B-Instruct-v0.1 model. The primary objective of Cosmepedia is to facilitate the pre-training of language models by providing a diverse and extensive collection of synthetic data. The dataset encompasses a wide range of topics, aiming to replicate the breadth of knowledge found in web datasets like RefinedWeb and RedPajama. It is organized into eight distinct subsets, each corresponding to different source materials used as seed data for generation:", "tags": ["synthetic", "textbooks", "blog posts", "stories", "WikiHow", "language model pre-training"], "web_samples_v1": "Derived from web data similar to RefinedWeb.", "web_samples_v2": "Similar to v1 with refined prompts for enhanced depth.", "wikihow": "Articles generated from scraped WikiHow titles."}, {"companion": "https://aclanthology.org/D13-1160 (EMNLP 2013 paper introducing WebQuestions)", "download": "Original data from the authors\u2019 site (Berant et al. 2013) or via HuggingFace `web_questions` (which has 6,642 examples version).", "name": "WebQuestions", "notes": "Answers are Freebase entries (or lists of entities). The dataset is commonly used for semantic parsing to knowledge base (with the WebQuestionsSP variant augmenting questions with SPARQL). Domain is broad open trivia.", "size": "5,810 question-answer pairs (original)\u0026#8203;:contentReference[oaicite:11]{index=11}; processed version WebQuestionsSP has 6,642 QA pairs\u0026#8203;:contentReference[oaicite:12]{index=12} with annotated SPARQL queries.", "summary": "An open-domain QA set of questions from Google Suggest queries, curated to be answerable using the Freebase knowledge graph. It has about 5.8K questions with crowdsourced answers (entities in Freebase)\u0026#8203;:contentReference[oaicite:10]{index=10}. Often used to evaluate KB-QA or open QA.", "tags": ["human", "QA", "atomic", "answer:verifiable", "question:human", "answer:crowd-sourced", "domain:nlp"]}, {"companion": "https://arxiv.org/abs/1905.09381", "download": "https://github.com/princeton-vl/CoqGym", "name": "CoqGym", "size": "70,000 proof steps", "summary": "Large-scale dataset compiled from various 71,000 Coq projects."}, {"companion": "https://arxiv.org/abs/1806.03822 (Rajpurkar et al. 2018, SQuAD2.0 paper)", "download": "https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json (train) and dev-v2.0.json; also available via `squad_v2` on HuggingFace", "name": "SQuAD2.0", "notes": "Includes unanswerable questions requiring models to predict a special null response. Leaderboard evaluates combined accuracy on answerable and rejection of unanswerable questions.", "size": "Approximately 152,500 Q\u0026A pairs (including unanswerable)\u0026#8203;:contentReference[oaicite:3]{index=3}; ~54 MB JSON", "summary": "An expanded version of SQuAD featuring over 150k questions combining SQuAD1.1\u2019s queries with 50k new unanswerable questions written adversarially by crowdworkers\u0026#8203;:contentReference[oaicite:2]{index=2}. Systems must answer when possible and abstain on no-answer questions.", "tags": ["crowd-sourced", "QA", "atomic", "answer:verifiable", "question:crowd-sourced", "answer:human", "domain:nlp"]}, {"companion": "The dataset is introduced in the paper \"Competition-Level Code Generation with AlphaCode,\" accessible at [https://arxiv.org/abs/2203.07814](https://arxiv.org/abs/2203.07814).", "download": "The dataset is available for download at [https://huggingface.co/datasets/deepmind/code_contests](https://huggingface.co/datasets/deepmind/code_contests).", "name": "CodeContests", "notes": "CodeContests includes both correct and incorrect solutions, providing a rich dataset for training and debugging models. The problems are sourced from reputable competitive programming platforms, ensuring a diverse range of challenges. The dataset\u0027s structure facilitates research in code synthesis, analysis of programming techniques, and the development of educational tools for programming instruction.", "size": "Approximately 13,000 samples, totaling around 2 GB.", "summary": "CodeContests is a comprehensive dataset curated by DeepMind to support machine learning research in competitive programming. It comprises programming problems sourced from multiple platforms, including Aizu, AtCoder, CodeChef, Codeforces, and HackerEarth. Each problem is accompanied by test cases in the form of paired inputs and outputs, as well as both correct and incorrect human solutions in various programming languages. This dataset was instrumental in training AlphaCode, DeepMind\u0027s code generation system designed to tackle complex programming challenges.", "tags": ["competitive programming", "code generation", "machine learning", "programming challenges", "AlphaCode"]}, {"Lean Workbook": "Contains 57,231 problems.", "Lean Workbook Plus": "Comprises 82,893 problems.", "companion": "The dataset is introduced in the paper \"Lean Workbook: A large-scale Lean problem set formalized from natural language math problems,\" accessible at [https://arxiv.org/abs/2406.03847](https://arxiv.org/abs/2406.03847).", "download": "The dataset is available for download at [https://huggingface.co/datasets/internlm/Lean-Workbook](https://huggingface.co/datasets/internlm/Lean-Workbook).", "name": "Lean-Workbook", "notes": "The Lean-Workbook dataset was developed using an iterative autoformalization pipeline that translates natural language mathematical problems into Lean 4 statements and vice versa. This process involved generating and filtering synthetic data to ensure high-quality formalizations. The dataset supports research in autoformalization and automated theorem proving, providing a valuable resource for training models to understand and generate formal mathematical proofs.", "size": "Approximately 57,231 problems in the Lean Workbook split and 82,893 problems in the Lean Workbook Plus split.", "summary": "Lean-Workbook is a comprehensive dataset comprising tens of thousands of mathematical problems formalized in Lean 4. Developed to support autoformalization model training and automated theorem proving, it includes both natural language statements and their corresponding formal representations in Lean 4. The dataset is structured into two main splits:", "tags": ["human", "compilation", "filtration", "mathematics"]}, {"companion": "https://arxiv.org/abs/2405.14333", "download": "https://huggingface.co/datasets/deepseek-ai/DeepSeek-Prover-V1", "name": "DeepSeek-Prover-V1", "size": "27,000 samples, 6 MB", "summary": "Synthetic dataset of Lean proofs generated by DeepSeek, solving half of miniF2F."}, {"companion": "https://arxiv.org/pdf/1705.04146", "download": "https://huggingface.co/datasets/deepmind/aqua_rat", "name": "AQuA (Algebra Question Answering)", "size": "98,000 samples; 52 MB", "summary": "DeepMind Dataset built by extracting 34,000 questions from undergrad, and grad student admission test (GMAT and GRE), with answer and rational scrapped on the web. Plus crowdsourcing to provide similar questions."}, {"download": "The dataset is available for download at [https://huggingface.co/datasets/glaiveai/reasoning-v1-20m](https://huggingface.co/datasets/glaiveai/reasoning-v1-20m).", "name": "GlaiveAI Reasoning v1-20M", "notes": "The dataset comprises reasoning traces distilled from the DeepSeek-R1 model. It is important to note that the dataset does not include verification of the correctness of these reasoning traces. Users should exercise caution and consider implementing additional verification mechanisms when utilizing this dataset for training or evaluation purposes.", "size": "Approximately 20 million examples, totaling around 87 GB.", "summary": "GlaiveAI Reasoning v1-20M is a substantial dataset comprising 20 million examples, totaling approximately 87 GB. It consists of reasoning traces distilled from the DeepSeek-R1 model, encompassing tasks across mathematics, coding, and science domains. The dataset is designed to facilitate the training and evaluation of large language models (LLMs) in complex reasoning tasks. Notably, the dataset does not include verification of correctness for the reasoning traces, which may impact its applicability in certain contexts.", "tags": ["synthetic", "reasoning", "DeepSeek-R1", "mathematics", "coding", "science"]}, {"L\u012bla-IID": "In-distribution split with train, dev, and test sets.", "L\u012bla-OOD": "Out-of-distribution split to assess generalization.", "L\u012bla-Robust": "Split designed to test robustness to language perturbations.", "companion": "The dataset is introduced in the paper \"L\u012bla: A Unified Benchmark for Mathematical Reasoning,\" accessible at [https://arxiv.org/abs/2210.17517](https://arxiv.org/abs/2210.17517).", "download": "The dataset is available for download at [https://huggingface.co/datasets/allenai/lila](https://huggingface.co/datasets/allenai/lila).", "name": "L\u012bla", "notes": "L\u012bla includes multiple splits to evaluate models under different conditions:", "size": "Over 140,000 questions across 23 tasks.", "summary": "L\u012bla is a comprehensive benchmark for mathematical reasoning, encompassing over 140,000 natural language questions annotated with Python programs and natural language instructions. The benchmark integrates 23 diverse tasks across four dimensions:", "tags": ["compilation", "QA", "benchmark", "answer:verifiable", "question:human", "answer:human", "mathematics"]}, {"companion": "https://arxiv.org/abs/1903.00161 (Dua et al. 2019, DROP paper)", "download": "Available via the AllenAI leaderboard (https://leaderboard.allenai.org/drop) and on HuggingFace `drop`.", "name": "DROP", "notes": "Answers can be numbers, dates, or spans from text; sometimes a list or yes/no. The dataset specifically tests numeric and discrete reasoning (e.g., \"How many...\", arithmetic, set difference) beyond simple lookup. Human performance is high ~95%, while early models struggled ~50%.", "size": "~96,567 QA pairs on 6,713 paragraphs (train ~77k, dev ~9.5k, test ~9.5k)\u0026#8203;:contentReference[oaicite:46]{index=46}\u0026#8203;:contentReference[oaicite:47]{index=47}.", "summary": "Discrete Reasoning Over Paragraphs is a challenging reading comprehension benchmark (2019) with 96k questions that require numeric reasoning, like addition, counting, or date comparisons, over a given passage\u0026#8203;:contentReference[oaicite:44]{index=44}\u0026#8203;:contentReference[oaicite:45]{index=45}. Questions were adversarially crowdsourced to be hard for trivial extraction.", "tags": ["crowd-sourced", "QA", "atomic", "answer:verifiable", "question:crowd-sourced", "answer:human", "domain:mathematics"]}, {"companion": "The dataset is introduced in the paper \"OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems,\" accessible at [https://arxiv.org/abs/2402.14008](https://arxiv.org/abs/2402.14008).", "download": "The dataset is available for download at [https://huggingface.co/datasets/Hothan/OlympiadBench](https://huggingface.co/datasets/Hothan/OlympiadBench).", "name": "OlympiadBench", "notes": "OlympiadBench was developed to address the need for more rigorous benchmarks as LLMs and LMMs approach expert-level performance in various tasks. The problems are bilingual (Chinese and English) and span a range of difficulties, with 66% categorized as Chinese College Entrance Exam level and 34% as competition level. The inclusion of visual elements in over half of the problems allows for the evaluation of models\u0027 multimodal reasoning abilities.", "size": "8,476 problems.", "summary": "OlympiadBench is a comprehensive benchmark designed to evaluate the reasoning capabilities of large language models (LLMs) and large multimodal models (LMMs) using Olympiad-level problems in mathematics and physics. The dataset comprises 8,476 problems sourced from international and regional Olympiad competitions, including the Chinese College Entrance Exam (Gaokao). Each problem is accompanied by expert-level, step-by-step solutions, facilitating in-depth assessment of model performance. Notably, 57% of the problems include visual elements, making it a multimodal benchmark.", "tags": ["human", "QA", "atomic", "answer:verifiable", "with rationale", "question:human", "answer:human", "rationale:human", "mathematics", "physics"]}, {"companion": "https://arxiv.org/abs/1809.09600 (Yang et al. 2018, HotpotQA paper)", "download": "Official website https://hotpotqa.github.io with JSON files; also on HuggingFace (`hotpot_qa`).", "name": "HotpotQA", "notes": "Includes two settings: (1) Distractor: relevant paragraphs mixed with 8 distractors, (2) Fullwiki: requiring a full Wikipedia search. Supports evaluation of multi-hop reasoning and evidence retrieval. Annotated with sentence-level supporting facts (rationale: human).", "size": "~112,000 train questions, 5k dev, 5k test\u0026#8203;:contentReference[oaicite:26]{index=26}. Each question comes with 2 gold paragraphs and supporting sentence labels. ~1.3 GB including context text.", "summary": "A multi-hop QA dataset with 113k crowd-written questions that require reasoning over multiple Wikipedia paragraphs\u0026#8203;:contentReference[oaicite:25]{index=25}. It provides supporting facts annotations to explain the reasoning path, enabling evaluation of explainability.", "tags": ["crowd-sourced", "QA", "atomic", "with rationale", "answer:verifiable", "question:crowd-sourced", "answer:human", "rationale:human", "domain:nlp"]}, {"companion": "https://aclanthology.org/I17-1099 (Li et al. 2017, DailyDialog paper)", "download": "Available via the DailyDialog project (through GitHub or websites) and on HuggingFace `daily_dialog`.", "name": "DailyDialog", "notes": "The dialogues tend to be short and focused on a single scenario. It\u0027s often used for training/evaluating chit-chat dialogue models. Each utterance also has emotion and dialogue act annotations, which can facilitate research in those areas. The data is relatively formal and clean, given it\u2019s written by learners and then corrected.", "size": "13,118 dialogues, 87,170 utterances (average ~7 turns per dialog).", "summary": "A high-quality multi-turn dialogue dataset of 13,118 conversations about everyday topics (not knowledge-grounded) in English. It was written by Chinese learners of English. The conversations average 7.9 turns (102k utterances total) and cover topics like relationships, ordinary life, etc., annotated with emotion and act labels.", "tags": ["human", "dialog", "atomic", "domain:nlp"]}, {"download": "https://huggingface.co/datasets/MatrixStudio/Codeforces-Python-Submissions", "name": "CodeForces", "notes": "Quite extensive datasets with unit test", "size": "700,000 samples, 1.7 GB", "summary": "Website with competitive programming puzzles."}, {"download": "Various formatted datasets based on historical AIME questions are available. For example, the [di-zhang-fdu/AIME_1983_2024](https://huggingface.co/datasets/di-zhang-fdu/AIME_1983_2024) dataset on Hugging Face provides problems from 1983 to 2024.", "name": "AIME (American Invitational Mathematics Examination)", "notes": "As AIME is conducted annually, new problems are introduced each year, expanding the dataset. However, it\u0027s noteworthy that some questions may resemble existing problems available online, potentially overlapping with content in pretraining corpora.", "size": "Each AIME exam comprises 15 questions. Given its annual occurrence since 1983, the dataset includes a substantial collection of problems over the years.", "summary": "The AIME is a prestigious annual mathematics competition in the United States, serving as an intermediate stage between the American Mathematics Competitions (AMC) and the USA Mathematical Olympiad (USAMO). It consists of 15 challenging questions to be solved within a 3-hour time limit, designed to assess and promote problem-solving skills among high school students.", "tags": ["mathematics", "competition", "high school", "problem-solving", "benchmark", "QA", "question:human", "answer:human"]}, {"name": "Various Websites", "summary": "- The art of problem solving (https://artofproblemsolving.com/): a website that prepare students to various STEM competition, and is often used by researchers to craft datasets with rationale. - ProofWiki (https://proofwiki.org/):  a website that aim at collecting math proofs. - StackExchange - Wikipedia # Semi-Native Sources Data that was built by leveraging existing assets with substantial data scrapping work."}, {"companion": "https://arxiv.org/abs/2312.14852", "download": "https://huggingface.co/datasets/BAAI/TACO", "name": "TACO: Topics in Algorithmic Code generation", "size": "26,443 problems with 1.5M solutions, .", "summary": "Algorithmic problems collected from various platforms such as CodeChef, CodeForces, HackerRank, and GeeksforGeeks, as well as existing datasets such as APPS, CodeContest, and Description2code. Each problems come with unit tests that allows to test for correctness. # Filtered Sources Data that come from filtering a bigger dataset."}, {"Task IDs 1-10": "Reserved for few-shot prompting.", "Task IDs 11-510": "Used for testing.", "Task IDs 511-600": "Allocated for validation during fine-tuning.", "Task IDs 601-974": "Designated for training.", "companion": "The dataset is introduced in the paper \"Program Synthesis with Large Language Models,\" accessible at [https://arxiv.org/abs/2108.07732](https://arxiv.org/abs/2108.07732). ", "download": "The dataset is available for download at [https://github.com/google-research/google-research/tree/master/mbpp](https://github.com/google-research/google-research/tree/master/mbpp).", "name": "Mostly Basic Python Problems (MBPP)", "notes": "The MBPP dataset is structured to facilitate both few-shot and fine-tuning evaluations of code generation models. It specifies train and test splits for consistent evaluation:", "size": "Approximately 1,000 samples.", "summary": "The MBPP dataset comprises approximately 1,000 crowd-sourced Python programming problems designed for entry-level programmers. Each problem includes a task description, a reference solution, and three automated test cases. The problems cover a range of topics, including programming fundamentals and standard library functionalities. A subset of the dataset has been hand-verified for quality assurance.", "tags": ["crowd-sourced", "QA", "atomic", "answer:verifiable", "question:crowd-sourced", "answer:human", "programming"]}, {"companion": "https://arxiv.org/abs/1809.02789 (Mihaylov et al. 2018, OpenBookQA paper)", "download": "AI2 OpenBookQA site (AI2 leaderboards) or HuggingFace `openbookqa`.", "name": "OpenBookQA", "notes": "Along with questions, an \"open book\" of 1,326 science facts (mostly from elementary science textbooks) is provided. Successful QA often requires using one of those facts plus additional commonsense or knowledge. The dataset tests multi-hop reasoning combining a known fact and contextual reasoning. Part of the ARC/OpenBookQA leaderboard challenges.", "size": "5,957 questions (4,957 train, 500 dev, 500 test)\u0026#8203;:contentReference[oaicite:72]{index=72}; each question has 4 options.", "summary": "A multiple-choice QA dataset of elementary science questions that require combining a core fact (from a given small \"open book\" of science facts) with broad common knowledge\u0026#8203;:contentReference[oaicite:70]{index=70}. It has 5,957 questions with 4 answer choices each, focusing on science concepts and facts\u0026#8203;:contentReference[oaicite:71]{index=71}.", "tags": ["crowd-sourced", "QA", "benchmark", "question:crowd-sourced", "answer:crowd-sourced", "domain:physics"]}, {"companion": "https://arxiv.org/abs/1705.03551 (Joshi et al. 2017, TriviaQA paper)", "download": "http://nlp.cs.washington.edu/triviaqa/ (contains Wikipedia and Web domain splits); also on HuggingFace `trivia_qa`", "name": "TriviaQA", "notes": "Includes \"unfiltered\" raw set and a \"filtered\" subset where answers appear in the provided text. Each question has a human-authored answer and a set of supporting documents (web pages or Wiki). Often used for open QA and reading comprehension with retrieval.", "size": "~95,000 question instances (filtered set), or 650K question-evidence-answer triples\u0026#8203;:contentReference[oaicite:5]{index=5}; original unfiltered has 950K Q-A pairs across 662K documents\u0026#8203;:contentReference[oaicite:6]{index=6}.", "summary": "A large-scale open-domain QA dataset with trivia questions and answers. It contains ~950K question-answer pairs gathered from trivia websites and Wikipedia, with evidence documents for each question\u0026#8203;:contentReference[oaicite:4]{index=4}. Questions are realistic and often require multi-sentence reasoning.", "tags": ["human", "QA", "compilation", "answer:verifiable", "question:human", "answer:human", "domain:nlp"]}, {"companion": "https://arxiv.org/pdf/2309.05653", "download": "https://huggingface.co/datasets/TIGER-Lab/MathInstruct", "name": "MathInstruct", "size": "262,000 samples, 200 MB", "summary": "A compilation of datasets (gsm8k, aqua,camel) with rationale (some of them being generated by LLMs) used to train MAmmoTH"}, {"companion": "https://arxiv.org/pdf/2402.14008", "download": "https://huggingface.co/datasets/Hothan/OlympiadBench", "name": "OlympiadBench", "size": "8,000 questions.", "summary": "Datasets collected from Olympiads with figures (multi-modal), rationale (derived by humans), various level of difficulty."}, {"companion": "https://arxiv.org/abs/2310.10631", "download": "https://huggingface.co/datasets/EleutherAI/proof-pile-2", "name": "Algebraic Stack", "size": "3,000,000, 11MB.", "summary": "Subset of ProofPile-2. Obtained by filtering GitHub for Coq, Isabelle, Lean and Matlab, extracting data from Mathlib 4 (the Lean library), building a dataset of Isabelle proofs, and filtering the Stack."}, {"companion": "https://arxiv.org/abs/2411.18872", "download": "https://huggingface.co/datasets/internlm/Lean-Workbook", "name": "Lean-Workbook", "size": "25,000 samples, 5MB", "summary": "DOWNLOADED IN `/checkpoint/amaia/explore/datasets/reasoning/raw` Tens of thousands of math problems formalized in Lean4"}, {"companion": "https://arxiv.org/abs/2110.14168 (Cobbe et al. 2021, training verifiers on GSM8K paper)", "download": "OpenAI\u2019s GitHub (Grade School Math 8K data) or HuggingFace `gsm8k`.", "name": "GSM8K", "notes": "The dataset was introduced to facilitate research into chain-of-thought prompting. It\u2019s now a standard test for math word problem solving by language models. Evaluated by exact match on the final numeric answer. Many solutions require several steps of reasoning, which are provided as ground truth rationales.", "size": "8,500 problems (train 7,473, test 1,319)\u0026#8203;:contentReference[oaicite:80]{index=80}; each with a chain-of-thought solution and an answer.", "summary": "Grade School Math 8K is a dataset of 8.5k diverse grade-school level math word problems, created by human writers\u0026#8203;:contentReference[oaicite:79]{index=79}. Each problem comes with a detailed step-by-step solution and the final numerical answer. The problems emphasize linguistic variety and often require multi-step arithmetic reasoning.", "tags": ["crowd-sourced", "QA", "atomic", "with rationale", "question:crowd-sourced", "answer:crowd-sourced", "rationale:human", "domain:mathematics"]}, {"download": "The dataset is available as part of the Proof-Pile-2 collection at [https://huggingface.co/datasets/EleutherAI/proof-pile-2](https://huggingface.co/datasets/EleutherAI/proof-pile-2). To load only the ArXiv subset, use the following code:", "name": "ArXiv Subset of Proof-Pile-2", "notes": "The ArXiv Subset is instrumental for training models on advanced mathematical concepts and research-level problems. It was utilized in training the Llemma 7B and Llemma 34B models, which are designed for mathematical reasoning tasks.", "size": "Approximately 29 billion tokens.", "summary": "The ArXiv Subset is a component of the Proof-Pile-2 dataset, comprising approximately 29 billion tokens of scientific and mathematical documents sourced from arXiv. This subset was curated to support the training of large language models, particularly in advanced mathematical and scientific reasoning. It serves as a valuable resource for developing models capable of understanding and generating research-level content.", "tags": ["arXiv", "Proof-Pile-2", "mathematics", "scientific research", "language model training"]}, {"companion": "https://arxiv.org/abs/1811.01241 (Dinan et al. 2019, Wizard of Wikipedia paper)", "download": "Available via ParlAI (Wizard of Wikipedia task) and on HuggingFace `wizard_of_wikipedia`.", "name": "Wizard of Wikipedia", "notes": "This dataset is used to train conversational agents that can incorporate factual knowledge smoothly. The Wizard was asked to naturally steer the conversation and inform the Apprentice using retrieved knowledge. It\u2019s a multi-turn knowledge-QA hybrid (the apprentice often asks questions, wizard answers with facts). Evaluations often consider knowledge F1 (overlapping facts) and fluency.", "size": "~22,311 dialogs, ~201,000 utterances total. (Train 18,430, Dev 1,948, Test 1,933 dialogs). Each turn by Wizard is annotated with the Wiki sentence used.", "summary": "A knowledge-grounded open-domain dialogue dataset where one speaker (the \"Wizard\") has access to Wikipedia knowledge and the other (\"Apprentice\") does not. They converse on an open-ended topic. The wizard provides knowledgeable responses with relevant retrieved sentences. There are 22k dialogues with ~201k turns, each grounded in a Wikipedia sentence\u0026#8203;:contentReference[oaicite:95]{index=95}.", "tags": ["crowd-sourced", "dialog", "tool use", "question:crowd-sourced", "answer:crowd-sourced", "domain:nlp"]}, {"default": "Contains 94,000 problems and achieves the best performance after supervised fine-tuning (SFT).", "download": "The dataset is available for download at [https://huggingface.co/datasets/open-r1/OpenR1-Math-220k](https://huggingface.co/datasets/open-r1/OpenR1-Math-220k).", "extended": "Comprises 131,000 samples, incorporating additional data sources like `cn_k12`. While this provides more reasoning traces, performance after SFT is observed to be lower than the `default` subset, likely due to the varying difficulty levels of the questions.", "name": "OpenR1-Math-220k", "notes": "OpenR1-Math-220k is part of an ongoing effort by Hugging Face to reproduce and build upon DeepSeek R1. The dataset includes two splits:", "size": "The dataset contains approximately 220,000 samples, totaling around 8 GB.", "summary": "OpenR1-Math-220k is a large-scale dataset designed to enhance mathematical reasoning in AI models. It comprises approximately 220,000 math problems, each accompanied by two to four reasoning traces generated by the DeepSeek R1 model. The problems are sourced from NuminaMath 1.5, and the reasoning traces have been verified using Math Verify and, for a subset, Llama-3.3-70B-Instruct as a judge.", "tags": ["synthetic", "QA", "compilation", "answer:verifiable", "with rationale", "question:human", "answer:synthetic", "rationale:synthetic", "mathematics"]}, {"companion": "https://arxiv.org/abs/2406.17557", "download": "https://huggingface.co/datasets/HuggingFaceFW/fineweb", "name": "FineWeb", "summary": "Filtered from CommonCrawl by HuggingFace, seems to be slightly worse than DCLM."}, {"download": "https://huggingface.co/datasets/glaiveai/reasoning-v1-20m", "name": "GlaiveAI", "notes": "Comes with no verification of correctness", "size": "20 million examples, 87 GB", "summary": "Traces from DeepSeek R1"}, {"companion": "https://arxiv.org/abs/2108.07732 (Austin et al. 2021, MBPP in the context of program synthesis with LMs)", "download": "Google Research GitHub (MBPP dataset) and on HuggingFace `mbpp`.", "name": "MBPP (Mostly Basic Python Problems)", "notes": "Problems cover basic algorithms, string manipulation, etc., solvable in a few lines. Human-written solutions and test cases are provided. The evaluation typically measures the fraction of problems solved (i.e., code passes all tests). Often used to fine-tune or evaluate code generation models, complementary to HumanEval (MBPP has more but simpler tasks).", "size": "974 problems\u0026#8203;:contentReference[oaicite:86]{index=86} (MBPP official). They are split into 374 for training, 90 for validation, 500 for test in the original paper.", "summary": "A collection of 974 crowd-sourced Python programming tasks designed for entry-level programmers\u0026#8203;:contentReference[oaicite:85]{index=85}. Each task has a short natural language prompt (problem description), a reference solution, and 3 simple test cases. The goal is to generate correct code that passes the tests.", "tags": ["crowd-sourced", "QA", "atomic", "tool use", "question:crowd-sourced", "answer:crowd-sourced", "domain:programming"]}, {"companion": "The dataset is introduced in the paper \"MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models,\" accessible at [https://arxiv.org/abs/2309.12284](https://arxiv.org/abs/2309.12284).", "download": "The dataset is available for download at [https://huggingface.co/datasets/meta-math/MetaMathQA](https://huggingface.co/datasets/meta-math/MetaMathQA).", "name": "MetaMathQA", "notes": "MetaMathQA was developed to improve the mathematical reasoning capabilities of LLMs by providing a diverse set of problems with varying reasoning paths. The dataset has been utilized to fine-tune models such as MetaMath-7B, which demonstrated significant performance improvements on benchmarks like GSM8K and MATH.", "size": "Approximately 395,000 samples, totaling around 200 MB.", "summary": "MetaMathQA is a dataset comprising approximately 395,000 mathematical problems, each paired with detailed solutions. The dataset was constructed by augmenting the training sets of GSM8K and MATH through a process called \"question bootstrapping,\" which involves generating new questions by rephrasing existing ones and creating forward and backward reasoning paths. This approach aims to enhance the diversity and quality of mathematical problems available for training large language models (LLMs).", "tags": ["synthetic", "QA", "compilation", "answer:verifiable", "question:synthetic", "answer:synthetic", "mathematics"]}, {"companion": "https://arxiv.org/abs/2412.09413", "download": "https://huggingface.co/datasets/RUC-AIBOX/STILL-3-Preview-RL-Data", "name": "Still", "size": "30,000 samples, 10 MB", "summary": "Collection of question and verifiable answer extracted from Math, Numina, and Aime. # Additional notes (for additional assets) MIT-8 benchmark Proof-Pile - ArXiv.math (10GB) - Open-source math textbooks (50MB) - Formal mathematics libraries (500MB) - Lean mathlib and other Lean repositories - Isabelle AFP - Coq mathematical components and other Coq repositories - HOL Light - set.mm - Mizar Mathematical Library - Math Overflow and Math Stack Exchange (2.5GB) - Wiki-style sources (50MB) - ProofWiki - Wikipedia math articles - MATH dataset (6MB) https://huggingface.co/datasets/HuggingFaceTB/cosmopedia https://huggingface.co/datasets/open-web-math/open-web-math https://github.com/LiveCodeBench/LiveCodeBench https://huggingface.co/datasets/LLM360/MegaMath https://huggingface.co/datasets/math-ai/AutoMathText https://arxiv.org/abs/2402.07625 https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k AMPS (Khan + Mathematica) Dataset OCR: https://arxiv.org/pdf/2502.18443"}, {"companion": "https://arxiv.org/abs/2002.03786 (Hendrycks et al. 2021, MMLU paper)", "download": "The dataset can be obtained from Hendrycks et al.\u2019s repository (ETH-PUBLIC/MMLU) and on HuggingFace `cais/mmlu`.", "name": "MMLU (Massive Multitask Language Understanding)", "notes": "No training set is provided \u2013 meant as a zero or few-shot eval. Questions were sourced from exams like AP tests, college exams, etc., so they require real-world knowledge and problem-solving. Evaluation is simply accuracy. It\u0027s become a standard test for large language models\u2019 breadth of knowledge.", "size": "57 subjects, each ~100-500 questions (14,000+ questions total). All are 4-choice multiple-choice. Provided as a test set mostly (with a small dev for few-shot use).", "summary": "A benchmark of 57 academic and professional subjects, each as a set of multiple-choice questions (4 options). Totaling 14,000+ questions, covering elementary math, US history, college chemistry, law, etc. It is designed to evaluate broad knowledge and reasoning of models\u0026#8203;:contentReference[oaicite:76]{index=76}.", "tags": ["compilation", "QA", "benchmark", "question:human", "answer:human", "domain:nlp"]}, {"download": "https://huggingface.co/datasets/AI-MO/NuminaMath-1.5", "name": "NuminaMath", "size": "Approximately 900,000 samples, 531 MB.", "summary": "Mix of math problems solved with rationale. Sources: aops_forum (https://artofproblemsolving.com/), amc_aime, Chinese k12, gsm8k, math, Olympiads, Orca-math (https://arxiv.org/abs/2402.14830), synthetic_amc, synthetic_math"}, {"companion": "https://arxiv.org/abs/2303.17760", "name": "Camel", "notes": "Comes with no verification of correctness", "summary": "Exercise textbooks synthetically generated by GPT-4"}, {"companion": "The dataset is introduced in the paper \"MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning,\" accessible at [https://arxiv.org/pdf/2309.05653](https://arxiv.org/pdf/2309.05653).", "download": "The dataset is available for download at [https://huggingface.co/datasets/TIGER-Lab/MathInstruct](https://huggingface.co/datasets/TIGER-Lab/MathInstruct).", "name": "MathInstruct", "notes": "MathInstruct serves as the foundational dataset for training the MAmmoTH series of models, which have demonstrated substantial improvements over existing open-source models on multiple mathematical reasoning benchmarks. The dataset\u0027s emphasis on both CoT and PoT rationales allows models to employ diverse problem-solving methodologies, enhancing their generalization across various mathematical tasks.", "size": "Approximately 262,039 samples, totaling around 200 MB.", "summary": "MathInstruct is a meticulously curated instruction tuning dataset designed to enhance mathematical reasoning capabilities in large language models (LLMs). Compiled from 13 distinct mathematical rationale datasets\u2014including GSM8K, AQuA, Camel-Math, and others\u2014it uniquely integrates both chain-of-thought (CoT) and program-of-thought (PoT) rationales. This hybrid approach ensures extensive coverage across diverse mathematical fields, facilitating the development of models capable of versatile problem-solving strategies.", "tags": ["synthetic", "QA", "compilation", "answer:verifiable", "question:synthetic", "answer:synthetic", "mathematics"]}, {"companion": "https://arxiv.org/abs/2410.01560", "download": "https://huggingface.co/datasets/nvidia/OpenMathInstruct-2", "name": "OpenMathInstruct-v2", "size": "22,000,000 samples, 12 GB", "summary": "Augmentation of MATH and GSM8K from LLM (rephrase questions, provide answers)."}, {"companion": "https://arxiv.org/abs/2305.20050", "download": "https://github.com/openai/prm800k", "name": "PRM800K", "notes": "Some annotations were reported to be incorrect", "size": "800,000 samples,", "summary": "Datasets created by OpenAI by using LLMs to answer question from the MATH datasets, with rationale graded by humnas."}, {"download": "https://huggingface.co/datasets/open-r1/OpenR1-Math-220k", "name": "Open-R1-220k", "notes": "They have other datasets, see https://huggingface.co/open-r1", "size": "225,000 samples, 8 GB.", "summary": "This part of an ongoing effort by HuggingFace to reproduce DeepSeek R1. Consists of DeepSeek R1 traces answering problems from NuminaMath, verified with Math Verify, a HuggingFace parser to check numerical equality."}, {"companion": "https://arxiv.org/abs/1905.07830", "download": "https://github.com/rowanz/hellaswag", "name": "HellaSwag", "notes": "The dataset is structured to be trivial for humans (\u003e95% accuracy) but challenging for state-of-the-art models, which have achieved less than 48% accuracy. It is commonly used to benchmark the performance of AI models in commonsense reasoning tasks.", "size": "Approximately 70,000 instances", "summary": "HellaSwag is a challenge dataset designed to evaluate the commonsense natural language inference (NLI) capabilities of AI models. It presents models with sentence completion tasks that are straightforward for humans but challenging for state-of-the-art models.", "tags": ["commonsense reasoning", "natural language inference", "sentence completion", "benchmark", "adversarial filtering"]}, {"companion": "The dataset is associated with the paper \"TORA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving,\" accessible at [https://arxiv.org/abs/2309.17452](https://arxiv.org/abs/2309.17452).", "download": "The dataset is available for download at [https://huggingface.co/datasets/AI-MO/NuminaMath-TIR](https://huggingface.co/datasets/AI-MO/NuminaMath-TIR).", "name": "NuminaMath-TIR", "notes": "NuminaMath-TIR was developed to facilitate the training of language models capable of solving complex mathematical problems by integrating external computational tools into their reasoning processes. The dataset includes problems from various mathematical competitions, providing a diverse set of challenges. Each problem is paired with a solution that combines natural language reasoning and executable code, allowing models to learn how to utilize tools effectively in problem-solving.", "size": "Approximately 70,000 samples, totaling around 150 MB.", "summary": "NuminaMath-TIR is a dataset comprising approximately 70,000 mathematical competition problems, each accompanied by solutions that integrate tool-based reasoning. The solutions are generated using Tool-Integrated Reasoning (TIR), a method that combines natural language explanations with executable code to enhance the problem-solving process. This dataset is designed to improve the mathematical reasoning capabilities of language models by providing step-by-step solutions that incorporate computational tools.", "tags": ["synthetic", "QA", "compilation", "answer:verifiable", "with rationale", "question:human", "answer:synthetic", "rationale:synthetic", "tool use", "mathematics"]}, {"companion": "https://cogcomp.seas.upenn.edu/multirc/ (dataset info) and http://aclweb.org/anthology/N18-1023 (Khashabi et al. 2018, MultiRC paper)", "download": "Released via AI2 (CogComp) and part of the ERASER benchmark. HuggingFace: `multi_rc` (as part of SuperGLUE).", "name": "MultiRC", "notes": "In MultiRC, a question can have multiple correct choices (each is marked true/false). Evaluation uses metrics like F1 and Exact Match over the set of correct options. Also part of the SuperGLUE benchmark. It encourages compositional reasoning across sentence boundaries and reducing guesswork from answer count.", "size": "~6,000 questions on ~870 paragraphs\u0026#8203;:contentReference[oaicite:50]{index=50}; 4,848 question-answer pairs in train (with 10,000+ candidate evals), 7 domains. Each question averages ~4 answer options.", "summary": "Multi-Sentence Reading Comprehension is a dataset of short paragraphs and multi-sentence questions that require reasoning across sentences for the answer\u0026#8203;:contentReference[oaicite:48]{index=48}. Each question has several candidate answer options, and one or more can be correct (not only one)\u0026#8203;:contentReference[oaicite:49]{index=49}. It challenges models to evaluate each option independently.", "tags": ["crowd-sourced", "QA", "atomic", "answer:verifiable", "question:crowd-sourced", "answer:crowd-sourced", "domain:nlp"]}, {"companion": "The dataset is introduced in the paper \"DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models,\" accessible at [https://arxiv.org/abs/2402.03300](https://arxiv.org/abs/2402.03300).", "download": "As of now, the DeepSeek Math Corpus has not been publicly released for download. Researchers interested in this dataset may refer to the associated publications for more details.", "name": "DeepSeek Math Corpus", "notes": "The DeepSeek Math Corpus was instrumental in training the DeepSeekMath 7B model, which achieved a score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of models like GPT-4 and Gemini-Ultra. The corpus includes multilingual data, contributing to improved performance on both English and Chinese mathematical benchmarks. The data collection process involved several iterations of classifier training and data refinement to ensure the quality and relevance of the mathematical content.", "size": "Approximately 120 billion tokens.", "summary": "The DeepSeek Math Corpus is a large-scale, high-quality dataset comprising approximately 120 billion tokens of mathematical content. It was constructed to enhance the mathematical reasoning capabilities of language models. The dataset was developed through an iterative data selection pipeline that involved training a fastText classifier on mathematical content from OpenWebMath, using this classifier to identify math-related content from the Common Crawl dataset, and refining the selection through manual annotation and domain-specific heuristics. This approach resulted in a corpus that is significantly larger than previous mathematical datasets, such as Minerva and OpenWebMath, and includes multilingual data, supporting performance improvements in both English and Chinese mathematical benchmarks.", "tags": ["web", "compilation", "filtration", "CommonCrawl", "mathematics", "multilingual"]}, {"dataset": "https://huggingface.co/datasets/HuggingFaceTB/cosmopedia", "name": "Cosmepedia"}, {"companion": "The dataset is introduced in the paper \"Measuring Coding Challenge Competence With APPS,\" accessible at [https://arxiv.org/pdf/2105.09938](https://arxiv.org/pdf/2105.09938).", "download": "The dataset is available for download at [https://huggingface.co/datasets/codeparrot/apps](https://huggingface.co/datasets/codeparrot/apps).", "name": "APPS (Automated Programming Progress Standard)", "notes": "While APPS provides a substantial collection of programming problems with corresponding test cases, it has been noted that the test coverage may be insufficient in some instances. This limitation can lead to false positives, where incorrect solutions pass the available test cases. For example, the AlphaCode system reported a false positive rate of up to 60% when evaluated with an average of 20 unit tests per problem. Therefore, users should exercise caution and consider augmenting the test cases or employing additional verification methods when utilizing this dataset for model evaluation or training purposes.", "size": "10,000 problems with associated solutions and test cases.", "summary": "APPS is a comprehensive benchmark dataset designed to evaluate the code generation capabilities of language models. It comprises 10,000 Python programming problems sourced from platforms such as Codewars, AtCoder, Kattis, and Codeforces. Each problem includes a natural language description, multiple ground-truth solutions, and corresponding test cases to assess solution correctness. The dataset spans various difficulty levels, from introductory to competition-level challenges, aiming to mirror the evaluation process of human programmers by emphasizing both coding proficiency and problem-solving skills.", "tags": ["code generation", "programming challenges", "Python", "benchmark", "test cases", "Codewars", "AtCoder", "Kattis", "Codeforces"]}, {"companion": "The dataset is introduced in the paper \"MegaMath: Pushing the Limits of Open Math Corpora,\" accessible at [https://arxiv.org/abs/2504.02807](https://arxiv.org/abs/2504.02807). ", "download": "The dataset is available for download at [https://huggingface.co/datasets/LLM360/MegaMath](https://huggingface.co/datasets/LLM360/MegaMath). ", "name": "MegaMath", "notes": "MegaMath surpasses previous open math pretraining datasets, such as DeepSeekMath, by over 30% in token count. Extensive experiments during development led to optimized practices for text extraction, deduplication, and fastText training, ensuring high data quality. Training language models on MegaMath has demonstrated a 15% to 20% performance boost on ten downstream benchmarks, underscoring its efficacy. ", "size": "Approximately 215 million samples totaling 371.6 billion tokens. ", "summary": "MegaMath is an extensive open math pretraining dataset curated by the LLM360 team, encompassing over 300 billion tokens. The dataset is constructed through three primary efforts:", "tags": ["synthetic", "compilation", "answer:verifiable", "question:synthetic", "answer:synthetic", "mathematics", "programming"]}, {"FineMath-3+": "Contains 34 billion tokens across 21.4 million documents.", "FineMath-4+": "A higher-quality subset with 9.6 billion tokens in 6.7 million documents, focusing on detailed explanations.", "companion": "The dataset is introduced in the paper \"SmolLM2: When Smol Goes Big -- Data-Centric Training of a Small Language Model,\" accessible at [https://arxiv.org/abs/2502.02737](https://arxiv.org/abs/2502.02737).", "download": "The dataset is available for download at [https://huggingface.co/datasets/HuggingFaceTB/finemath](https://huggingface.co/datasets/HuggingFaceTB/finemath).", "name": "FineMath", "notes": "FineMath was developed by training a mathematical content classifier using annotations generated by Llama-3.1-70B-Instruct. This classifier filtered Common Crawl data to retain only the most educational mathematical content, prioritizing clear explanations and step-by-step problem-solving over advanced academic papers. The dataset supports research in mathematical reasoning and problem-solving, providing a valuable resource for training models in these areas.", "size": "Approximately 34 billion tokens (FineMath-3+) and 9.6 billion tokens (FineMath-4+).", "summary": "FineMath is a curated dataset comprising high-quality mathematical educational content filtered from Common Crawl. Developed to enhance the training of language models in mathematical reasoning and problem-solving, FineMath emphasizes clear explanations and step-by-step solutions, utilizing Markdown and LaTeX formatting for clarity. The dataset is structured into two main subsets:", "tags": ["web", "compilation", "filtration", "CommonCrawl", "mathematics", "education"]}, {"companion": "https://www.aclweb.org/anthology/Q19-1026 (Kwiatkowski et al. 2019, NQ paper)", "download": "Official JSON from Google at https://ai.google.com/research/NaturalQuestions/download; or via TensorFlow Datasets and HuggingFace (`natural_questions`)", "name": "Natural Questions (NQ)", "notes": "Each example provides the full Wikipedia page and annotated answer(s). Long answers are typically a paragraph or list, short answers are spans or yes/no. Often used for open retrieval QA and reading comprehension. License is CC BY-SA for Wikipedia content.", "size": "Total ~307k training questions, 8k dev, 8k test\u0026#8203;:contentReference[oaicite:9]{index=9}; each with long answer (paragraph) and/or short answer. Dataset JSON size ~2.7GB (including all text).", "summary": "A Google benchmark of real anonymized search queries paired with Wikipedia articles. It contains 307,373 training questions (real Google queries) and human-annotated answers (long answer paragraphs and short answer spans or yes/no) from the related Wikipedia page\u0026#8203;:contentReference[oaicite:7]{index=7}\u0026#8203;:contentReference[oaicite:8]{index=8}.", "tags": ["human", "QA", "atomic", "answer:verifiable", "question:human", "answer:human", "domain:nlp"]}, {"download": "The dataset is accessible at [https://huggingface.co/datasets/greengerong/leetcode](https://huggingface.co/datasets/greengerong/leetcode).", "name": "LeetCode (greengerong version)", "notes": "The dataset\u0027s absence of unit tests and unspecified data collection methods may impact its effectiveness for certain applications. Users should exercise caution and consider supplementing it with additional resources or creating custom unit tests to ensure comprehensive evaluation of code solutions.", "size": "Approximately 2,000 samples, totaling around 7 MB.", "summary": "This dataset comprises a collection of programming problems sourced from LeetCode, a platform renowned for its extensive array of coding challenges used primarily for technical interview preparation. The dataset includes problem statements and example solutions. However, it lacks accompanying unit tests, which are crucial for validating the correctness of solutions. The specific methodology employed for data collection is not detailed, and the dataset appears to have limited coverage compared to the full spectrum of problems available on LeetCode. Consequently, while it may serve as a foundational resource, its utility might be constrained due to these limitations.", "tags": ["human", "QA", "compilation", "answer:verifiable", "question:human", "answer:human", "programming"]}, {"companion": "https://arxiv.org/abs/2312.17120", "download": "https://huggingface.co/datasets/GAIR/MathPile", "name": "MathPile", "size": "Approximately 9.5 billion tokens.", "summary": "Dataset collected by GAIR (the lab behind LIMO) by compiling textbook, arXiv, ProofWiki, and filtering common crawl."}, {"companion": "https://aaai.org/ojs/index.php/AAAI/article/view/7005 (Sakaguchi et al. 2020, WinoGrande paper)", "download": "https://mosaic.allenai.org/projects/winogrande (data available via AI2 download). Also on HuggingFace `winogrande`.", "name": "WinoGrande", "notes": "Each question provides a sentence with a pronoun blank and two answer choices (the two nouns). Example: \"The trophy didn\u0027t fit in the suitcase because ___ was too big.\" (Options: trophy or suitcase). WinoGrande increased scale by crowdsourcing and applying bias reduction so that statistical cues are minimized. It\u2019s a commonsense reasoning test included in some LLM evals.", "size": "44,000 problems (train 40k, dev 1.2k, test 1.8k)\u0026#8203;:contentReference[oaicite:63]{index=63}.", "summary": "A large-scale Winograd Schema Challenge dataset (44k pronoun resolution problems) designed to mitigate annotation artifacts\u0026#8203;:contentReference[oaicite:62]{index=62}. Each problem is a sentence with an ambiguous pronoun referring to one of two entities, and the task is to choose the correct referent. WinoGrande is adversarially filtered and much larger than the original WSC.", "tags": ["crowd-sourced", "QA", "atomic", "question:crowd-sourced", "answer:crowd-sourced", "domain:nlp"]}, {"companion": "https://arxiv.org/abs/2105.09938 (Hendrycks et al. 2021, APPS NeurIPS paper)", "download": "APPS dataset on the authors\u2019 GitHub (Measuring Coding Challenge Competence). Also on HuggingFace `codeparrot/apps`.", "name": "APPS (Automated Programming Progress Standard)", "notes": "Problem difficulty ranges from easy (100-line solutions) to very hard (algorithmic puzzles). The test set is further divided by difficulty for evaluation. APPS is used to benchmark code generation at scale. Models are typically evaluated on the test set by execution-based metrics (% of tests passed). It\u0027s significantly larger and more challenging than MBPP or HumanEval.", "size": "10,000 problems (5k training with solutions, 5k test)\u0026#8203;:contentReference[oaicite:88]{index=88}. Total ~131k test cases across all problems\u0026#8203;:contentReference[oaicite:89]{index=89}; ~232k human solutions for train problems\u0026#8203;:contentReference[oaicite:90]{index=90}.", "summary": "A large dataset of coding problems, including competitive programming challenges. It contains 10,000 problems of varying difficulty, each with a problem description and public test cases\u0026#8203;:contentReference[oaicite:87]{index=87}. Solutions (in Python) are provided for the training set. Models are evaluated by writing code that passes the test cases.", "tags": ["compilation", "QA", "benchmark", "tool use", "question:human", "answer:human", "domain:programming"]}, {"download": "Various formatted datasets based on historical AMC questions are available, e.g. https://huggingface.co/datasets/AI-MO/aimo-validation-amc/", "name": "AMC", "size": "3 (levels) times 25 multi-choice questions per year.", "summary": "American Math competition for various high-school students, which acts as a pre-selection to AIME."}, {"download": "https://huggingface.co/datasets/LLM360/MegaMath", "name": "MegaMath"}, {"name": "DeepScaleR", "summary": "Compiled from Aime, AMC, Omni-Math and Still Compilation of data from AIME, AMC, Omni-Math, and Still. Used to reproduce R1."}, {"companion": "The dataset is introduced in the paper \"DataComp-LM: In search of the next generation of training sets for language models,\" accessible at [https://arxiv.org/abs/2406.11794](https://arxiv.org/abs/2406.11794).", "download": "The dataset is available for download at [https://huggingface.co/datasets/mlfoundations/dclm-baseline-1.0](https://huggingface.co/datasets/mlfoundations/dclm-baseline-1.0).", "name": "DataComp-LM (DCLM)", "notes": "DCLM emphasizes the importance of dataset design in training language models and offers a starting point for further research on data curation. The benchmark consists of multiple scales, with various candidate pool sizes and associated compute budgets, facilitating the study of scaling trends and making the benchmark accessible to researchers with varying resources.", "size": "240 trillion tokens.", "summary": "DataComp-LM (DCLM) is a comprehensive benchmark designed to facilitate controlled experiments in dataset curation for training large language models (LLMs). It provides a standardized corpus of 240 trillion tokens extracted from Common Crawl, effective pretraining recipes based on the OpenLM framework, and a broad suite of 53 downstream evaluations. Researchers can experiment with data curation strategies such as deduplication, filtering, and data mixing across model scales ranging from 412 million to 7 billion parameters. Notably, model-based filtering has been identified as key to assembling high-quality training sets. The resulting dataset, DCLM-Baseline, enables training a 7B parameter language model from scratch to achieve 64% 5-shot accuracy on MMLU with 2.6 trillion training tokens, outperforming previous state-of-the-art open-data language models while using 40% less compute.", "tags": ["benchmark", "dataset curation", "language models", "Common Crawl", "data filtering", "data deduplication"]}, {"name": "AMPS"}, {"download": "https://huggingface.co/datasets/greengerong/leetcode", "name": "LeetCode", "notes": "Unclear how the dataset was collected, comes with no unit tests", "size": "2,000 samples, 7 MB (for the small version I found on HuggingFace)", "summary": "Website with programming puzzles, typically used to prepare coding interviews."}, {"companion": "https://arxiv.org/abs/2103.03874", "download": "https://github.com/hendrycks/math (was taken down from HuggingFace due to copyright issue filled by the art of problem solving).", "name": "MATH", "size": "12,500 samples", "summary": "Classical evaluation datasets, created by Dan Hendrycks, by collecting various US high-school competition problems with solution."}, {"companion": "https://arxiv.org/abs/2502.02737", "download": "https://huggingface.co/datasets/HuggingFaceTB/finemath", "name": "FineMath", "summary": "Filtered from CommonCrawl by HuggingFace for SmolLM focused on Math domain."}, {"companion": "https://arxiv.org/abs/2009.03300", "download": "https://github.com/hendrycks/test", "name": "MMLU", "notes": "MMLU serves as a comprehensive test for language models, assessing both world knowledge and reasoning skills. The dataset\u0027s broad subject coverage and varying difficulty levels make it a robust tool for identifying a model\u0027s strengths and weaknesses across different domains.", "size": "Approximately 16,000 multiple-choice questions", "summary": "The Massive Multitask Language Understanding (MMLU) dataset is a benchmark designed to evaluate the knowledge and problem-solving abilities of language models across 57 diverse subjects, including mathematics, history, law, and medicine.", "tags": ["human", "QA", "benchmark", "answer:verifiable", "question:human", "answer:human"]}, {"companion": "https://www.aclweb.org/anthology/D14-1052 (Richardson et al. 2013, MCTest paper)", "download": "Microsoft MCTest: http://research.microsoft.com/mctest (includes dataset as TSV); also on HuggingFace (`mctest`).", "name": "MCTest", "notes": "Stories and questions were crowd-sourced to be answerable from the text alone, targeting understanding by 7-year-old level readers\u0026#8203;:contentReference[oaicite:39]{index=39}. While small in size, it was influential as a controlled reading comprehension test with open research usage.", "size": "660 stories, 2640 questions (4 Q per story)\u0026#8203;:contentReference[oaicite:37]{index=37}\u0026#8203;:contentReference[oaicite:38]{index=38}, with 4 answer choices per question.", "summary": "One of the early machine comprehension datasets (2013) with 660 short fictional stories written for the task\u0026#8203;:contentReference[oaicite:36]{index=36}. Each story has 4 multiple-choice questions (with 4 options each). Aimed at elementary-level text understanding and reasoning.", "tags": ["crowd-sourced", "QA", "atomic", "answer:verifiable", "question:crowd-sourced", "answer:crowd-sourced", "domain:nlp"]}, {"companion": "https://arxiv.org/abs/1808.07036 (Choi et al. 2018, QuAC paper)", "download": "https://quac.ai/ (provides train/dev data JSON). Also on HuggingFace as `quac`.", "name": "QuAC", "notes": "QuAC involves context retention across turns (each question comes with dialog history). Answers are spans from passages and often require the context of previous Q\u0026A. It also includes teacher\u2019s dialogue acts and answer summaries for some turns.", "size": "13,594 dialogs with 98,407 QA pairs\u0026#8203;:contentReference[oaicite:28]{index=28}. (~83k train questions, 7k dev, 7k test).", "summary": "Question Answering in Context is a dialogue-style QA dataset with 14k information-seeking Q\u0026A dialogs (100k questions total) about Wikipedia articles\u0026#8203;:contentReference[oaicite:27]{index=27}. A student asks a series of questions and a teacher provides extractive answers from the text, or a special \"CANNOTANSWER\" if unanswerable.", "tags": ["crowd-sourced", "dialog", "atomic", "answer:verifiable", "question:crowd-sourced", "answer:human", "domain:nlp"]}, {"companion": "https://arxiv.org/abs/2203.07814", "download": "https://huggingface.co/datasets/deepmind/code_contests", "name": "CodeContests", "notes": "Includes correct and incorrect solutions. Rich data for training/debugging models.", "size": "13,000 samples, 2GB", "summary": "Dataset used for AlphaCode. Competitive programming problems from Aizu, AtCoder, CodeChef, Codeforces and HackerEarth."}, {"companion": "The dataset is introduced in the paper \"MathPile: A Billion-Token-Scale Pretraining Corpus for Math,\" accessible at [https://arxiv.org/abs/2312.17120](https://arxiv.org/abs/2312.17120).", "download": "The dataset is available for download at [https://huggingface.co/datasets/GAIR/MathPile](https://huggingface.co/datasets/GAIR/MathPile).", "name": "MathPile", "notes": "MathPile adheres to the principle of \"less is more,\" emphasizing data quality over quantity. The dataset underwent extensive preprocessing, including prefiltering, language identification, cleaning, filtering, and deduplication, to ensure high quality. Additionally, data contamination detection was performed to eliminate duplicates from benchmark test sets like MATH and MMLU-STEM.", "size": "Approximately 9.5 billion tokens.", "summary": "MathPile is a diverse and high-quality math-centric corpus comprising approximately 9.5 billion tokens. Developed by the Generative AI Research Lab (GAIR), the dataset aggregates mathematical content from various sources, including textbooks, arXiv papers, ProofWiki, StackExchange, Wikipedia, and filtered Common Crawl data. The collection aims to enhance the mathematical reasoning abilities of language models by providing a comprehensive and meticulously curated pretraining corpus.", "tags": ["human", "compilation", "filtration", "mathematics"]}, {"companion": "https://arxiv.org/abs/2310.10631", "download": "https://huggingface.co/datasets/EleutherAI/proof-pile-2", "name": "Proof-Pile-2", "size": "60B tokens,", "summary": "Massive pretraining corpus of formal mathematics and related documents (Lean, Coq, math papers)."}, {"companion": "https://arxiv.org/pdf/2402.03300", "name": "DeepSeek Math Corpus", "summary": "Filtering of Common Crawl based on a FastText classifier based on OpenWebMath as initial positive examples, and additional heuristics. Datasets not available."}, {"companion": "While there isn\u0027t a specific companion paper, the dataset is associated with the Open Thoughts initiative, detailed at [https://github.com/open-thoughts/open-thoughts](https://github.com/open-thoughts/open-thoughts).", "download": "The dataset is available for download at [https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k](https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k).", "name": "OpenThoughts-114k", "notes": "OpenThoughts-114k was developed collaboratively by Bespoke Labs and the DataComp community to democratize access to advanced AI training resources. The dataset includes subsets for specific domains, such as mathematics, which can be accessed separately. It has been utilized to train models like OpenThinker-7B and OpenThinker-32B, demonstrating significant improvements in reasoning benchmarks.", "size": "The dataset contains 114,000 samples, totaling approximately 3.55 GB.", "summary": "OpenThoughts-114k is an open-source synthetic reasoning dataset comprising 114,000 high-quality examples across domains such as mathematics, science, coding, and puzzles. Each example includes a problem statement and a detailed reasoning trace generated by the DeepSeek-R1 model, facilitating the training and evaluation of AI models in complex reasoning tasks.", "tags": ["synthetic", "QA", "compilation", "answer:verifiable", "with rationale", "question:synthetic", "answer:synthetic", "rationale:synthetic", "mathematics", "physics", "programming"]}, {"companion": "https://aclweb.org/anthology/D17-1088 (Ling et al. 2017, AQuA paper); https://arxiv.org/abs/1905.13319 (Amini et al. 2019, MathQA paper)", "download": "AQuA-RAT: via DeepMind\u2019s dataset (on GitHub). MathQA: on GitHub (math-qa) and HuggingFace `math_qa`.", "name": "AQuA (Algebra Question Answering) - MathQA", "notes": "Each problem in AQuA has a free-form rationale text explaining the solution and the correct option. MathQA further provides a functional program representation of the solution. These datasets target math reasoning and are often used for training models to perform arithmetic with explanations.", "size": "~100,000 questions with rationales\u0026#8203;:contentReference[oaicite:82]{index=82} (AQuA-RAT). MathQA (2019) uses a refined subset of 37k questions with programs.", "summary": "AQuA-RAT is a collection of ~100k crowd-sourced algebra word problems, each with 5 multiple-choice options and a detailed rationalized answer\u0026#8203;:contentReference[oaicite:81]{index=81}. The questions are GMAT/GRE style math problems requiring reasoning. A subsequent MathQA dataset re-formatted 37k of these with structured rationale annotations.", "tags": ["crowd-sourced", "QA", "atomic", "with rationale", "question:crowd-sourced", "answer:crowd-sourced", "rationale:crowd-sourced", "domain:mathematics"]}, {"companion": "https://arxiv.org/abs/1905.07830 (Zellers et al. 2019, HellaSwag paper)", "download": "Available from the authors\u2019 website (rowanzellers.com/hellaswag) and on HuggingFace `hellaswag`.", "name": "HellaSwag", "notes": "Built upon SWAG dataset but made harder via adversarial filtering. The contexts come from video descriptions or procedural text, and endings require physical and contextual commonsense. Human performance is near 95%, while GPT-2 struggled ~41% on it, showing the challenge. It\u2019s often used to test common sense and coherence in LMs.", "size": "~70,000 problems (Train ~70k, Dev 10k, Test 10k) with 4 answer choices each\u0026#8203;:contentReference[oaicite:65]{index=65}.", "summary": "A difficult dataset for commonsense NLI, focusing on picking the most plausible continuation of a given situation description. It contains 70k multiple-choice questions\u0026#8203;:contentReference[oaicite:64]{index=64}. Each question is a partial description (from video caption or wikiHow), and the task is to choose the correct ending out of four, where distractors are adversarially generated to be challenging.", "tags": ["synthetic", "QA", "filtration", "question:human", "answer:human", "domain:nlp"]}, {"companion": "https://aclanthology.org/N18-1025 (Saha et al. 2018, DuoRC paper)", "download": "From the authors\u2019 GitHub (duorc) or on HuggingFace (`duorc`).", "name": "DuoRC", "notes": "In SelfRC, questions are answerable from a single plot (one of the pair). In ParaphraseRC, question is based on one plot summary but must be answered from the other, testing cross-document reasoning and paraphrase understanding. Answers are free-form text (often phrases from the target plot).", "size": "186,089 unique QA pairs over 7,680 pairs of movie plots\u0026#8203;:contentReference[oaicite:41]{index=41}. (Contains two subsets: SelfRC ~ 96k Q and ParaphraseRC ~ 90k Q.)", "summary": "A reading comprehension dataset derived from movie plot summaries. It uses pairs of parallel plot descriptions: one from Wikipedia and one from IMDb. ~186k question-answer pairs were crowdsourced such that some questions require combining information across the two versions\u0026#8203;:contentReference[oaicite:40]{index=40}.", "tags": ["crowd-sourced", "QA", "compilation", "answer:verifiable", "question:crowd-sourced", "answer:human", "domain:nlp"]}, {"companion": "https://arxiv.org/abs/1801.07243 (Zhang et al. 2018, PersonaChat paper)", "download": "Facebook ParlAI provides PersonaChat data. Also on HuggingFace as `personachat`.", "name": "PersonaChat", "notes": "This dataset is often used for personalized dialogue modeling. In the conversation, each speaker tries to reveal personal facts naturally. Metrics involve engagingness and consistency. It\u0027s not question-answering per se, but multi-turn open-ended dialogue. Often used in training conversational models (like BlenderBot).", "size": "10,907 dialogues, 162,064 utterances\u0026#8203;:contentReference[oaicite:93]{index=93}. Each persona profile: 4-5 sentences (1,155 unique personas in total).", "summary": "A chit-chat dialogue dataset where each of two crowdworkers is assigned a persona (a set of 5 profile sentences) and they have a conversation in character\u0026#8203;:contentReference[oaicite:91]{index=91}. There are 10,907 dialogues with over 162k utterances\u0026#8203;:contentReference[oaicite:92]{index=92}. The goal is to build agents that can integrate persona information into the conversation naturally.", "tags": ["crowd-sourced", "dialog", "atomic", "question:crowd-sourced", "answer:crowd-sourced", "domain:nlp"]}, {"companion": "https://arxiv.org/abs/1704.04683 (Lai et al. 2017, RACE paper)", "download": "Available at http://www.cs.cmu.edu/~glai1/data/race/ or via HuggingFace (`race`).", "name": "RACE", "notes": "Each question has 4 choices with one correct answer. Topics range from science to social studies. The dataset is challenging for models due to the need for inference and sometimes world knowledge. It became part of the SuperGLUE benchmark (RACE-H as task).", "size": "97,687 questions on 27,933 passages\u0026#8203;:contentReference[oaicite:35]{index=35} (split: RACE-Middle ~28k Q, RACE-High ~69k Q).", "summary": "A large-scale reading comprehension dataset of English exam passages for Chinese students. It consists of 27,933 passages and 97,687 questions (4-option multiple choice) from middle school and high school exams\u0026#8203;:contentReference[oaicite:34]{index=34}. Designed to test comprehensive understanding and reasoning.", "tags": ["human", "QA", "compilation", "answer:verifiable", "question:human", "answer:human", "domain:nlp"]}, {"companion": "https://arxiv.org/abs/2309.16575", "download": "https://github.com/lukemelas/mtob", "name": "MTOB", "notes": "MTOB presents a novel challenge by requiring models to learn translation capabilities from limited resources, mimicking scenarios encountered with low-resource languages. The benchmark includes a comprehensive grammar book detailing Kalamang\u0027s linguistic features, a bilingual word list, and a small set of parallel sentences. Evaluations have shown that while current language models demonstrate promising results, they still fall short of human performance, highlighting the difficulty of the task and the need for further advancements in low-resource language processing.", "size": "573-page grammar book, 2,531-word bilingual word list, and 500 parallel sentence pairs", "summary": "MTOB (Machine Translation from One Book) is a benchmark designed to evaluate the ability of language models to learn translation between English and Kalamang\u2014a language with fewer than 200 speakers\u2014using only a single grammar book as the primary resource.", "tags": ["human", "QA", "benchmark", "answer:verifiable", "question:human", "answer:human", "nlp"]}, {"companion": "https://arxiv.org/abs/2310.06786", "download": "https://huggingface.co/datasets/open-web-math/open-web-math", "name": "OpenWebMath", "size": "6,300,000 samples, 27 GB.", "summary": "Filtering of CommonCrawl done in 2023, that is considered good quality. Not donwloaded, as FineMath is more recent."}, {"companion": "https://arxiv.org/abs/2110.14168", "download": "https://github.com/openai/grade-school-math", "name": "GSM8k", "size": "8,500 problems, 5 MB", "summary": "Grade School Math (GSM) benchmark, created by human annotator for OpenAI."}, {"companion": "https://arxiv.org/abs/1911.11641 (Bisk et al. 2020, PIQA paper)", "download": "Dataset on AllenAI (https://allenai.org/data/piqa) and HuggingFace `piqa`.", "name": "PIQA", "notes": "Examples: \"How do you separate two stuck glasses?\" Option A vs Option B. It focuses on naive physics and ergonomics knowledge. It\u0027s a binary choice task (accuracy measured). As an adversarially constructed dataset, random guess yields 50%. Evaluates models\u2019 grasp of physical commonsense (manipulating objects, tool use).", "size": "21,000 questions (Train 16k, Dev 2k, Test 2k). Each question has 2 answer options.", "summary": "Physical Interaction QA is a dataset for physical commonsense reasoning. It has 20,000 QA pairs about how to solve everyday physical tasks, where each question is a goal and the answer is the more feasible of two solutions\u0026#8203;:contentReference[oaicite:61]{index=61}. The task is to pick the most plausible outcome of an action involving objects.", "tags": ["crowd-sourced", "QA", "atomic", "question:crowd-sourced", "answer:crowd-sourced", "domain:physics"]}, {"companion": "The dataset is introduced in the paper \"MiniF2F: a cross-system benchmark for formal Olympiad-level mathematics,\" accessible at [https://arxiv.org/abs/2109.00110](https://arxiv.org/abs/2109.00110).", "download": "The dataset is available for download at [https://github.com/facebookresearch/miniF2F](https://github.com/facebookresearch/miniF2F).", "name": "MiniF2F", "notes": "MiniF2F serves as a stepping stone towards achieving the IMO Grand Challenge, which envisions an AI capable of winning a gold medal in the International Mathematical Olympiad through formal-to-formal problem-solving. The benchmark\u0027s cross-platform nature and range of difficulty levels make it a valuable resource for developing and evaluating neural theorem proving systems. ", "size": "488 problems with formal and informal statements.", "summary": "MiniF2F is a benchmark dataset designed to evaluate the performance of neural theorem provers across multiple formal systems. It comprises 488 Olympiad-level mathematics problems sourced from competitions such as the American Invitational Mathematics Examination (AIME), American Mathematics Competitions (AMC), and the International Mathematical Olympiad (IMO). Each problem is provided with both informal descriptions and formal statements in systems including Metamath, Lean, and partially in Isabelle and HOL Light. The dataset aims to facilitate advancements in automated theorem proving by offering a unified test suite for cross-system evaluation. ", "tags": ["human", "QA", "benchmark", "answer:verifiable", "question:human", "answer:human", "mathematics"]}, {"companion": "https://arxiv.org/abs/2406.17557", "download": "https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu", "name": "FineWeb-Edu", "summary": "Filtered from FineWeb to focus on sample with educational value"}, {"companion": "The dataset is introduced in the paper \"Learning to Prove Theorems via Interacting with Proof Assistants,\" accessible at [https://arxiv.org/abs/1905.09381](https://arxiv.org/abs/1905.09381).", "download": "The dataset is available for download at [https://github.com/princeton-vl/CoqGym](https://github.com/princeton-vl/CoqGym).", "name": "CoqGym", "notes": "CoqGym includes tools for interacting with the Coq proof assistant, facilitating the training and evaluation of automated theorem-proving agents. The dataset\u0027s structural information, such as ASTs and proof trees, supports the development of models capable of generating tactics and proofs. Additionally, CoqGym provides synthetic proofs generated from intermediate proof steps, augmenting the training data for machine learning models.", "size": "Approximately 70,856 proofs from 123 Coq projects.", "summary": "CoqGym is a comprehensive dataset and learning environment designed to facilitate research in automated theorem proving through interaction with the Coq proof assistant. It encompasses 70,856 human-written proofs extracted from 123 open-source Coq projects, covering diverse domains such as mathematics, computer hardware, and programming languages. The dataset provides rich structural information, including abstract syntax trees (ASTs) and detailed proof trees, enabling the development and evaluation of machine learning models for tactic prediction and proof generation.", "tags": ["theorem proving", "Coq", "machine learning", "proof assistant", "formal verification"]}, {"companion": "https://arxiv.org/abs/1907.09190 (Fan et al. 2019 used ELI5 in their long-form QA paper, although dataset existed from Reddit).", "download": "Originally scraped from Reddit (2019). Processed versions on HuggingFace: `eli5` dataset (with train/val/test splits).", "name": "ELI5 (Explain Like I\u0027m Five)", "notes": "Questions often require multi-sentence explanatory answers, using general and commonsense knowledge. No single correct answer \u2013 the dataset provides one or more top-voted human answers. It\u0027s used for long-form QA evaluation. Not limited to a fixed context; often paired with a retrieval step to fetch supporting info (as in the KILT benchmark).", "size": "~270k question-answer pairs (in the open-domain version, including subquestions). A filtered high-quality subset has ~30k QA. Answers are paragraphs of length ~100 tokens.", "summary": "A dataset of complex questions and long-form answers from the Reddit \"Explain Like I\u2019m Five\" forum. It contains diverse open-ended questions (often starting with \"Why/How\") and answers averaging around a few paragraphs, written to be simple and clear. The task is open-ended answer generation rather than span selection.", "tags": ["human", "QA", "atomic", "answer:human", "domain:nlp"]}]</script>

  <script>
    // Markdown parser
    let md = new markdownit({linkify: true,});

    // Data from flask server
    const datasets = JSON.parse(document.getElementById("data").textContent);
    const tagData = JSON.parse('[{"group": "Source", "tags": {"crowd-sourced": "data collected via human crowd-sourcing platforms to train AI models", "human": "data generated by humans without AI in mind", "synthetic": "data generated by machines"}}, {"group": "Type", "tags": {"QA": "question\u2013answer data", "dialog": "data that can be segmented into individual turns (e.g., `user, llm, python-tool, llm, user, llm, user, llm`)", "monolithic": "continuous, unstructured blocks of text useful for pretraining", "tool use": "data illustrating the integration of external tools"}}, {"group": "Collection process", "tags": {"atomic": "data from a single, consistent source (e.g., standardized exam sets)", "benchmark": "datasets originally created for evaluation or benchmarking", "compilation": "a mixture of sources compiled for training purposes", "filtration": "data obtained by filtering a large chunk of the internet"}}, {"group": "QA specific", "tags": {"answer:crowd-sourced": "answers collected via crowd-sourcing", "answer:human": "answers written by humans", "answer:synthetic": "answers generated by machines", "answer:verifiable": "answers can be independently verified (e.g., factual or numerical)", "question:crowd-sourced": "questions collected via crowd-sourcing", "question:human": "questions written by humans", "question:synthetic": "questions generated by machines", "rationale:crowd-sourced": "rationales collected via crowd-sourcing", "rationale:human": "rationales written by humans", "rationale:synthetic": "rationales generated by machines", "with rationale": "answers that include supporting justifications"}}, {"group": "Domain", "tags": {"mathematics": "focused on math problems and reasoning", "nlp": "focused on natural language processing", "physics": "focused on physics content", "programming": "focused on coding and software tasks"}}]');
    let searchTerm = '';
    let selectedTags = [];

    // Render datasets with markdown and LaTeX support
    function renderDatasets(list) {
      const container = document.getElementById("datasets-container");
      container.innerHTML = "";
      list.forEach(dataset => {
        const summaryHtml = md.renderInline(dataset.summary || "");
        const sizeHtml = md.renderInline(dataset.size || "");
        const dateHtml = md.renderInline(dataset.date || "N/A");

        // Only include full details if present
        let optionalDetails = "";
        if (dataset.download) {
          optionalDetails += `<div class="dataset-detail"><strong>Download Location:</strong> ${md.renderInline(dataset.download)}</div>`;
        }
        if (dataset.companion) {
          optionalDetails += `<div class="dataset-detail"><strong>Companion Paper:</strong> ${md.renderInline(dataset.companion)}</div>`;
        }
        if (dataset.notes) {
          optionalDetails += `<div class="dataset-detail"> ${md.renderInline(dataset.notes)}</div>`;
        }

        const elem = document.createElement("div");
        elem.className = "dataset";
        elem.innerHTML = `
          <div class="dataset-minimal">
            <div class="dataset-title">${dataset.name}</div>
            <div class="dataset-detail">${summaryHtml}</div>
            <div class="dataset-detail"><strong>Size:</strong> ${sizeHtml}</div>
            <div class="dataset-detail"><strong>Date Created:</strong> ${dateHtml}</div>
          </div>
          <div class="dataset-full" style="display: none;">
            ${optionalDetails}
            <div class="dataset-detail"><strong>Tags:</strong> ${renderDatasetTags(dataset.tags)}</div>
          </div>
        `;
        // Toggle details on click.
        elem.addEventListener("click", () => {
          const details = elem.querySelector(".dataset-full");
          details.style.display = details.style.display === "block" ? "none" : "block";
        });
        container.appendChild(elem);

        // Add hyperlink logic
        const links = elem.querySelectorAll("a");
        links.forEach(link => {
          link.setAttribute("target", "_blank");
          link.setAttribute("rel", "noopener noreferrer");
          link.setAttribute("onclick", "event.stopPropagation()");
        });
      });
    }

    // Filter datasets based on search term and selected tags.
    const updateView = () => {
      const filtered = datasets.filter(dataset => {
        const matchesSearch = !searchTerm ||
          (dataset.name && dataset.name.toLowerCase().includes(searchTerm.toLowerCase())) ||
          (dataset.summary && dataset.summary.toLowerCase().includes(searchTerm.toLowerCase())) ||
          (dataset.notes && dataset.notes.toLowerCase().includes(searchTerm.toLowerCase()));
        const matchesTags = selectedTags.length === 0 ||
          (Array.isArray(dataset.tags) && selectedTags.every(tag => dataset.tags.includes(tag)));
        return matchesSearch && matchesTags;
      });
      renderDatasets(filtered);
    };

    // Sort datasets
    function updateOrder() {
      const sortOption = document.getElementById("sort-select").value;
      datasets.sort((a, b) => {
        if (sortOption === "name") {
          console.log(a);
          console.log(b);
          console.log(a.name, b.name);
          return a.name.localeCompare(b.name)
        };
        if (sortOption === "date_latest") return new Date(b.date) - new Date(a.date);
        if (sortOption === "date_earliest") return new Date(a.date) - new Date(b.date);
      });
      updateView();
    }

    // Render and handle tag buttons with tooltips.
    function renderTags() {
      const container = document.getElementById("tags-container");
      container.innerHTML = "";
      // iterate over the array to preserve order.
      tagData.forEach(groupObj => {
        const groupContainer = document.createElement("div");
        groupContainer.className = "tag-group-line";
        const header = document.createElement("span");
        header.className = "tag-group-header";
        header.textContent = `${groupObj.group}: `;
        groupContainer.appendChild(header);
        // iterate over tags within the group.
        Object.keys(groupObj.tags).forEach(tag => {
          const tagElem = document.createElement("span");
          tagElem.className = "tag";
          tagElem.textContent = tag;
          tagElem.setAttribute("data-tooltip", groupObj.tags[tag]);
          tagElem.style.cursor = "pointer";
          tagElem.addEventListener("click", e => {
            e.stopPropagation();
            tagElem.classList.toggle("selected");
            const tagText = tagElem.textContent;
            selectedTags = tagElem.classList.contains("selected")
              ? [...selectedTags, tagText]
              : selectedTags.filter(t => t !== tagText);
            updateView();
          });
          groupContainer.appendChild(tagElem);
          groupContainer.appendChild(document.createTextNode(" "));
        });
        container.appendChild(groupContainer);
      });
    }

    // Render tags for a dataset's tag list.
    function renderDatasetTags(tags) {
      if (!tags) return "";
      return tags.map(tag => {
        let tooltip = "";
        for (let group in tagData) {
          if (tagData[group][tag]) {
            tooltip = tagData[group][tag];
            break;
          }
        }
        return `<span class="tag" data-tooltip="${tooltip}">${tag}</span>`;
      }).join(" ");
    }

    // Event listeners for search and sorting.
    document.getElementById("search-input").addEventListener("input", e => {
      searchTerm = e.target.value;
      updateView();
    });
    document.getElementById("sort-select").addEventListener("change", updateOrder);

    // help button toggles the display of help text.
    document.getElementById("help-button").addEventListener("click", () => {
      const helpText = document.getElementById("help-text");
      helpText.style.display = helpText.style.display === "none" ? "block" : "none";
    });

    // theme toggle button: toggles dark mode and updates its icon.
    document.getElementById("theme-toggle-button").addEventListener("click", () => {
      document.body.classList.toggle("dark-mode");
      const button = document.getElementById("theme-toggle-button");
      if(document.body.classList.contains("dark-mode")){
        button.textContent = "‚òÄÔ∏è";
      } else {
        button.textContent = "üåô";
      }
    });

    // Initialiation
    renderTags();
    updateOrder();
    updateView();

    // Add hovering logic over tags
    document.querySelectorAll('.tag').forEach(tag => {
      let tooltipDiv;
      let hoverTimer; // Timer for the delay

      tag.addEventListener('mouseenter', () => {
        const text = tag.getAttribute('data-tooltip');
        if (!text) return;

        // Start the timer for 1 second delay
        hoverTimer = setTimeout(() => {
          tooltipDiv = document.createElement('div');
          tooltipDiv.className = 'custom-tooltip';
          tooltipDiv.innerText = text;
          document.body.appendChild(tooltipDiv);

          // Position the tooltip
          const rect = tag.getBoundingClientRect();
          const tooltipRect = tooltipDiv.getBoundingClientRect();

          let top = rect.bottom + window.scrollY + 5;
          let left = rect.left + window.scrollX + (rect.width - tooltipRect.width) / 2;

          if (left + tooltipRect.width > window.innerWidth) {
            left = window.innerWidth - tooltipRect.width - 5;
          }
          if (left < 0) {
            left = 5;
          }
          if (top + tooltipRect.height > window.innerHeight + window.scrollY) {
            top = rect.top + window.scrollY - tooltipRect.height - 5;
          }

          tooltipDiv.style.top = `${top}px`;
          tooltipDiv.style.left = `${left}px`;
        }, 500); // 1 second delay
      });

      tag.addEventListener('mouseleave', () => {
        // Clear the timer if the user leaves early
        clearTimeout(hoverTimer);
        if (tooltipDiv) {
          tooltipDiv.remove();
          tooltipDiv = null;
        }
      });
    });

  </script>
</body>
</html>