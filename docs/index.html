<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>Dataset Explorer</title>
  <link rel="stylesheet" type="text/css" href="static/style.css">

  <!-- Use Katex to render math if needed -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.css" integrity="sha384-zh0CIslj+VczCZtlzBcjt5ppRcsAmDnRem7ESsYwWwg3m/OaJ2l4x7YBZl9Kxxib" crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.js" integrity="sha384-Rma6DA2IPUwhNxmrB/7S3Tno0YY7sFu9WSYMCuulLhIqYSGZ2gKCJWIqhBWqMQfh" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/contrib/auto-render.min.js" integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh" crossorigin="anonymous"
      onload="renderMathInElement(document.body);"></script>
</head>
<body>
  <div class="header">
    <h1>Dataset Explorer</h1>
    <div class="top-buttons">
      <button id="help-button" class="round-button" title="click for help">?</button>
      <button id="theme-toggle-button" class="round-button" title="toggle theme">üåô</button>
    </div>
  </div>
  
  <!-- Help text block, initially hidden -->
  <div id="help-text" style="display:none; margin: 10px 0;">
    <p>
      ‚ö†Ô∏è <strong>Under Construction:</strong> the dataset cards ranging from MMLU to WinoGrad need to be completed.<br/><br/>

      <strong>Welcome to the Dataset Explorer!</strong><br/>
      Use the filters to narrow your dataset choices, type in the search bar to find datasets by name or summary, and sort them as needed. 
      Click on a dataset to see further details.<br/><br/>
      ‚ö† Some cards display a warning signÔ∏è, indicating information gathered from a quick internet search without deep verification.
    </p>
  </div>
  
  <!-- Tags rendered below the help button -->
  <div id="tags-container">
    <!-- Tags will be dynamically inserted here -->
  </div>
  
  <!-- Controls row: Search bar (left) & Order menu (right) -->
  <div class="controls-row">
    <div class="search-bar">
      <input type="text" id="search-input" placeholder="Search datasets...">
    </div>
    <div class="sort-bar">
      <label for="sort-select">Sort by:</label>
      <select id="sort-select">
        <option value="name">Name (A-Z)</option>
        <option value="date_latest">Date (Latest)</option>
        <option value="date_earliest">Date (Earliest)</option>
      </select>
    </div>
  </div>
  
  <!-- Container for displaying datasets -->
  <div id="datasets-container">
    <!-- Matching datasets will be displayed here -->
  </div>

  <!-- Render markdown -->
  <script id="markdown-it" src="static/markdownit.js"></script>
  <script id="data" type="application/json">[{"companion": "https://arxiv.org/abs/2103.03874", "date": "2023-11-15", "download": "https://huggingface.co/datasets/HuggingFaceH4/MATH-500", "name": "MATH-500", "notes": "The dataset includes problems across various topics such as algebra, geometry, and calculus, providing a comprehensive evaluation framework for mathematical reasoning in AI systems.", "size": "500 problems.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA curated subset of 500 problems from the MATH benchmark, designed to evaluate mathematical problem-solving abilities of language models.", "tags": ["human", "QA", "benchmark", "answer:verifiable", "question:human", "answer:human", "mathematics"]}, {"companion": "https://arxiv.org/abs/1811.00937", "date": "2018-11-02", "download": "https://www.tau-nlp.org/commonsenseqa", "name": "CommonsenseQA", "notes": "Workers were given a concept from ConceptNet and asked to formulate a question involving that concept in a commonsense way, providing a correct answer and tricky distractors. Questions often require implicit everyday knowledge or reasoning about likely events. It\u0027s part of the GLUE-style leaderboards for commonsense reasoning.", "size": "12,247 questions, each with 5 answer choices (Train: 9,741; Dev: 1,221; Test: 1,285).", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA crowd-sourced multiple-choice question set targeting commonsense knowledge.", "tags": ["crowd-sourced", "QA", "atomic", "question:crowd-sourced", "answer:crowd-sourced", "nlp"]}, {"companion": "https://arxiv.org/abs/2402.07625", "date": "2024-02-12", "download": "https://huggingface.co/datasets/math-ai/AutoMathText", "name": "AutoMathText", "notes": "AutoMathText was developed using Autonomous Data Selection (AutoDS), employing language models as zero-shot verifiers to autonomously curate high-quality mathematical content. Empirical evaluations demonstrated that language models continually pretrained on AutoMathText achieved substantial improvements on mathematical benchmarks, underscoring the dataset\u0027s efficacy in enhancing mathematical reasoning capabilities.", "size": "Approximately 200 GB.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA 200 GB dataset of mathematical texts curated autonomously to enhance language models\u0027 mathematical reasoning.", "tags": ["human", "monolithic", "compilation", "mathematics"]}, {"companion": "https://medium.com/data-science-in-your-pocket/deepseek-r1-5b-new-deepseek-r1-model-tops-openai-o1-in-math-18dacfd04f1f", "date": "2025-02-10", "download": "https://huggingface.co/datasets/agentica-org/DeepScaleR-Preview-Dataset", "name": "DeepScaleR", "notes": "The dataset aggregates problems from AIME (1984\u20132023), AMC (prior to 2023), Omni-MATH, and Still datasets. It was utilized in training the DeepScaleR-1.5B-Preview model, which achieved notable performance improvements in mathematical reasoning tasks.", "size": "Approximately 40,000 problem-answer pairs.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset comprising approximately 40,000 unique mathematical problem-answer pairs from AIME, AMC, Omni-MATH, and Still datasets.", "tags": ["human", "QA", "compilation", "mathematics"]}, {"companion": "https://arxiv.org/abs/2105.09938", "date": "2021-05-20", "download": "https://huggingface.co/datasets/codeparrot/apps", "name": "APPS (Automated Programming Progress Standard)", "notes": "Problems are sourced from platforms like Codewars, AtCoder, Kattis, and Codeforces. Each problem includes a description, multiple solutions, and test cases. Difficulty levels range from introductory to competition-level challenges.", "size": "10,000 problems with solutions and test cases.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA benchmark dataset of 10,000 Python programming problems for evaluating code generation models.", "tags": ["crowd-sourced", "QA", "benchmark", "tool use", "question:human", "answer:human", "programming"]}, {"companion": "https://arxiv.org/abs/2303.17760", "date": "2023-03-31", "download": "https://huggingface.co/datasets/camel-ai/physics", "name": "CAMEL (Communicative Agents for \u0027Mind\u0027 Exploration of Large Scale Language Model Society)", "notes": "The dataset encompasses 25 physics topics, each with 25 subtopics, containing 32 problems per subtopic. Generated by GPT-4 without verification for correctness; users should independently validate content before use.", "size": "20,000 problem-solution pairs.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA synthetic dataset of 20,000 physics problem-solution pairs generated using GPT-4.", "tags": ["synthetic", "QA", "atomic", "answer:synthetic", "question:synthetic", "physics"]}, {"companion": "https://arxiv.org/abs/1903.00161", "date": "2019-03-01", "download": "https://leaderboard.allenai.org/drop", "name": "DROP (Discrete Reasoning Over Paragraphs)", "notes": "Questions necessitate operations like addition, counting, or sorting over passages, challenging models beyond simple extraction. Answers can be numbers, dates, or text spans, sometimes lists or yes/no. Human performance is around 96%, while initial models achieved approximately 32.7% F1 score.", "size": "Approximately 96,567 QA pairs on 6,713 paragraphs.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA reading comprehension benchmark requiring discrete reasoning over paragraphs.", "tags": ["crowd-sourced", "QA", "atomic", "answer:verifiable", "question:crowd-sourced", "answer:human", "mathematics"]}, {"companion": "https://arxiv.org/abs/2406.17557", "date": "2024-06-25", "download": "https://huggingface.co/datasets/HuggingFaceFW/fineweb", "name": "FineWeb", "notes": "FineWeb was developed by Hugging Face to optimize large language model training. While it has shown improved performance over datasets like RefinedWeb, subsequent datasets such as DCLM have reported further enhancements.", "size": "Approximately 15 trillion tokens (~44 TB).", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA 15-trillion-token dataset of cleaned and deduplicated English web data from 96 Common Crawl snapshots.", "tags": ["human", "monolithic", "filtration"]}, {"companion": "https://arxiv.org/abs/2203.10244", "date": "2022-03-19", "download": "https://github.com/vis-nlp/ChartQA", "name": "ChartQA", "notes": "Includes 9,608 human-written questions and 23,111 machine-generated questions, covering diverse chart styles and topics. Questions require arithmetic operations, logical inference, and visual feature interpretation.", "size": "32,719 questions across 20,882 charts.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA benchmark dataset of 32,719 questions on 20,882 charts, designed to evaluate systems on visual and logical reasoning.", "tags": ["human", "QA", "benchmark", "answer:verifiable", "question:human", "answer:human", "mathematics"]}, {"companion": "https://arxiv.org/abs/2110.14168", "date": "2021-10-27", "download": "https://github.com/openai/grade-school-math", "name": "GSM8K (Grade School Math 8K)", "notes": "GSM8K was developed by OpenAI and Surge AI to evaluate and enhance the mathematical reasoning abilities of language models. Problems involve 2 to 8 steps, focusing on basic arithmetic operations. Solutions are provided in natural language with step-by-step reasoning.", "size": "8,500 problems (7,500 training, 1,000 test).", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset of 8,500 linguistically diverse grade-school math word problems requiring multi-step reasoning.", "tags": ["crowd-sourced", "QA", "benchmark", "answer:verifiable", "with rationale", "question:crowd-sourced", "answer:crowd-sourced", "rationale:human", "mathematics"]}, {"companion": "https://arxiv.org/abs/2406.11794", "date": "2024-06-17", "download": "https://huggingface.co/datasets/mlfoundations/dclm-baseline-1.0", "name": "DCLM (DataComp-LM)", "notes": "DCLM allows researchers to experiment with data curation strategies such as deduplication, filtering, and data mixing across model scales ranging from 412 million to 7 billion parameters. The benchmark includes effective pretraining recipes based on the OpenLM framework and a suite of 53 downstream evaluations.", "size": "240 trillion tokens.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA benchmark for controlled dataset experiments aimed at improving language models, featuring a standardized corpus of 240 trillion tokens from Common Crawl.", "tags": ["synthetic", "monolithic", "compilation", "benchmark", "nlp"]}, {"companion": "https://arxiv.org/abs/1907.09190", "date": "2019-07-22", "download": "https://huggingface.co/datasets/eli5", "name": "ELI5 (Explain Like I\u0027m Five)", "notes": "Collected from Reddit\u0027s ELI5 forum, the dataset focuses on open-ended, explanatory questions (often \u0027Why\u0027 or \u0027How\u0027) and long-form human answers. It supports research in generative QA and has been integrated into benchmarks like KILT. Answers are not extractive, and multiple human answers may exist per question.", "size": "Approximately 270,000 QA pairs; a high-quality subset includes ~30,000.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset of long-form QA pairs from the Reddit ELI5 forum, containing complex open-ended questions and multi-paragraph human-written answers.", "tags": ["human", "QA", "atomic", "answer:human", "nlp"]}, {"companion": "https://arxiv.org/abs/1710.03957", "date": "2017-10-11", "download": "http://yanran.li/dailydialog", "name": "DailyDialog", "notes": "The dialogues are human-written, focusing on daily communication topics such as relationships and ordinary life. Each utterance is annotated with emotion and dialogue act labels, facilitating research in emotion recognition and dialogue act classification. The dataset is often used for training and evaluating chit-chat dialogue models.", "size": "13,118 dialogues, totaling 87,170 utterances (approximately 7 turns per dialogue).", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA high-quality multi-turn dialogue dataset of 13,118 conversations covering various everyday topics.", "tags": ["human", "dialog", "atomic", "nlp"]}, {"companion": "https://arxiv.org/abs/2210.17517", "date": "2022-10-31", "download": "https://huggingface.co/datasets/allenai/lila", "name": "L\u012bla", "notes": "Includes splits: L\u012bla-IID (in-distribution), L\u012bla-OOD (out-of-distribution), and L\u012bla-Robust (linguistic variants).", "size": "Over 140,000 questions across 23 tasks.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA benchmark of over 140,000 natural language math questions annotated with Python programs and instructions.", "tags": ["compilation", "QA", "benchmark", "answer:verifiable", "question:human", "answer:human", "mathematics"]}, {"companion": "https://arxiv.org/abs/1905.07830", "date": "2019-05-19", "download": "https://github.com/rowanz/hellaswag", "name": "HellaSwag", "notes": "HellaSwag\u0027s questions are trivial for humans (\u003e95% accuracy) but challenging for state-of-the-art models (\u003c48% accuracy). The dataset employs Adversarial Filtering to generate plausible but incorrect answer choices, making it a robust benchmark for commonsense reasoning.", "size": "Approximately 70,000 multiple-choice questions.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset of ~70,000 adversarially filtered multiple-choice questions for evaluating commonsense natural language inference.", "tags": ["human", "QA", "benchmark", "answer:verifiable", "question:human", "answer:human", "nlp"]}, {"download": "Various formatted datasets based on historical AMC questions are available, e.g., https://huggingface.co/datasets/AI-MO/aimo-validation-amc.", "name": "AMC (American Mathematics Competitions)", "notes": "The competition lasts 75 minutes for levels 10 and 12, 40 minutes for level 8.", "size": "25 multiple-choice questions per year per level.", "summary": "US middle and high school examinations. THe competition is divided in three levels AMC 8, AMC 10, AMC 12 for students below 14, 16 and 18 years old.", "tags": ["human", "QA", "dialog", "atomic", "benchmark", "answer:human", "answer:verifiable", "question:human", "mathematics"]}, {"companion": "https://arxiv.org/abs/2108.07732", "date": "2021-08-16", "download": "https://github.com/google-research/google-research/tree/master/mbpp", "name": "MBPP (Mostly Basic Python Problems)", "notes": "Problems cover programming fundamentals and standard library functionalities. The dataset is structured for both few-shot and fine-tuning evaluations, with specific task ID splits for training, validation, and testing.", "size": "974 problems.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset of 974 crowd-sourced Python programming problems designed for entry-level programmers, each with a task description, code solution, and three test cases.", "tags": ["crowd-sourced", "QA", "atomic", "answer:verifiable", "question:crowd-sourced", "answer:human", "programming"]}, {"companion": "https://arxiv.org/abs/2311.12022", "date": "2023-11-20", "download": "https://github.com/idavidrein/gpqa", "name": "GPQA Diamond", "notes": "The Diamond subset represents the most difficult questions within the GPQA benchmark, designed to be \u0027Google-proof\u0027 and requiring deep subject matter expertise. Expert validators achieved 65% accuracy, while highly skilled non-experts reached only 34%, despite unrestricted web access and extended time per question. This subset serves as a rigorous test for assessing advanced reasoning and knowledge in language models.", "size": "198 multiple-choice questions.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA subset of 198 exceptionally challenging, expert-crafted multiple-choice questions in biology, physics, and chemistry from the GPQA benchmark.", "tags": ["human", "QA", "benchmark", "answer:verifiable", "question:human", "answer:human", "biology", "physics", "chemistry"]}, {"companion": "https://huggingface.co/blog/cosmopedia", "date": "2024-03-20", "download": "https://huggingface.co/datasets/HuggingFaceTB/cosmopedia", "name": "Cosmopedia", "notes": "Cosmopedia is organized into eight subsets based on the source materials used as seed data for generation: web_samples_v1, web_samples_v2, stanford, stories, wikihow, openstax, khanacademy, and auto_math_text. The dataset was developed to facilitate language model pre-training by providing diverse synthetic data across a broad range of topics.", "size": "Over 30 million documents totaling approximately 25 billion tokens.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA synthetic dataset of over 30 million documents generated by Mixtral-8x7B-Instruct-v0.1, including textbooks, blog posts, stories, and WikiHow articles.", "tags": ["synthetic", "monolithic", "compilation", "nlp"]}, {"companion": "https://arxiv.org/abs/1804.07927", "download": "https://duorc.github.io/", "name": "DuoRC", "notes": "The dataset comprises two subsets: SelfRC, with questions and answers from the same plot summary, and ParaphraseRC, where questions from one plot are answered using the other, testing cross-document reasoning and paraphrase understanding.", "size": "186,089 unique QA pairs over 7,680 pairs of movie plots.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA reading comprehension dataset with 186,089 QA pairs from parallel movie plot summaries.", "tags": ["crowd-sourced", "QA", "compilation", "answer:verifiable", "question:crowd-sourced", "answer:human", "nlp"]}, {"download": "https://huggingface.co/datasets/MatrixStudio/Codeforces-Python-Submissions", "name": "Codeforces-Python-Submissions", "notes": "The dataset includes unit tests for many problems, facilitating automated evaluation of code submissions. It is useful for research in code synthesis and programming education.", "size": "Approximately 690,000 samples, totaling around 1.7 GB.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset of Python solutions submitted to Codeforces, including problem statements, user submissions, and metadata.", "tags": ["crowd-sourced", "monolithic", "atomic", "programming"]}, {"companion": "https://arxiv.org/abs/2502.01456", "date": "2025-02-03", "download": "https://huggingface.co/datasets/PRIME-RL/Eurus-2-RL-Data", "name": "Eurus-RL", "notes": "Eurus-RL aggregates problems from sources like NuminaMath-CoT, APPS, CodeContests, TACO, and Codeforces. The dataset underwent rigorous filtering to ensure problem solvability and answer correctness, including reformatting multiple-choice questions to open-ended formats and removing duplicates. It supports research in reinforcement learning for complex reasoning tasks.", "size": "Approximately 480,537 samples (~2 GB).", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset of mathematics and coding problems with verifiable outcomes for reinforcement learning training.", "tags": ["human", "compilation", "filtration", "mathematics", "programming"]}, {"companion": "Introduced in the data mix to the train Llema: https://arxiv.org/abs/2310.10631", "date": "2023-10-16", "download": "Available as part of the [Proof-Pile-2 mix](https://huggingface.co/datasets/EleutherAI/proof-pile-2).", "name": "Algebraic Stack", "notes": "The dataset is dominated by Python (65%), Isabelle (10%), and C++ (9%). Among others, the formal proofs are extracted from Lean mathlib library and Isabelle Archive of Formal Proofs.", "size": "Approximately 11 billion tokens (~40 MB).", "summary": "Code data coming from filtering the Stack, as well as extracting formal proofs.", "tags": ["human", "monolithic", "filtration", "programming"]}, {"companion": "https://arxiv.org/abs/2310.10631", "date": "2023-10-16", "download": "https://huggingface.co/datasets/EleutherAI/proof-pile-2", "name": "ArXiv Subset of Proof-Pile-2", "notes": "The ArXiv subset was filtered and compiled from a 2023 snapshot by RedPajama. It supports training on research-level reasoning and was used in the development of the Llemma models for mathematical tasks.", "size": "Approximately 29 billion tokens.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA 29B-token subset of scientific and mathematical documents from arXiv, used in Proof-Pile-2 for training language models on formal and technical content.", "tags": ["human", "monolithic", "filtration", "mathematics"]}, {"companion": "https://arxiv.org/abs/2311.12022", "date": "2023-11-20", "download": "https://github.com/idavidrein/gpqa", "name": "GPQA (Graduate-Level Google-Proof Q\u0026A)", "notes": "GPQA\u0027s questions are designed to be exceptionally challenging and \u0027Google-proof,\u0027 with experts achieving 65% accuracy and highly skilled non-experts only 34%, despite unrestricted web access and extensive time per question. As of March 2024, Claude 3 Opus achieved approximately 60% accuracy on this benchmark, highlighting rapid advancements in AI capabilities.", "size": "448 multiple-choice questions.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA benchmark of 448 expert-crafted, graduate-level multiple-choice questions in biology, physics, and chemistry.", "tags": ["human", "QA", "benchmark", "answer:verifiable", "question:human", "answer:human", "biology", "physics", "chemistry"]}, {"companion": "https://arxiv.org/abs/1808.07042", "date": "2018-08-21", "download": "https://stanfordnlp.github.io/coqa/", "name": "CoQA (Conversational Question Answering Challenge)", "notes": "Domains include children\u0027s stories, literature, and middle/high school textbooks. Answers can be yes/no or free text, not necessarily extractive. Each answer is accompanied by a highlighted evidence span from the passage. CoQA evaluates F1 of answer text and a conversational Exact Match.", "size": "8,000+ conversations, 127,000+ Q\u0026A pairs (Train: ~108k; Dev: 7.2k; Test: ~7.3k).", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset with 127k+ conversational Q\u0026A pairs across 8k+ dialogues spanning multiple domains.", "tags": ["crowd-sourced", "dialog", "atomic", "with rationale", "answer:verifiable", "question:crowd-sourced", "answer:crowd-sourced", "rationale:human", "nlp"]}, {"companion": "https://arxiv.org/abs/1803.05457", "date": "2018-03-14", "download": "https://huggingface.co/datasets/allenai/ai2_arc", "name": "AI2 ARC (Easy \u0026 Challenge)", "notes": "Each question has four answer choices and is accompanied by a 14M-sentence science corpus for retrieval. The Challenge set contains questions that simple retrieval or co-occurrence methods fail to answer correctly.", "size": "7,787 questions (5,228 Easy; 2,559 Challenge).", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset of 7,787 grade-school science multiple-choice questions, divided into Easy and Challenge sets.", "tags": ["compilation", "QA", "benchmark", "question:human", "answer:human", "nlp"]}, {"companion": "https://arxiv.org/abs/1905.09381", "date": "2019-05-21", "download": "https://github.com/princeton-vl/CoqGym", "name": "CoqGym", "notes": "CoqGym includes proofs from diverse domains such as mathematics, computer hardware, and programming languages. It provides structural information like abstract syntax trees (ASTs) and detailed proof trees, supporting the development of machine learning models for tactic prediction and proof generation. Additionally, it offers synthetic proofs generated from intermediate proof steps to augment training data.", "size": "70,856 proofs from 123 Coq projects.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset and learning environment comprising over 70,000 human-written Coq proofs from 123 projects.", "tags": ["human", "monolithic", "compilation", "programming"]}, {"companion": "https://arxiv.org/abs/2305.01210", "date": "2023-05-02", "download": "https://github.com/evalplus/evalplus", "name": "HumanEval+", "notes": "HumanEval+ increases the number of test cases approximately 80-fold compared to the original HumanEval dataset, aiming to reduce false positives in model evaluations and offer a more stringent assessment of functional correctness in generated code.", "size": "164 programming problems with over 1.3 million test cases.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eAn augmented version of the HumanEval dataset, enhancing test coverage to provide a more rigorous evaluation framework for code generation models.", "tags": ["human", "QA", "benchmark", "answer:verifiable", "question:human", "answer:human", "programming"]}, {"companion": "https://arxiv.org/abs/1705.04146", "date": "2017-05-11", "download": "https://huggingface.co/datasets/deepmind/aqua_rat", "name": "AQuA (Algebra Question Answering with Rationales)", "notes": "Developed by DeepMind, the dataset includes problems sourced from exams like GMAT and GRE, supplemented with crowdsourced questions. Each problem features a question, five answer choices, and a detailed rationale explaining the solution.", "size": "Approximately 100,000 problems.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset of approximately 100,000 algebraic word problems, each with multiple-choice answers and detailed rationales.", "tags": ["crowd-sourced", "QA", "benchmark", "with rationale", "question:crowd-sourced", "answer:crowd-sourced", "rationale:crowd-sourced", "mathematics"]}, {"download": "Various formatted datasets based on historical AIME questions are available, e.g., https://huggingface.co/datasets/di-zhang-fdu/AIME_1983_2024.", "name": "AIME (American Invitational Mathematics Examination)", "notes": "The competition lasts 3 hours. The answer is a number between 0 and 999.", "size": "15 questions per year.", "summary": "Annual US high-school competition, harder than the AMC and easier than the USAMO.", "tags": ["human", "QA", "dialog", "atomic", "benchmark", "answer:human", "answer:verifiable", "question:human", "mathematics"]}, {"companion": "https://arxiv.org/abs/2403.07974", "date": "2024-03-12", "download": "https://github.com/LiveCodeBench/LiveCodeBench", "name": "LiveCodeBench", "notes": "Continuously updated to prevent test set contamination, assessing capabilities beyond code generation, including self-repair, code execution, and test output prediction.", "size": "400 coding problems.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA benchmark of 400 coding problems from LeetCode, AtCoder, and CodeForces, designed to evaluate large language models\u0027 coding capabilities.", "tags": ["human", "QA", "benchmark", "answer:verifiable", "question:human", "answer:human", "programming"]}, {"companion": "https://arxiv.org/abs/2109.00110", "date": "2021-08-31", "download": "https://github.com/openai/miniF2F", "name": "MiniF2F", "notes": "MiniF2F includes problems from competitions like AIME, AMC, and IMO, formalized in systems such as Lean, Metamath, Isabelle, and HOL Light. It serves as a unified cross-system benchmark for neural theorem proving.", "size": "488 problems with formal and informal statements.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA benchmark of 488 formalized Olympiad-level mathematics problems for evaluating neural theorem provers.", "tags": ["human", "QA", "benchmark", "answer:verifiable", "question:human", "answer:human", "mathematics"]}, {"companion": "https://arxiv.org/abs/2203.07814", "date": "2022-03-15", "download": "https://huggingface.co/datasets/deepmind/code_contests", "name": "CodeContests", "notes": "Includes both correct and incorrect solutions in various programming languages, providing rich data for training and debugging models.", "size": "Approximately 13,000 problems, totaling around 2 GB.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset of competitive programming problems with test cases and human solutions from platforms like Aizu, AtCoder, CodeChef, Codeforces, and HackerEarth.", "tags": ["crowd-sourced", "QA", "compilation", "benchmark", "question:crowd-sourced", "answer:crowd-sourced", "programming"]}, {"companion": "https://arxiv.org/abs/2405.14333", "date": "2024-05-23", "download": "https://huggingface.co/datasets/deepseek-ai/DeepSeek-Prover-V1", "name": "DeepSeek-Prover-V1", "notes": "Developed to enhance LLM capabilities in formal theorem proving, this dataset includes formal proofs constructed in Lean 4, targeting problems from benchmarks like miniF2F. Fine-tuning DeepSeekMath 7B on this dataset achieved a 50.0% pass rate on the miniF2F test set, surpassing previous methods.", "size": "27,503 samples, 19.6 MB.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA synthetic dataset of 27,503 Lean 4 formal proofs generated by DeepSeek, focusing on high-school and undergraduate-level mathematical competition problems.", "tags": ["synthetic", "monolithic", "atomic", "mathematics"]}, {"date": "2025-03-18", "download": "https://huggingface.co/datasets/glaiveai/reasoning-v1-20m", "name": "GlaiveAI Reasoning v1-20M", "notes": "The dataset includes reasoning traces from DeepSeek-R1 across domains such as mathematics, coding, and science. Notably, it does not include verification of correctness for the reasoning traces, which may impact its applicability in certain contexts.", "size": "Approximately 87 GB.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset of 20 million reasoning traces across various domains, distilled from the DeepSeek-R1 model.", "tags": ["synthetic", "monolithic", "compilation", "mathematics", "programming", "science"]}, {"companion": "https://arxiv.org/abs/2309.05653", "date": "2023-09-11", "download": "https://huggingface.co/datasets/TIGER-Lab/MathInstruct", "name": "MathInstruct", "notes": "Compiled from 13 datasets, including GSM8K, AQuA, and Camel-Math, to train the MAmmoTH models for enhanced mathematical reasoning.", "size": "262,039 samples (~200 MB).", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA curated instruction tuning dataset of 262,039 mathematical problems with chain-of-thought and program-of-thought rationales.", "tags": ["synthetic", "QA", "compilation", "answer:verifiable", "question:synthetic", "answer:synthetic", "mathematics"]}, {"companion": "https://arxiv.org/abs/2402.03300", "date": "2024-04-27", "download": "Not publicly available.", "name": "DeepSeek Math Corpus", "notes": "The dataset was filtered using a classifier trained on OpenWebMath, with manual annotation and heuristic refinement. It was used to train DeepSeekMath 7B, which scored 51.7% on the MATH benchmark without external tools. The data includes math problems in multiple languages from Common Crawl.", "size": "Approximately 120 billion tokens.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA 120B-token dataset of mathematical content filtered from Common Crawl using a fastText classifier trained on OpenWebMath.", "tags": ["synthetic", "monolithic", "filtration", "mathematics"]}, {"companion": "https://arxiv.org/abs/2411.18872", "date": "2024-11-28", "download": "https://huggingface.co/datasets/roozbeh-yz/IMO-Steps", "name": "IMO-Steps", "notes": "Includes formal proofs for 14 IMO problems from the miniF2F benchmark and 3 additional problems from IMO 2022 and 2023. The dataset also provides a decomposition into 1,329 lemmas, totaling over 40,000 lines of Lean code, serving as building blocks for the formal proofs.", "size": "Approximately 17 formalized IMO problems, totaling around 5,880 lines of Lean code.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset of formal proofs for International Mathematical Olympiad (IMO) problems, formalized in Lean 4.", "tags": ["human", "atomic", "benchmark", "answer:verifiable", "question:human", "answer:human", "mathematics"]}, {"companion": "https://arxiv.org/abs/1905.10044", "date": "2019-05-23", "download": "https://huggingface.co/datasets/boolq", "name": "BoolQ (Boolean Questions)", "notes": "Each example consists of a real user question (from search logs), a relevant Wikipedia passage, and a yes/no answer. Annotators labeled answers based on the passage. Questions often require inference or commonsense reasoning. Included in SuperGLUE.", "size": "15,942 question\u2013passage pairs.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset of 15,942 naturally occurring yes/no questions paired with Wikipedia passages.", "tags": ["human", "QA", "atomic", "benchmark", "answer:verifiable", "question:human", "answer:human", "nlp"]}, {"companion": "https://arxiv.org/abs/2007.00398", "date": "2020-07-01", "download": "https://rrc.cvc.uab.es/?ch=17\u0026com=downloads", "name": "DocVQA", "notes": "The dataset includes diverse document types such as forms, tables, and figures, requiring models to interpret text within complex layouts. A significant performance gap exists between current models and human accuracy, underscoring the need for improved document understanding in VQA systems.", "size": "50,000 questions on 12,767 document images.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset of 50,000 questions on 12,767 document images, designed to evaluate Visual Question Answering models\u0027 ability to understand and extract information from various document types.", "tags": ["human", "QA", "benchmark", "answer:verifiable", "question:human", "answer:human", "nlp"]}, {"companion": "https://arxiv.org/abs/2406.03847", "date": "2024-06-06", "download": "https://huggingface.co/datasets/internlm/Lean-Workbook", "name": "Lean-Workbook", "notes": "Developed using an iterative autoformalization pipeline, translating natural language problems into Lean 4 statements. The dataset includes formal-informal question pairs and new IMO questions.", "size": "Approximately 57,000 problems.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset of approximately 57,000 mathematical problems formalized in Lean 4.", "tags": ["human", "compilation", "mathematics"]}, {"companion": "https://arxiv.org/abs/1804.07927", "download": "https://duorc.github.io/", "name": "DuoRC", "notes": "The dataset comprises two subsets: SelfRC, with questions and answers from the same plot summary, and ParaphraseRC, where questions from one plot are answered using the other, testing cross-document reasoning and paraphrase understanding.", "size": "186,089 unique QA pairs over 7,680 pairs of movie plots.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA reading comprehension dataset with 186,089 QA pairs from parallel movie plot summaries.", "tags": ["crowd-sourced", "QA", "compilation", "answer:verifiable", "question:crowd-sourced", "answer:human", "nlp"]}, {"companion": "https://arxiv.org/abs/2312.17120", "date": "2023-12-28", "download": "https://huggingface.co/datasets/GAIR/MathPile", "name": "MathPile", "notes": "Developed by the Generative AI Research Lab (GAIR), MathPile emphasizes data quality over quantity. It includes content from textbooks, arXiv, Wikipedia, ProofWiki, StackExchange, and filtered Common Crawl data. Extensive preprocessing ensures high-quality data, with measures taken to eliminate duplicates from benchmark test sets like MATH and MMLU-STEM.", "size": "Approximately 9.5 billion tokens.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA diverse, high-quality math-centric corpus of approximately 9.5 billion tokens, compiled from sources like textbooks, arXiv papers, and web pages.", "tags": ["human", "compilation", "filtration", "mathematics"]}, {"companion": "https://aclanthology.org/D13-1020/", "date": "2013-10-18", "download": "http://research.microsoft.com/mctest", "name": "MCTest", "notes": "Each story is accompanied by four multiple-choice questions, each with four answer options. The dataset targets elementary-level text understanding and reasoning, with content suitable for readers aged 7. It was influential as a controlled reading comprehension test with broad research usage.", "size": "660 stories, 2,640 questions.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset of 660 crowd-sourced fictional stories with 2,640 multiple-choice questions designed to evaluate machine comprehension.", "tags": ["crowd-sourced", "QA", "atomic", "answer:verifiable", "question:crowd-sourced", "answer:crowd-sourced", "nlp"]}, {"companion": "https://arxiv.org/abs/1809.09600", "date": "2018-09-25", "download": "https://hotpotqa.github.io", "name": "HotpotQA", "notes": "HotpotQA includes sentence-level supporting facts annotations, enabling evaluation of explainability. It features two settings: (1) Distractor: relevant paragraphs mixed with 8 distractors, (2) Fullwiki: requiring a full Wikipedia search. The dataset supports evaluation of multi-hop reasoning and evidence retrieval.", "size": "113,000 question-answer pairs.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset of 113k crowd-sourced questions requiring multi-hop reasoning over multiple Wikipedia paragraphs.", "tags": ["crowd-sourced", "QA", "benchmark", "answer:verifiable", "with rationale", "question:crowd-sourced", "answer:human", "rationale:human", "nlp"]}, {"companion": "https://people.ict.usc.edu/~gordon/publications/Chambers-etal-IJCAI2011.pdf", "date": "2011-03-01", "download": "https://people.ict.usc.edu/~gordon/copa.html", "name": "COPA (Choice of Plausible Alternatives)", "notes": "Each COPA item presents a premise and asks for either a cause or an effect, with two plausible alternatives. The dataset is balanced to avoid superficial cues and has been included in the SuperGLUE benchmark. Human performance is approximately 100%.", "size": "1,000 questions (500 development, 500 test); each with 2 answer options.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset for evaluating commonsense causal reasoning through multiple-choice questions.", "tags": ["human", "QA", "atomic", "question:human", "answer:human", "nlp"]}, {"companion": "https://arxiv.org/abs/2406.17557", "date": "2024-06-17", "download": "https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu", "name": "FineWeb-Edu", "notes": "Filtered using a classifier trained on 450,000 annotated web samples by Llama 3 to emphasize high educational value.", "size": "Approximately 1.3 trillion tokens.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eCurated educational texts extracted from FineWeb using an educational quality classifier.", "tags": ["human", "monolithic", "filtration"]}, {"companion": "https://arxiv.org/abs/2107.03374", "date": "2021-07-07", "download": "https://github.com/openai/human-eval", "name": "HumanEval", "notes": "Each problem includes a function signature, docstring, and unit tests to assess functional correctness. The dataset emphasizes functional correctness over syntactic accuracy, providing a robust benchmark for code generation models. Notably, OpenAI\u0027s Codex achieved a 28.8% pass rate on these problems.", "size": "164 programming problems.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset of 164 hand-crafted Python programming problems designed to evaluate functional correctness in code generation models.", "tags": ["human", "QA", "benchmark", "answer:verifiable", "question:human", "answer:human", "programming"]}, {"companion": "https://arxiv.org/abs/2103.03874", "date": "2021-03-05", "download": "https://github.com/hendrycks/math", "name": "AMPS (Auxiliary Mathematics Problems and Solutions)", "notes": "Developed to support mathematical problem-solving model training, AMPS includes problems from Khan Academy covering topics from basic addition to multivariable calculus, and Mathematica-generated problems based on 100 hand-designed modules across various mathematical subjects.", "size": "Over 5 million problems with solutions.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA comprehensive dataset of over 100,000 Khan Academy problems and approximately 5 million Mathematica-generated problems, each with step-by-step solutions.", "tags": ["crowd-sourced", "synthetic", "monolithic", "compilation", "with rationale", "question:crowd-sourced", "question:synthetic", "answer:synthetic", "rationale:synthetic", "mathematics"]}, {"companion": "https://arxiv.org/abs/2103.03874", "date": "2021-03-05", "download": "https://github.com/hendrycks/math", "name": "MATH (Mathematics Aptitude Test of Heuristics)", "notes": "Problems are sourced from competitions like AMC 10, AMC 12, and AIME, covering topics such as algebra, geometry, number theory, and calculus. Each problem includes a detailed solution, facilitating training and evaluation of mathematical reasoning models.", "size": "12,500 problems with solutions.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset of 12,500 challenging high school competition math problems with step-by-step solutions.", "tags": ["human", "QA", "benchmark", "answer:verifiable", "with rationale", "question:human", "answer:human", "rationale:human", "mathematics"]}, {"companion": "https://arxiv.org/abs/2309.12284", "date": "2023-09-21", "download": "https://huggingface.co/datasets/meta-math/MetaMathQA", "name": "MetaMathQA", "notes": "Developed to enhance the mathematical reasoning capabilities of large language models, MetaMathQA has been utilized to fine-tune models like MetaMath-7B, which demonstrated significant performance improvements on benchmarks such as GSM8K and MATH.", "size": "Approximately 395,000 samples, totaling around 200 MB.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset of approximately 395,000 mathematical problems with detailed solutions, generated through question bootstrapping from GSM8K and MATH training sets.", "tags": ["synthetic", "QA", "compilation", "answer:verifiable", "question:synthetic", "answer:synthetic", "mathematics"]}, {"companion": "https://arxiv.org/abs/2303.04488", "date": "2023-03-08", "download": "https://huggingface.co/datasets/Simontwice/premise_selection_in_isabelle", "name": "Isabelle Premise Selection", "notes": "Includes premises from original proofs and those generated using Isabelle\u0027s Sledgehammer tool, enhancing diversity for training models in premise selection tasks.", "size": "Over 4 million samples.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset of over 4 million proof context and premise pairs from Isabelle\u0027s Archive of Formal Proofs.", "tags": ["human", "compilation", "programming"]}, {"companion": "https://arxiv.org/abs/2502.02737", "date": "2025-02-12", "download": "https://huggingface.co/datasets/HuggingFaceTB/finemath", "name": "FineMath", "notes": "Developed using a Llama-3.1-70B-Instruct based classifier to retain educational math content with clear explanations and step-by-step solutions.", "size": "Approximately 34B tokens (FineMath-3+) and 9.6B tokens (FineMath-4+).", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eCurated mathematical content filtered from CommonCrawl for training language models.", "tags": ["human", "monolithic", "filtration", "mathematics"]}, {"date": "2023-08-06", "download": "https://huggingface.co/datasets/greengerong/leetcode", "name": "LeetCode (greengerong version)", "notes": "The dataset includes problem statements and example solutions but lacks unit tests. The data collection methodology is not specified, and the dataset may not cover the full range of LeetCode problems.", "size": "Approximately 2,000 samples (~7 MB).", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset of approximately 2,000 programming problems sourced from LeetCode, lacking unit tests and with unspecified collection methods.", "tags": ["human", "QA", "compilation", "answer:verifiable", "question:human", "answer:human", "programming"]}, {"companion": "https://arxiv.org/abs/2504.02807", "date": "2025-04-03", "download": "https://huggingface.co/datasets/LLM360/MegaMath", "name": "MegaMath", "notes": "MegaMath surpasses previous open math pretraining datasets, such as DeepSeekMath, by over 30% in token count. Extensive experiments during development led to optimized practices for text extraction, deduplication, and fastText training, ensuring high data quality. Training language models on MegaMath has demonstrated a 15% to 20% performance boost on ten downstream benchmarks, underscoring its efficacy.", "size": "Approximately 215 million samples totaling 371.6 billion tokens.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eAn extensive open math pretraining dataset with over 300 billion tokens, curated to enhance mathematical reasoning in language models.", "tags": ["synthetic", "compilation", "answer:verifiable", "question:synthetic", "answer:synthetic", "mathematics", "programming"]}]</script>

  <script>
    // Markdown parser and data setup.
    let md = new markdownit({ linkify: true, html: true });
    const datasets = JSON.parse(document.getElementById("data").textContent);
    const tagData = JSON.parse('[{"group": "Source", "tags": {"crowd-sourced": "data collected via human crowd-sourcing platforms to train AI models", "human": "data generated by humans without AI in mind", "synthetic": "data generated by machines"}}, {"group": "Type", "tags": {"QA": "question\u2013answer data", "dialog": "data that can be segmented into individual turns (e.g., `user, llm, python-tool, llm, user, llm, user, llm`)", "monolithic": "continuous, unstructured blocks of text useful for pretraining", "tool use": "data illustrating the integration of external tools"}}, {"group": "Collection process", "tags": {"atomic": "data from a single, consistent source (e.g., standardized exam sets)", "benchmark": "datasets originally created for evaluation or benchmarking", "compilation": "a mixture of sources compiled for training purposes", "filtration": "data obtained by filtering a large chunk of the internet"}}, {"group": "QA specific", "tags": {"answer:crowd-sourced": "answers collected via crowd-sourcing", "answer:human": "answers written by humans", "answer:synthetic": "answers generated by machines", "answer:verifiable": "answers can be independently verified (e.g., factual or numerical)", "question:crowd-sourced": "questions collected via crowd-sourcing", "question:human": "questions written by humans", "question:synthetic": "questions generated by machines", "rationale:crowd-sourced": "rationales collected via crowd-sourcing", "rationale:human": "rationales written by humans", "rationale:synthetic": "rationales generated by machines", "with rationale": "answers that include supporting justifications"}}, {"group": "Domain", "tags": {"mathematics": "focused on math problems and reasoning", "nlp": "focused on natural language processing", "physics": "focused on physics content", "programming": "focused on coding and software tasks"}}]');
    let searchTerm = '';
    let selectedTags = [];
    let expandedStates = {};
  
    // Function to attach hover event listeners to tag elements within a given container.
    function attachHoverToTags(container) {
      container.querySelectorAll('.tag').forEach(tag => {
        let tooltipDiv;
        let hoverTimer;
  
        tag.addEventListener('mouseenter', () => {
          const text = tag.getAttribute('data-tooltip');
          if (!text) return;
          hoverTimer = setTimeout(() => {
            tooltipDiv = document.createElement('div');
            tooltipDiv.className = 'custom-tooltip';
            tooltipDiv.innerText = text;
            document.body.appendChild(tooltipDiv);
            const rect = tag.getBoundingClientRect();
            const tooltipRect = tooltipDiv.getBoundingClientRect();
            let top = rect.bottom + window.scrollY + 5;
            let left = rect.left + window.scrollX + (rect.width - tooltipRect.width) / 2;
            if (left + tooltipRect.width > window.innerWidth) {
              left = window.innerWidth - tooltipRect.width - 5;
            }
            if (left < 0) {
              left = 5;
            }
            if (top + tooltipRect.height > window.innerHeight + window.scrollY) {
              top = rect.top + window.scrollY - tooltipRect.height - 5;
            }
            tooltipDiv.style.top = `${top}px`;
            tooltipDiv.style.left = `${left}px`;
          }, 500);
        });
  
        tag.addEventListener('mouseleave', () => {
          clearTimeout(hoverTimer);
          if (tooltipDiv) {
            tooltipDiv.remove();
            tooltipDiv = null;
          }
        });
      });
    }
  
    // Render dataset cards. Note that after rendering the cards,
    // we call attachHoverToTags to bind hover logic to the newly created tag elements.
    function renderDatasets(list) {
      const container = document.getElementById("datasets-container");
      container.innerHTML = "";
      list.forEach(dataset => {
        const summaryHtml = md.renderInline(dataset.summary || "");
        const sizeHtml = md.renderInline(dataset.size || "");
        const dateHtml = md.renderInline(dataset.date || "N/A");
        // Only include full details if present
        let optionalDetails = "";
        if (dataset.download) {
          optionalDetails += `<div class="dataset-detail"><strong>Download Location:</strong> ${md.renderInline(dataset.download)}</div>`;
        }
        if (dataset.companion) {
          optionalDetails += `<div class="dataset-detail"><strong>Companion Paper:</strong> ${md.renderInline(dataset.companion)}</div>`;
        }
        if (dataset.notes) {
          optionalDetails += `<div class="dataset-detail">${md.renderInline(dataset.notes)}</div>`;
        }
        // Check the global expanded state for this dataset.
        const isExpanded = expandedStates[dataset.name];
        const elem = document.createElement("div");
        elem.className = "dataset";
        elem.innerHTML = `
          <div class="dataset-minimal">
            <div class="dataset-title">${dataset.name}</div>
            <div class="dataset-detail">${summaryHtml}</div>
            <div class="dataset-detail"><strong>Size:</strong> ${sizeHtml}</div>
            <div class="dataset-detail"><strong>Date Created:</strong> ${dateHtml}</div>
          </div>
          <div class="dataset-full" style="display: ${isExpanded ? 'block' : 'none'};">
            ${optionalDetails}
            <div class="dataset-detail"><strong>Tags:</strong> ${renderDatasetTags(dataset.tags)}</div>
          </div>
        `;
        elem.addEventListener("click", () => {
          const details = elem.querySelector(".dataset-full");
          if (expandedStates[dataset.name]) {
            expandedStates[dataset.name] = false;
            details.style.display = "none";
          } else {
            expandedStates[dataset.name] = true;
            details.style.display = "block";
          }
        });
        container.appendChild(elem);
  
        // Ensure links inside the card don‚Äôt trigger the parent click event.
        const links = elem.querySelectorAll("a");
        links.forEach(link => {
          link.setAttribute("target", "_blank");
          link.setAttribute("rel", "noopener noreferrer");
          link.setAttribute("onclick", "event.stopPropagation()");
        });
      });
      // Re-attach hover handlers to tag elements in the newly rendered datasets.
      attachHoverToTags(container);
    }
  
    // Render dataset tags (inside each dataset card).
    function renderDatasetTags(tags) {
      if (!tags) return "";
      return tags.map(tag => {
        let tooltip = "";
        for (let group of tagData) {
          if (group.tags && group.tags[tag]) {
            tooltip = group.tags[tag];
            break;
          }
        }
        return `<span class="tag" data-tooltip="${tooltip}">${tag}</span>`;
      }).join(" ");
    }
  
    // Update view based on search and tag filters.
    const updateView = () => {
      const filtered = datasets.filter(dataset => {
        const matchesSearch = !searchTerm ||
          (dataset.name && dataset.name.toLowerCase().includes(searchTerm.toLowerCase())) ||
          (dataset.summary && dataset.summary.toLowerCase().includes(searchTerm.toLowerCase())) ||
          (dataset.notes && dataset.notes.toLowerCase().includes(searchTerm.toLowerCase()));
        const matchesTags = selectedTags.length === 0 ||
          (Array.isArray(dataset.tags) && selectedTags.every(tag => dataset.tags.includes(tag)));
        return matchesSearch && matchesTags;
      });
      renderDatasets(filtered);
    };
  
    // Sort datasets.
    function updateOrder() {
      const sortOption = document.getElementById("sort-select").value;
      datasets.sort((a, b) => {
        if (sortOption === "name") return a.name.localeCompare(b.name);
        if (sortOption === "date_latest") return new Date(b.date) - new Date(a.date);
        if (sortOption === "date_earliest") return new Date(a.date) - new Date(b.date);
      });
      updateView();
    }
  
    // Render the top-level tag filters.
    function renderTags() {
      const container = document.getElementById("tags-container");
      container.innerHTML = "";
      tagData.forEach(groupObj => {
        const groupContainer = document.createElement("div");
        groupContainer.className = "tag-group-line";
        const header = document.createElement("span");
        header.className = "tag-group-header";
        header.textContent = `${groupObj.group}: `;
        groupContainer.appendChild(header);
        Object.keys(groupObj.tags).forEach(tag => {
          const tagElem = document.createElement("span");
          tagElem.className = "tag";
          tagElem.textContent = tag;
          tagElem.setAttribute("data-tooltip", groupObj.tags[tag]);
          tagElem.style.cursor = "pointer";
          tagElem.addEventListener("click", e => {
            e.stopPropagation();
            tagElem.classList.toggle("selected");
            const tagText = tagElem.textContent;
            selectedTags = tagElem.classList.contains("selected")
              ? [...selectedTags, tagText]
              : selectedTags.filter(t => t !== tagText);
            updateView();
          });
          groupContainer.appendChild(tagElem);
          groupContainer.appendChild(document.createTextNode(" "));
        });
        container.appendChild(groupContainer);
      });
      // Attach hover handlers for the top-level tags.
      attachHoverToTags(container);
    }
  
    // Event listeners for search and sorting.
    document.getElementById("search-input").addEventListener("input", e => {
      searchTerm = e.target.value;
      updateView();
    });
    document.getElementById("sort-select").addEventListener("change", updateOrder);
    document.getElementById("help-button").addEventListener("click", () => {
      const helpText = document.getElementById("help-text");
      helpText.style.display = helpText.style.display === "none" ? "block" : "none";
    });
    document.getElementById("theme-toggle-button").addEventListener("click", () => {
      document.body.classList.toggle("dark-mode");
      const button = document.getElementById("theme-toggle-button");
      button.textContent = document.body.classList.contains("dark-mode") ? "‚òÄÔ∏è" : "üåô";
    });
  
    // Initialization.
    renderTags();
    updateOrder();
    updateView();
  </script>
</body>
</html>