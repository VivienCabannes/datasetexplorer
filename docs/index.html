<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>Dataset Explorer</title>
  <style>
    body {
        font-family: Arial, sans-serif;
        padding: 20px;
    }
    .search-bar {
        margin-bottom: 20px;
    }
    .tags {
        margin-bottom: 20px;
    }
    .tag {
        display: inline-block;
        background-color: #eee;
        padding: 5px 10px;
        margin: 5px;
        border-radius: 5px;
        cursor: pointer;
    }
    .tag.selected {
        background-color: #cce5ff;
    }
    .sort-bar {
        margin-bottom: 20px;
    }
    .dataset {
        border: 1px solid #ccc;
        padding: 10px;
        margin-bottom: 10px;
        cursor: pointer;
        transition: background-color 0.2s ease;
    }
    .dataset:hover {
        background-color: #f9f9f9;
    }
    .dataset-title {
        font-size: 1.2em;
        font-weight: bold;
    }
    .dataset-detail {
        margin: 5px 0;
    }
    /* The full details section is hidden by default */
    .dataset-full {
        display: none;
        margin-top: 10px;
        border-top: 1px dashed #ccc;
        padding-top: 10px;
    }
    /* Styling for tags shown within each dataset */
    .dataset-tag {
        display: inline-block;
        background-color: #ddd;
        padding: 2px 5px;
        margin: 2px;
        border-radius: 3px;
    }
    /* Updated custom tooltip styling */
    .tooltip {
        position: relative;
        display: inline-block;
    }
    .tooltip .tooltiptext {
        visibility: hidden;
        background-color: #f9f9f9; /* Light background */
        color: #000;               /* Dark text */
        text-align: center;
        padding: 5px 8px;
        border: 1px solid #ccc;
        border-radius: 4px;
        position: absolute;
        z-index: 1;
        top: 110%;               /* Appear under the button */
        left: 50%;
        transform: translateX(-50%);
        white-space: nowrap;     /* Single line */
        opacity: 0;
        transition: opacity 0.3s;
    }
    .tooltip:hover .tooltiptext {
        visibility: visible;
        opacity: 1;
    }
  </style>
</head>
<body>
  <h1>Dataset Explorer</h1>
  <div class="search-bar">
    <input type="text" id="search-input" placeholder="Search datasets...">
  </div>
  <div class="tags" id="tags-container">
    <!-- Tags will be dynamically inserted here -->
  </div>
  <div class="sort-bar">
    <label for="sort-select">Sort by:</label>
    <select id="sort-select">
      <option value="name">Name (A-Z)</option>
      <option value="date_latest">Date (Latest)</option>
      <option value="date_earliest">Date (Earliest)</option>
    </select>
  </div>
  <div id="datasets-container">
    <!-- Matching datasets will be displayed here -->
  </div>

  <script>
    // Retrieve dataset and tag specifications passed from Flask via Jinja2.
    let datasets = JSON.parse('[{"companion": "https://arxiv.org/abs/2411.18872", "download": "https://huggingface.co/datasets/roozbeh-yz/IMO-Steps", "name": "IMO-Steps", "size": "20 samples, 6 kB", "summary": "DOWNLOADED IN `/checkpoint/amaia/explore/datasets/reasoning/raw` 20 Lean proofs of IMO problems"}, {"companion": "https://arxiv.org/abs/2410.07985", "download": "https://huggingface.co/datasets/KbsdJames/Omni-MATH", "name": "OmniMath", "size": "4,000, 7 MB.", "summary": "Data obtained from regional to international Olympiads, from the art-of-problem solving."}, {"companion": "https://arxiv.org/abs/2310.06786", "download": "https://huggingface.co/datasets/open-web-math/open-web-math", "name": "OpenWebMath", "size": "6,300,000 samples, 27 GB.", "summary": "Filtering of CommonCrawl done in 2023, that is considered good quality. Not donwloaded, as FineMath is more recent."}, {"download": "https://huggingface.co/datasets/PRIME-RL/Eurus-2-RL-Data", "name": "Eurus-RL", "notes": "Includes multiple datasets with corresponding training recipes.", "size": "500,000 samples, 2 GB", "summary": "Collection of question with verifiable answer extracted from Numina, Apps, CodeContests, Taco and Codeforces. https://arxiv.org/abs/2502.01456"}, {"companion": "https://arxiv.org/abs/2412.09413", "download": "https://huggingface.co/datasets/RUC-AIBOX/long_form_thought_data_5k", "name": "Still long format", "size": "5,000 samples, 20 MB.", "summary": "Similar to Still, with a focus on long answer, questions come from NuminaMath, Aime, Leetcode, OpenCoder, Camel, Gaokao (Chinese A-level) and RiddleSense. # Formal Math"}, {"companion": "https://arxiv.org/abs/2103.03874", "download": "https://github.com/hendrycks/math (was taken down from HuggingFace due to copyright issue filled by the art of problem solving).", "name": "MATH", "size": "12,500 samples", "summary": "Classical evaluation datasets, created by Dan Hendrycks, by collecting various US high-school competition problems with solution."}, {"companion": "https://arxiv.org/pdf/2309.05653", "download": "https://huggingface.co/datasets/TIGER-Lab/MathInstruct", "name": "MathInstruct", "size": "262,000 samples, 200 MB", "summary": "A compilation of datasets (gsm8k, aqua,camel) with rationale (some of them being generated by LLMs) used to train MAmmoTH"}, {"download": "https://huggingface.co/datasets/EleutherAI/proof-pile-2 contains data scrapped from the ArXiv website, according to a snapshot taken in 2023 by RedPajama.", "name": "ArXiv", "notes": "The dataset is valuable for training models on advanced mathematical concepts and research-level problems.", "size": "29B tokens", "summary": ""}, {"companion": "https://arxiv.org/abs/2310.10631", "download": "https://huggingface.co/datasets/EleutherAI/proof-pile-2", "name": "Proof-Pile-2", "size": "60B tokens,", "summary": "Massive pretraining corpus of formal mathematics and related documents (Lean, Coq, math papers)."}, {"companion": "https://arxiv.org/abs/2310.06770", "download": "https://github.com/swe-bench/SWE-bench", "name": "SWE-bench", "size": "2,300 samples", "summary": "Datasets of codebase, issues and unit tests. The goal is to fix the codebase that currently yield the issue resulting in failing unit tests. The data was collected from popular Github repository."}, {"companion": "https://arxiv.org/abs/2405.14333", "download": "https://huggingface.co/datasets/deepseek-ai/DeepSeek-Prover-V1", "name": "DeepSeek-Prover-V1", "size": "27,000 samples, 6 MB", "summary": "Synthetic dataset of Lean proofs generated by DeepSeek, solving half of miniF2F."}, {"name": "Various Websites", "summary": "- The art of problem solving (https://artofproblemsolving.com/): a website that prepare students to various STEM competition, and is often used by researchers to craft datasets with rationale. - ProofWiki (https://proofwiki.org/):  a website that aim at collecting math proofs. - StackExchange - Wikipedia # Semi-Native Sources Data that was built by leveraging existing assets with substantial data scrapping work."}, {"companion": "https://arxiv.org/abs/2410.01560", "download": "https://huggingface.co/datasets/nvidia/OpenMathInstruct-2", "name": "OpenMathInstruct-v2", "size": "22,000,000 samples, 12 GB", "summary": "Augmentation of MATH and GSM8K from LLM (rephrase questions, provide answers)."}, {"download": "Various formatted datasets based on historical AMC questions are available, e.g. https://huggingface.co/datasets/AI-MO/aimo-validation-amc/", "name": "AMC", "size": "3 (levels) times 25 multi-choice questions per year.", "summary": "American Math competition for various high-school students, which acts as a pre-selection to AIME."}, {"companion": "https://arxiv.org/abs/1905.09381", "download": "https://github.com/princeton-vl/CoqGym", "name": "CoqGym", "size": "70,000 proof steps", "summary": "Large-scale dataset compiled from various 71,000 Coq projects."}, {"companion": "https://arxiv.org/abs/2312.14852", "download": "https://huggingface.co/datasets/BAAI/TACO", "name": "TACO", "size": "26,000 problems with 1.5M solutions, .", "summary": "Algorithmic problems collected from various platforms such as CodeChef, CodeForces, HackerRank, and GeeksforGeeks, as well as existing datasets such as APPS, CodeContest, and Description2code. Each problems come with unit tests that allows to test for correctness. # Filtered Sources Data that come from filtering a bigger dataset."}, {"companion": "https://arxiv.org/abs/2502.00203", "download": "https://huggingface.co/datasets/nvidia/Llama-Nemotron-Post-Training-Dataset-v1", "name": "Nemotron Post-Training Dataset v1", "notes": "Developed to fine-tune models for improved accuracy and reliability.", "size": "15,000,000 samples, 6 GB.", "summary": "We do not have too many details on this datasets, it seems to have been curated from Llama, Qwen and DeepSeek answers."}, {"companion": "https://arxiv.org/abs/2406.11794", "download": "https://huggingface.co/datasets/mlfoundations/dclm-baseline-1.0", "name": "DataComp-LM (DCLM)", "size": "4T token", "summary": "Filtering of Common Crawl based on heuristic cleaning and filtering (see RefinedWeb), deduplication (through Bloom), filtering with fastText classifier to match the reddit channel ExplainLikeImFive, and an instruct model."}, {"companion": "https://arxiv.org/abs/2310.10631", "download": "https://huggingface.co/datasets/EleutherAI/proof-pile-2", "name": "Algebraic Stack", "size": "3,000,000, 11MB.", "summary": "Subset of ProofPile-2. Obtained by filtering GitHub for Coq, Isabelle, Lean and Matlab, extracting data from Mathlib 4 (the Lean library), building a dataset of Isabelle proofs, and filtering the Stack."}, {"companion": "https://arxiv.org/abs/2203.07814", "download": "https://huggingface.co/datasets/deepmind/code_contests", "name": "CodeContests", "notes": "Includes correct and incorrect solutions. Rich data for training/debugging models.", "size": "13,000 samples, 2GB", "summary": "Dataset used for AlphaCode. Competitive programming problems from Aizu, AtCoder, CodeChef, Codeforces and HackerEarth."}, {"companion": "https://arxiv.org/abs/2109.00110", "download": "https://github.com/facebookresearch/miniF2F", "name": "MiniF2F", "size": "500 examples with Lean formal statements and informal statements and proofs.", "summary": "MiniF2F is a benchmark for formal mathematics, created by Kunhao, consisting of 500 Olympiad-level mathematics problems from competitions like AIME, AMC, and IMO. Each problem is provided with both informal and formal statements. # Compilation"}, {"companion": "https://arxiv.org/abs/2309.12284", "download": "https://huggingface.co/datasets/meta-math/MetaMathQA", "name": "MetaMathQA", "notes": "OpenMathInstruct is a more recent iteration of the same idea.", "size": "400,000 samples, 200 MB.", "summary": "Datasets collected by \u0027bootstrapping\u0027 gsm8k and Math"}, {"companion": "https://arxiv.org/abs/2502.02737", "download": "https://huggingface.co/datasets/HuggingFaceTB/stack-edu", "name": "Stack-Edu", "notes": "Need to be download with S3, still really big in terms of number of tokens.", "size": "167,000,000 samples, ~1 TB", "summary": "Filtering version of the Stack V2. # LLM Augmented Source Data being synthesized with LLMs. The LLM can be used to synthesize texts or questions (usually providing example of questions from existing datasets). It can also be used to synthesize a rationale to answer questions. The answer may be verified, either with parser checking for numerical equality (i.e. `\\frac13` = `0.333`), or using LLM as a judge. Synthesized questions (bootstrapped from existing one), synthesized, eventually verified, answers."}, {"companion": "https://arxiv.org/abs/2210.17517", "download": "https://huggingface.co/datasets/allenai/lila", "name": "Lila", "notes": "The datasets were collected for evaluation, it seems small and outdated.", "summary": "Compilation of many datasets including addsub, amps, apps, asdiv, conala, mathematics, dolphin, draw, gsm8k, math, mathqa, mbpp, mctaco, multiarith, numersense, numglus, simuleq, singleop, singleq, svamp."}, {"name": "DeepScaleR", "summary": "Compiled from Aime, AMC, Omni-Math and Still Compilation of data from AIME, AMC, Omni-Math, and Still. Used to reproduce R1."}, {"companion": "https://arxiv.org/abs/2305.20050", "download": "https://github.com/openai/prm800k", "name": "PRM800K", "notes": "Some annotations were reported to be incorrect", "size": "800,000 samples,", "summary": "Datasets created by OpenAI by using LLMs to answer question from the MATH datasets, with rationale graded by humnas."}, {"companion": "https://arxiv.org/abs/2406.17557", "download": "https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu", "name": "FineWeb-Edu", "summary": "Filtered from FineWeb to focus on sample with educational value"}, {"companion": "https://arxiv.org/abs/2110.14168", "download": "https://github.com/openai/grade-school-math", "name": "GSM8k", "size": "8,500 problems, 5 MB", "summary": "Grade School Math (GSM) benchmark, created by human annotator for OpenAI."}, {"download": "Various formatted datasets based on historical AIME questions are available, e.g., [on HuggingFace](https://huggingface.co/datasets/di-zhang-fdu/AIME_1983_2024)", "name": "AIME", "notes": "Since AIME is an annual examination, new data becomes available each year. However, designers may select questions already present on the internet, which could be included in pretraining corpora.", "size": "15 questions per year", "summary": "US high-school competition, American Invitational Mathematics Examination."}, {"companion": "https://arxiv.org/abs/2411.18872", "download": "https://huggingface.co/datasets/internlm/Lean-Workbook", "name": "Lean-Workbook", "size": "25,000 samples, 5MB", "summary": "DOWNLOADED IN `/checkpoint/amaia/explore/datasets/reasoning/raw` Tens of thousands of math problems formalized in Lean4"}, {"companion": "https://arxiv.org/pdf/2402.03300", "name": "DeepSeek Math Corpus", "summary": "Filtering of Common Crawl based on a FastText classifier based on OpenWebMath as initial positive examples, and additional heuristics. Datasets not available."}, {"download": "https://huggingface.co/datasets/AI-MO/NuminaMath-1.5", "name": "NuminaMath", "size": "Approximately 900,000 samples, 531 MB.", "summary": "Mix of math problems solved with rationale. Sources: aops_forum (https://artofproblemsolving.com/), amc_aime, Chinese k12, gsm8k, math, Olympiads, Orca-math (https://arxiv.org/abs/2402.14830), synthetic_amc, synthetic_math"}, {"companion": "https://arxiv.org/abs/2108.07732", "download": "https://github.com/google-research/google-research/tree/master/mbpp", "name": "MBPP (Mostly Basic Python Problems)", "size": "1,000 samples", "summary": "Crowd-sourced datasets of small Python problems A dataset consisting of 1,000 Python programming problems aimed at entry-level programmers."}, {"companion": "https://arxiv.org/abs/2303.17760", "name": "Camel", "notes": "Comes with no verification of correctness", "summary": "Exercise textbooks synthetically generated by GPT-4"}, {"companion": "https://arxiv.org/pdf/2402.14008", "download": "https://huggingface.co/datasets/Hothan/OlympiadBench", "name": "OlympiadBench", "size": "8,000 questions.", "summary": "Datasets collected from Olympiads with figures (multi-modal), rationale (derived by humans), various level of difficulty."}, {"companion": "https://arxiv.org/abs/2412.09413", "download": "https://huggingface.co/datasets/RUC-AIBOX/STILL-3-Preview-RL-Data", "name": "Still", "size": "30,000 samples, 10 MB", "summary": "Collection of question and verifiable answer extracted from Math, Numina, and Aime. # Additional notes (for additional assets) MIT-8 benchmark Proof-Pile - ArXiv.math (10GB) - Open-source math textbooks (50MB) - Formal mathematics libraries (500MB) - Lean mathlib and other Lean repositories - Isabelle AFP - Coq mathematical components and other Coq repositories - HOL Light - set.mm - Mizar Mathematical Library - Math Overflow and Math Stack Exchange (2.5GB) - Wiki-style sources (50MB) - ProofWiki - Wikipedia math articles - MATH dataset (6MB) https://huggingface.co/datasets/HuggingFaceTB/cosmopedia https://huggingface.co/datasets/open-web-math/open-web-math https://github.com/LiveCodeBench/LiveCodeBench https://huggingface.co/datasets/LLM360/MegaMath https://huggingface.co/datasets/math-ai/AutoMathText https://arxiv.org/abs/2402.07625 https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k AMPS (Khan + Mathematica) Dataset OCR: https://arxiv.org/pdf/2502.18443"}, {"download": "https://huggingface.co/datasets/glaiveai/reasoning-v1-20m", "name": "GlaiveAI", "notes": "Comes with no verification of correctness", "size": "20 million examples, 87 GB", "summary": "Traces from DeepSeek R1"}, {"name": "Stack V2", "size": "5,500,000,000 samples, 67 TB", "summary": "Scrapping of the Software Heritage archive, which contains software source code, in September 2023."}, {"companion": "https://arxiv.org/pdf/2309.17452", "download": "https://huggingface.co/datasets/AI-MO/NuminaMath-TIR", "name": "Numina-Tool", "size": "70,000 samples, 150 MB", "summary": "Subset numina problems solved with tool-use."}, {"companion": "https://arxiv.org/abs/2312.17120", "download": "https://huggingface.co/datasets/GAIR/MathPile", "name": "MathPile", "size": "Approximately 9.5 billion tokens.", "summary": "Dataset collected by GAIR (the lab behind LIMO) by compiling textbook, arXiv, ProofWiki, and filtering common crawl."}, {"companion": "https://arxiv.org/pdf/2105.09938", "download": "https://huggingface.co/datasets/codeparrot/apps", "name": "APPS", "notes": "AlphaCode mentions that test coverage was insufficient, leading to false positive.", "size": "10,000 problems", "summary": "10,000 programming problems with python solutions and test cases for correctness.  Curated from Codewars, AtCoder, Kattis, and Codeforces."}, {"download": "https://huggingface.co/datasets/greengerong/leetcode", "name": "LeetCode", "notes": "Unclear how the dataset was collected, comes with no unit tests", "size": "2,000 samples, 7 MB (for the small version I found on HuggingFace)", "summary": "Website with programming puzzles, typically used to prepare coding interviews."}, {"companion": "https://arxiv.org/abs/2502.02737", "download": "https://huggingface.co/datasets/HuggingFaceTB/finemath", "name": "FineMath", "summary": "Filtered from CommonCrawl by HuggingFace for SmolLM focused on Math domain."}, {"companion": "https://arxiv.org/pdf/1705.04146", "download": "https://huggingface.co/datasets/deepmind/aqua_rat", "name": "AQuA (Algebra Question Answering)", "size": "98,000 samples; 52 MB", "summary": "DeepMind Dataset built by extracting 34,000 questions from undergrad, and grad student admission test (GMAT and GRE), with answer and rational scrapped on the web. Plus crowdsourcing to provide similar questions."}, {"companion": "none, but the datasets was key to https://arxiv.org/abs/2303.04488", "download": "https://huggingface.co/datasets/Simontwice/premise_selection_in_isabelle", "name": "Isabelle Premise Selection", "size": "4,000,000 samples", "summary": "Datasets of proofs in Isabelle collected from the Archive of Formal Proofs (https://www.isa-afp.org/). Useful to study premise selection (i.e. selecting potential lemmas to apply mid-proof)."}, {"companion": "https://arxiv.org/abs/2406.17557", "download": "https://huggingface.co/datasets/HuggingFaceFW/fineweb", "name": "FineWeb", "summary": "Filtered from CommonCrawl by HuggingFace, seems to be slightly worse than DCLM."}, {"download": "https://huggingface.co/datasets/open-r1/OpenR1-Math-220k", "name": "Open-R1-220k", "notes": "They have other datasets, see https://huggingface.co/open-r1", "size": "225,000 samples, 8 GB.", "summary": "This part of an ongoing effort by HuggingFace to reproduce DeepSeek R1. Consists of DeepSeek R1 traces answering problems from NuminaMath, verified with Math Verify, a HuggingFace parser to check numerical equality."}, {"download": "https://huggingface.co/datasets/MatrixStudio/Codeforces-Python-Submissions", "name": "CodeForces", "notes": "Quite extensive datasets with unit test", "size": "700,000 samples, 1.7 GB", "summary": "Website with competitive programming puzzles."}, {"companion": "https://arxiv.org/abs/2502.13124", "download": "https://huggingface.co/datasets/facebook/natural_reasoning", "name": "Natural Reasoning", "summary": "From Fair RAM group, questions filtered from DCLM and FineMath, with answers provided by Llama."}]');
    let tagData = JSON.parse('{"benchmark": "datasets often used for benchmarking purposes", "competition": "datasets from various competitions", "education": "education-related datasets", "math": "dataset focused on math"}');

    // Create an array of available tag names from the tagData.
    let availableTags = Object.keys(tagData);

    // Render the tags using custom tooltip (with the tag's description shown on hover).
    function renderTags() {
      const tagsContainer = document.getElementById("tags-container");
      tagsContainer.innerHTML = "";
      availableTags.forEach(tag => {
        // Create the tag element.
        const tagElem = document.createElement("span");
        tagElem.classList.add("tag");
        tagElem.textContent = tag;
        tagElem.addEventListener("click", () => {
          tagElem.classList.toggle("selected");
          filterDatasets();
        });

        // Create tooltip element.
        const tooltipText = document.createElement("span");
        tooltipText.classList.add("tooltiptext");
        tooltipText.textContent = tagData[tag];

        // Wrap the tag element in a tooltip container.
        const tooltipContainer = document.createElement("span");
        tooltipContainer.classList.add("tooltip");
        tooltipContainer.appendChild(tagElem);
        tooltipContainer.appendChild(tooltipText);

        tagsContainer.appendChild(tooltipContainer);
      });
    }

    // Retrieve the currently selected tags.
    function getSelectedTags() {
      return Array.from(document.getElementsByClassName("tag"))
        .filter(tagElem => tagElem.classList.contains("selected"))
        .map(tagElem => tagElem.textContent.toLowerCase());
    }

    // Render tags for a dataset with tooltips.
    function renderDatasetTags(tags) {
      if (!tags) return "";
      return tags.map(tag => {
        let tooltip = tagData[tag.toLowerCase()] || "";
        return `<span class="tooltip dataset-tag">${tag}
                  <span class="tooltiptext">${tooltip}</span>
                </span>`;
      }).join(" ");
    }

    // Filter and render datasets based on search input, selected tags, and sort order.
    function filterDatasets() {
      const searchTerm = document.getElementById("search-input").value.toLowerCase();
      const selectedTags = getSelectedTags();
      const sortValue = document.getElementById("sort-select").value;
      const datasetsContainer = document.getElementById("datasets-container");
      datasetsContainer.innerHTML = "";

      // Filter datasets by checking search term and tag match.
      let filtered = datasets.filter(ds => {
        // Basic search match in name, summary, or notes.
        const matchesSearch = ds.name.toLowerCase().includes(searchTerm) ||
                              ds.summary.toLowerCase().includes(searchTerm) ||
                              ds.notes.toLowerCase().includes(searchTerm);
        // For tag matching, check the ds.tags field if available, or fallback to text search.
        const matchesTags = selectedTags.length === 0 ||
          selectedTags.every(tag => {
            const inTags = ds.tags ? ds.tags.map(t => t.toLowerCase()).includes(tag) : false;
            const inText = ds.name.toLowerCase().includes(tag) ||
                           ds.summary.toLowerCase().includes(tag) ||
                           ds.notes.toLowerCase().includes(tag);
            return inTags || inText;
          });
        return matchesSearch && matchesTags;
      });

      // Sort datasets based on selected sort option.
      if (sortValue === "name") {
        filtered.sort((a, b) => a.name.localeCompare(b.name));
      } else if (sortValue === "date_latest") {
        filtered.sort((a, b) => new Date(b.date) - new Date(a.date));
      } else if (sortValue === "date_earliest") {
        filtered.sort((a, b) => new Date(a.date) - new Date(b.date));
      }

      // Render each filtered dataset.
      filtered.forEach(ds => {
        const dsElem = document.createElement("div");
        dsElem.classList.add("dataset");

        // Minimal view: title, summary, size, creation date, and tags.
        dsElem.innerHTML = `
          <div class="dataset-minimal">
            <div class="dataset-title">${ds.name}</div>
            <div class="dataset-detail"><strong>Summary:</strong> ${ds.summary}</div>
            <div class="dataset-detail"><strong>Size:</strong> ${ds.size}</div>
            <div class="dataset-detail"><strong>Date Created:</strong> ${ds.date || "N/A"}</div>
          </div>
          <div class="dataset-full">
            <div class="dataset-detail"><strong>Download Location:</strong> <a href="${ds.download}" target="_blank">${ds.download}</a></div>
            <div class="dataset-detail"><strong>Notes:</strong> ${ds.notes}</div>
            <div class="dataset-detail"><strong>Companion Paper:</strong> ${
              ds.companion === "Not available" 
                ? "Not available" 
                : `<a href="${ds.companion}" target="_blank">${ds.companion}</a>`
            }</div>
            <div class="dataset-detail"><strong>Tags:</strong> ${renderDatasetTags(ds.tags)}</div>
          </div>
        `;
        // Toggle the display of the full details on click.
        dsElem.addEventListener("click", function() {
          const details = dsElem.querySelector(".dataset-full");
          details.style.display = (details.style.display === "block") ? "none" : "block";
        });
        datasetsContainer.appendChild(dsElem);
      });
    }

    // Listen for changes in the search input and sort selection.
    document.getElementById("search-input").addEventListener("input", filterDatasets);
    document.getElementById("sort-select").addEventListener("change", filterDatasets);

    // Initial render.
    renderTags();
    filterDatasets();
  </script>
</body>
</html>