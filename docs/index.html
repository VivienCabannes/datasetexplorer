<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>Dataset Explorer</title>
  <link rel="stylesheet" type="text/css" href="static/style.css">

  <!-- Use Katex to render math if needed -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.css" integrity="sha384-zh0CIslj+VczCZtlzBcjt5ppRcsAmDnRem7ESsYwWwg3m/OaJ2l4x7YBZl9Kxxib" crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.js" integrity="sha384-Rma6DA2IPUwhNxmrB/7S3Tno0YY7sFu9WSYMCuulLhIqYSGZ2gKCJWIqhBWqMQfh" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/contrib/auto-render.min.js" integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh" crossorigin="anonymous"
      onload="renderMathInElement(document.body);"></script>
</head>
<body>
  <div class="header">
    <h1>Dataset Explorer</h1>
    <div class="top-buttons">
      <button id="help-button" class="round-button" title="click for help">?</button>
      <button id="theme-toggle-button" class="round-button" title="toggle theme">ðŸŒ™</button>
    </div>
  </div>
  
  <!-- Help text block, initially hidden -->
  <div id="help-text" style="display:none; margin: 10px 0;">
    <p>
      <strong>Welcome to the Dataset Explorer!</strong><br/>
      Use the filters to narrow your dataset choices, type in the search bar to find datasets by name or summary, and sort them as needed. 
      Click on a dataset to see further details.<br/><br/>
      âš  Some cards display a warning sign, indicating information gathered from a quick internet search without deep verification.
    </p>
  </div>
  
  <!-- Tags rendered below the help button -->
  <div id="tags-container">
    <!-- Tags will be dynamically inserted here -->
  </div>
  
  <!-- Controls row: Search bar (left) & Order menu (right) -->
  <div class="controls-row">
    <div class="search-bar">
      <input type="text" id="search-input" placeholder="Search datasets...">
    </div>
    <div class="sort-bar">
      <label for="sort-select">Sort by:</label>
      <select id="sort-select">
        <option value="name">Name (A-Z)</option>
        <option value="date_latest">Date (Latest)</option>
        <option value="date_earliest">Date (Earliest)</option>
      </select>
    </div>
  </div>
  
  <!-- Container for displaying datasets -->
  <div id="datasets-container">
    <!-- Matching datasets will be displayed here -->
  </div>

  <!-- Render markdown -->
  <script id="markdown-it" src="static/markdownit.js"></script>
  <script id="data" type="application/json">[{"companion": "https://arxiv.org/abs/2103.03874", "date": "2023-11-15", "download": "https://huggingface.co/datasets/HuggingFaceH4/MATH-500", "name": "MATH-500", "notes": "The dataset includes problems across various topics such as algebra, geometry, and calculus, providing a comprehensive evaluation framework for mathematical reasoning in AI systems.", "size": "500 problems.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA curated subset of 500 problems from the MATH benchmark, designed to evaluate mathematical problem-solving abilities of language models.", "tags": ["human", "QA", "benchmark", "answer:verifiable", "question:human", "answer:human", "mathematics"]}, {"companion": "https://arxiv.org/abs/1811.00937", "date": "2018-11-02", "download": "https://www.tau-nlp.org/commonsenseqa", "name": "CommonsenseQA", "notes": "Workers were given a concept from ConceptNet and asked to formulate a question involving that concept in a commonsense way, providing a correct answer and tricky distractors. Questions often require implicit everyday knowledge or reasoning about likely events. It\u0027s part of the GLUE-style leaderboards for commonsense reasoning.", "size": "12,247 questions, each with 5 answer choices (Train: 9,741; Dev: 1,221; Test: 1,285).", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA crowd-sourced multiple-choice question set targeting commonsense knowledge.", "tags": ["crowd-sourced", "QA", "atomic", "question:crowd-sourced", "answer:crowd-sourced", "nlp"]}, {"companion": "https://arxiv.org/abs/2410.07985", "date": "2024-10-15", "download": "https://huggingface.co/datasets/KbsdJames/Omni-MATH", "name": "Omni-MATH", "notes": "Sourced from regional to international Olympiads via the Art of Problem Solving platform, covering over 33 sub-domains with varying difficulty levels.", "size": "4,428 problems, approximately 7 MB.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA benchmark of 4,428 Olympiad-level math problems for evaluating LLMs\u0027 mathematical reasoning.", "tags": ["human", "QA", "atomic", "answer:verifiable", "question:human", "answer:human", "mathematics"]}, {"companion": "https://arxiv.org/abs/2402.07625", "date": "2024-02-12", "download": "https://huggingface.co/datasets/math-ai/AutoMathText", "name": "AutoMathText", "notes": "AutoMathText was developed using Autonomous Data Selection (AutoDS), employing language models as zero-shot verifiers to autonomously curate high-quality mathematical content. Empirical evaluations demonstrated that language models continually pretrained on AutoMathText achieved substantial improvements on mathematical benchmarks, underscoring the dataset\u0027s efficacy in enhancing mathematical reasoning capabilities.", "size": "Approximately 200 GB.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA 200 GB dataset of mathematical texts curated autonomously to enhance language models\u0027 mathematical reasoning.", "tags": ["human", "monolithic", "compilation", "mathematics"]}, {"companion": "https://medium.com/data-science-in-your-pocket/deepseek-r1-5b-new-deepseek-r1-model-tops-openai-o1-in-math-18dacfd04f1f", "date": "2025-02-10", "download": "https://huggingface.co/datasets/agentica-org/DeepScaleR-Preview-Dataset", "name": "DeepScaleR", "notes": "The dataset aggregates problems from AIME (1984\u20132023), AMC (prior to 2023), Omni-MATH, and Still datasets. It was utilized in training the DeepScaleR-1.5B-Preview model, which achieved notable performance improvements in mathematical reasoning tasks.", "size": "Approximately 40,000 problem-answer pairs.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset comprising approximately 40,000 unique mathematical problem-answer pairs from AIME, AMC, Omni-MATH, and Still datasets.", "tags": ["human", "QA", "compilation", "mathematics"]}, {"companion": "https://aclanthology.org/D13-1160/", "date": "2013-10-01", "download": "https://huggingface.co/datasets/Stanford/web_questions", "name": "WebQuestions", "notes": "Questions were sourced using the Google Suggest API and answers were crowdsourced via Amazon Mechanical Turk, focusing on single named entities. The dataset is widely used for evaluating semantic parsing and knowledge base question answering systems.", "size": "6,642 question-answer pairs.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset of 6,642 question-answer pairs designed for evaluating open-domain question answering systems using the Freebase knowledge graph.", "tags": ["human", "QA", "atomic", "answer:verifiable", "question:human", "answer:crowd-sourced", "nlp"]}, {"companion": "https://arxiv.org/abs/1810.12885", "date": "2018-10-30", "download": "https://sheng-z.github.io/ReCoRD-explorer/", "name": "ReCoRD (Reading Comprehension with Commonsense Reasoning Dataset)", "notes": "Each query is formed by masking out a noun phrase in a sentence from a news article, and the model must select the correct entity from the passage to fill in the blank, utilizing both the passage and broader commonsense knowledge. The dataset was added to the SuperGLUE benchmark in July 2019.", "size": "120,000+ queries from ~70,000 news articles.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA cloze-style reading comprehension dataset with over 120,000 queries from approximately 70,000 news articles, requiring commonsense reasoning.", "tags": ["crowd-sourced", "QA", "filtration", "with rationale", "answer:verifiable", "question:synthetic", "answer:human", "nlp"]}, {"companion": "https://arxiv.org/abs/2105.09938", "date": "2021-05-20", "download": "https://huggingface.co/datasets/codeparrot/apps", "name": "APPS (Automated Programming Progress Standard)", "notes": "Problems are sourced from platforms like Codewars, AtCoder, Kattis, and Codeforces. Each problem includes a description, multiple solutions, and test cases. Difficulty levels range from introductory to competition-level challenges.", "size": "10,000 problems with solutions and test cases.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA benchmark dataset of 10,000 Python programming problems for evaluating code generation models.", "tags": ["crowd-sourced", "QA", "benchmark", "tool use", "question:human", "answer:human", "programming"]}, {"companion": "https://arxiv.org/abs/2303.17760", "date": "2023-03-31", "download": "https://huggingface.co/datasets/camel-ai/physics", "name": "CAMEL (Communicative Agents for \u0027Mind\u0027 Exploration of Large Scale Language Model Society)", "notes": "The dataset encompasses 25 physics topics, each with 25 subtopics, containing 32 problems per subtopic. Generated by GPT-4 without verification for correctness; users should independently validate content before use.", "size": "20,000 problem-solution pairs.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA synthetic dataset of 20,000 physics problem-solution pairs generated using GPT-4.", "tags": ["synthetic", "QA", "atomic", "answer:synthetic", "question:synthetic", "physics"]}, {"companion": "https://arxiv.org/abs/1903.00161", "date": "2019-03-01", "download": "https://leaderboard.allenai.org/drop", "name": "DROP (Discrete Reasoning Over Paragraphs)", "notes": "Questions necessitate operations like addition, counting, or sorting over passages, challenging models beyond simple extraction. Answers can be numbers, dates, or text spans, sometimes lists or yes/no. Human performance is around 96%, while initial models achieved approximately 32.7% F1 score.", "size": "Approximately 96,567 QA pairs on 6,713 paragraphs.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA reading comprehension benchmark requiring discrete reasoning over paragraphs.", "tags": ["crowd-sourced", "QA", "atomic", "answer:verifiable", "question:crowd-sourced", "answer:human", "mathematics"]}, {"companion": "https://arxiv.org/abs/2406.17557", "date": "2024-06-25", "download": "https://huggingface.co/datasets/HuggingFaceFW/fineweb", "name": "FineWeb", "notes": "FineWeb was developed by Hugging Face to optimize large language model training. While it has shown improved performance over datasets like RefinedWeb, subsequent datasets such as DCLM have reported further enhancements.", "size": "Approximately 15 trillion tokens (~44 TB).", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA 15-trillion-token dataset of cleaned and deduplicated English web data from 96 Common Crawl snapshots.", "tags": ["human", "monolithic", "filtration"]}, {"companion": "https://arxiv.org/abs/1611.09830", "date": "2016-11-26", "download": "https://github.com/Maluuba/newsqa", "name": "NewsQA", "notes": "Questions were created by crowdworkers who only had access to article summaries, encouraging curiosity-driven inquiries. Answers are spans of text within the full articles, requiring models to perform reasoning beyond simple word matching. Some questions are unanswerable, adding complexity to the dataset.", "size": "119,633 question\u2013answer pairs across 12,744 articles.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA crowd-sourced dataset of questions and answers based on CNN news articles, designed to evaluate machine comprehension.", "tags": ["crowd-sourced", "QA", "atomic", "answer:verifiable", "question:crowd-sourced", "answer:crowd-sourced", "nlp"]}, {"companion": "https://aclanthology.org/D15-1237/", "date": "2015-09-01", "download": "https://www.microsoft.com/en-us/download/details.aspx?id=52419", "name": "WikiQA", "notes": "Derived from Bing query logs, each question is linked to a Wikipedia page\u0027s summary section sentences, with 1,473 labeled as correct answers. Approximately 40% of questions have no correct answer in the provided sentences, making it suitable for answer sentence selection and answer triggering tasks.", "size": "3,047 questions and 29,258 sentences.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset of question and sentence pairs for open-domain question answering research.", "tags": ["human", "QA", "filtration", "answer:verifiable", "question:human", "answer:human", "nlp"]}, {"companion": "https://arxiv.org/abs/2412.09413", "date": "2024-12-22", "download": "https://huggingface.co/datasets/RUC-AIBOX/STILL-3-Preview-RL-Data", "name": "STILL-3 Preview RL Data", "notes": "Sourced from MATH, NuminaMathCoT, and AIME (1983\u20132023). Each entry includes a \u0027question\u0027, \u0027messages\u0027 (formatted input using a chat template), and \u0027answer\u0027. Part of the STILL framework focusing on slow-thinking reasoning systems engaging in extended reasoning processes before solutions.", "size": "30,000 samples (~10 MB).", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset of 30,000 mathematical problems with verifiable answers, designed for reinforcement learning in mathematical reasoning.", "tags": ["human", "QA", "compilation", "answer:verifiable", "question:human", "answer:human", "mathematics"]}, {"companion": "https://arxiv.org/abs/2203.10244", "date": "2022-03-19", "download": "https://github.com/vis-nlp/ChartQA", "name": "ChartQA", "notes": "Includes 9,608 human-written questions and 23,111 machine-generated questions, covering diverse chart styles and topics. Questions require arithmetic operations, logical inference, and visual feature interpretation.", "size": "32,719 questions across 20,882 charts.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA benchmark dataset of 32,719 questions on 20,882 charts, designed to evaluate systems on visual and logical reasoning.", "tags": ["human", "QA", "benchmark", "answer:verifiable", "question:human", "answer:human", "mathematics"]}, {"companion": "https://arxiv.org/abs/2110.14168", "date": "2021-10-27", "download": "https://github.com/openai/grade-school-math", "name": "GSM8K (Grade School Math 8K)", "notes": "GSM8K was developed by OpenAI and Surge AI to evaluate and enhance the mathematical reasoning abilities of language models. Problems involve 2 to 8 steps, focusing on basic arithmetic operations. Solutions are provided in natural language with step-by-step reasoning.", "size": "8,500 problems (7,500 training, 1,000 test).", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset of 8,500 linguistically diverse grade-school math word problems requiring multi-step reasoning.", "tags": ["crowd-sourced", "QA", "benchmark", "answer:verifiable", "with rationale", "question:crowd-sourced", "answer:crowd-sourced", "rationale:human", "mathematics"]}, {"companion": "https://arxiv.org/abs/2406.11794", "date": "2024-06-17", "download": "https://huggingface.co/datasets/mlfoundations/dclm-baseline-1.0", "name": "DCLM (DataComp-LM)", "notes": "DCLM allows researchers to experiment with data curation strategies such as deduplication, filtering, and data mixing across model scales ranging from 412 million to 7 billion parameters. The benchmark includes effective pretraining recipes based on the OpenLM framework and a suite of 53 downstream evaluations.", "size": "240 trillion tokens.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA benchmark for controlled dataset experiments aimed at improving language models, featuring a standardized corpus of 240 trillion tokens from Common Crawl.", "tags": ["synthetic", "monolithic", "compilation", "benchmark", "nlp"]}, {"companion": "https://arxiv.org/abs/2112.09332", "date": "2021-12-16", "download": "https://huggingface.co/datasets/openai/webgpt_comparisons", "name": "WebGPT QA dataset", "notes": "Questions are sourced from Reddit\u0027s ELI5 subreddit and similar platforms. Human demonstrators used a text-based web browser to gather information and provide detailed, evidence-backed answers, including citations. The dataset was utilized to train WebGPT and serves as a benchmark for evaluating truthful, source-cited question answering. Due to web content licenses, the dataset is primarily available for research purposes.", "size": "Approximately 4,398 question-answer pairs with references.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset of question-answer pairs with supporting evidence, derived from OpenAI\u0027s WebGPT project, featuring human-generated answers with web-based references.", "tags": ["human", "QA", "tool use", "with rationale", "answer:verifiable", "question:human", "answer:human", "rationale:human", "nlp"]}, {"companion": "https://arxiv.org/abs/2406.01574", "date": "2024-06-03", "download": "https://huggingface.co/datasets/TIGER-Lab/MMLU-Pro", "name": "MMLU-Pro (Massive Multitask Language Understanding Professional)", "notes": "MMLU-Pro increases answer choices from four to ten to reduce random guessing and emphasizes reasoning over rote knowledge. Evaluations indicate a significant drop in accuracy compared to the original MMLU, providing a more rigorous assessment of language models\u0027 capabilities.", "size": "Approximately 12,000 multiple-choice questions.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eAn enhanced benchmark evaluating language models across 14 disciplines with complex, reasoning-focused multiple-choice questions.", "tags": ["human", "QA", "benchmark", "answer:verifiable", "question:human", "answer:human", "mathematics", "physics", "programming", "nlp"]}, {"companion": "https://arxiv.org/abs/1907.09190", "date": "2019-07-22", "download": "https://huggingface.co/datasets/eli5", "name": "ELI5 (Explain Like I\u0027m Five)", "notes": "Collected from Reddit\u0027s ELI5 forum, the dataset focuses on open-ended, explanatory questions (often \u0027Why\u0027 or \u0027How\u0027) and long-form human answers. It supports research in generative QA and has been integrated into benchmarks like KILT. Answers are not extractive, and multiple human answers may exist per question.", "size": "Approximately 270,000 QA pairs; a high-quality subset includes ~30,000.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset of long-form QA pairs from the Reddit ELI5 forum, containing complex open-ended questions and multi-paragraph human-written answers.", "tags": ["human", "QA", "atomic", "answer:human", "nlp"]}, {"companion": "https://arxiv.org/abs/2009.03300", "date": "2020-09-07", "download": "https://github.com/hendrycks/test", "name": "MMLU (Massive Multitask Language Understanding)", "notes": "Designed for zero-shot and few-shot evaluation, MMLU assesses both world knowledge and problem-solving abilities of language models. Subjects range from elementary mathematics to professional law and medicine. Some questions come from https://www.crackap.com/index.html", "size": "Approximately 16,000 multiple-choice questions.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA benchmark evaluating language models across 57 diverse subjects through multiple-choice questions.", "tags": ["human", "QA", "benchmark", "answer:verifiable", "question:human", "answer:human"]}, {"companion": "https://arxiv.org/abs/2309.16575", "date": "2023-09-28", "download": "https://github.com/lukemelas/mtob", "name": "MTOB (Machine Translation from One Book)", "notes": "MTOB challenges models to learn translation capabilities from limited resources, simulating low-resource language scenarios. It includes a comprehensive grammar book detailing Kalamang\u0027s linguistic features, a bilingual word list, and a small set of parallel sentences. Evaluations show that while current language models demonstrate promising results, they still fall short of human performance, highlighting the task\u0027s difficulty and the need for advancements in low-resource language processing.", "size": "573-page grammar book, 2,531-word bilingual word list, and 500 parallel sentence pairs.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA benchmark evaluating language models\u0027 ability to learn translation between English and Kalamang using a single grammar book.", "tags": ["human", "QA", "benchmark", "answer:verifiable", "question:human", "answer:human", "nlp"]}, {"companion": "https://arxiv.org/abs/1710.03957", "date": "2017-10-11", "download": "http://yanran.li/dailydialog", "name": "DailyDialog", "notes": "The dialogues are human-written, focusing on daily communication topics such as relationships and ordinary life. Each utterance is annotated with emotion and dialogue act labels, facilitating research in emotion recognition and dialogue act classification. The dataset is often used for training and evaluating chit-chat dialogue models.", "size": "13,118 dialogues, totaling 87,170 utterances (approximately 7 turns per dialogue).", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA high-quality multi-turn dialogue dataset of 13,118 conversations covering various everyday topics.", "tags": ["human", "dialog", "atomic", "nlp"]}, {"companion": "https://arxiv.org/abs/2210.17517", "date": "2022-10-31", "download": "https://huggingface.co/datasets/allenai/lila", "name": "L\u012bla", "notes": "Includes splits: L\u012bla-IID (in-distribution), L\u012bla-OOD (out-of-distribution), and L\u012bla-Robust (linguistic variants).", "size": "Over 140,000 questions across 23 tasks.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA benchmark of over 140,000 natural language math questions annotated with Python programs and instructions.", "tags": ["compilation", "QA", "benchmark", "answer:verifiable", "question:human", "answer:human", "mathematics"]}, {"companion": "https://arxiv.org/abs/2502.00203", "date": "2025-03-18", "download": "https://huggingface.co/datasets/nvidia/Llama-Nemotron-Post-Training-Dataset-v1", "name": "Llama-Nemotron-Post-Training-Dataset-v1", "notes": "The dataset encompasses multiple subsets categorized into five main splits: code (approximately 1.56 million samples), math (approximately 13.1 million samples), science (approximately 484,000 samples), chat (approximately 39,800 samples), and safety (approximately 31,400 samples). It was developed to fine-tune models for improved accuracy and reliability.", "size": "Approximately 15 million samples, totaling around 6 GB.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA comprehensive dataset developed by NVIDIA to enhance the post-training of large language models across various domains.", "tags": ["synthetic", "compilation", "programming", "mathematics", "physics", "nlp"]}, {"companion": "https://github.com/open-thoughts/open-thoughts", "date": "2025-01-28", "download": "https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k", "name": "OpenThoughts-114k", "notes": "Developed by Bespoke Labs and the DataComp community, this dataset includes problem statements with detailed reasoning traces generated by the DeepSeek-R1 model. It has been instrumental in training models like OpenThinker-7B and OpenThinker-32B, demonstrating significant improvements in reasoning benchmarks.", "size": "114,000 examples (~3.55 GB).", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eAn open-source synthetic reasoning dataset with 114,000 examples across domains like mathematics, science, coding, and puzzles.", "tags": ["synthetic", "QA", "compilation", "answer:verifiable", "with rationale", "question:synthetic", "answer:synthetic", "rationale:synthetic", "mathematics", "physics", "programming"]}, {"companion": "https://arxiv.org/abs/2502.02737", "date": "2025-02-04", "download": "https://huggingface.co/datasets/HuggingFaceTB/stack-edu", "name": "Stack-Edu", "notes": "Developed to enhance the training of small language models like SmolLM2. Due to its substantial size, downloading requires Amazon S3. The dataset was curated using classifiers fine-tuned on code files annotated by Llama3.1-70B-Instruct to assess educational quality.", "size": "Approximately 167 million samples, totaling around 1 terabyte.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA curated subset of The Stack v2, focusing on high-quality educational code across 15 programming languages.", "tags": ["human", "monolithic", "filtration", "programming"]}, {"companion": "https://arxiv.org/abs/2309.17452", "date": "2024-06-15", "download": "https://huggingface.co/datasets/AI-MO/NuminaMath-TIR", "name": "NuminaMath-TIR", "notes": "NuminaMath-TIR was developed to enhance the mathematical reasoning capabilities of language models by incorporating tool-based solutions. Each problem is paired with a solution that combines natural language reasoning and executable code, allowing models to learn effective tool utilization in problem-solving. The dataset includes problems from various mathematical competitions, providing a diverse set of challenges.", "size": "Approximately 72,540 samples, totaling around 148 MB.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset of approximately 72,540 mathematical competition problems with solutions generated using Tool-Integrated Reasoning (TIR).", "tags": ["synthetic", "QA", "compilation", "answer:verifiable", "with rationale", "question:human", "answer:synthetic", "rationale:synthetic", "tool use", "mathematics"]}, {"companion": "https://arxiv.org/abs/1905.07830", "date": "2019-05-19", "download": "https://github.com/rowanz/hellaswag", "name": "HellaSwag", "notes": "HellaSwag\u0027s questions are trivial for humans (\u003e95% accuracy) but challenging for state-of-the-art models (\u003c48% accuracy). The dataset employs Adversarial Filtering to generate plausible but incorrect answer choices, making it a robust benchmark for commonsense reasoning.", "size": "Approximately 70,000 multiple-choice questions.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset of ~70,000 adversarially filtered multiple-choice questions for evaluating commonsense natural language inference.", "tags": ["human", "QA", "benchmark", "answer:verifiable", "question:human", "answer:human", "nlp"]}, {"download": "Various formatted datasets based on historical AMC questions are available, e.g., https://huggingface.co/datasets/AI-MO/aimo-validation-amc.", "name": "AMC (American Mathematics Competitions)", "notes": "The competition lasts 75 minutes for levels 10 and 12, 40 minutes for level 8.", "size": "25 multiple-choice questions per year per level.", "summary": "US middle and high school examinations. THe competition is divided in three levels AMC 8, AMC 10, AMC 12 for students below 14, 16 and 18 years old.", "tags": ["human", "QA", "dialog", "atomic", "benchmark", "answer:human", "answer:verifiable", "question:human", "mathematics"]}, {"companion": "https://arxiv.org/abs/2412.09413", "date": "2024-12-22", "download": "https://huggingface.co/datasets/RUC-AIBOX/long_form_thought_data_5k", "name": "STILL Long Format", "notes": "Questions are sourced from platforms like NuminaMath, AIME, Leetcode, OpenCoder, Camel, Gaokao (Chinese A-level), and RiddleSense. Each entry includes the problem statement (\u0027question\u0027), the model\u0027s response with \u0027thought\u0027 and \u0027solution\u0027 components (\u0027combined_text\u0027), and the domain (e.g., mathematics, physics, chemistry, biology, coding, puzzles). The dataset is structured in JSONL format.", "size": "5,000 samples (~20 MB).", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset of 5,000 complex problems emphasizing long-form answers across various domains.", "tags": ["human", "QA", "compilation", "answer:verifiable", "with rationale", "question:human", "answer:human", "rationale:human", "mathematics", "programming"]}, {"companion": "https://arxiv.org/abs/2108.07732", "date": "2021-08-16", "download": "https://github.com/google-research/google-research/tree/master/mbpp", "name": "MBPP (Mostly Basic Python Problems)", "notes": "Problems cover programming fundamentals and standard library functionalities. The dataset is structured for both few-shot and fine-tuning evaluations, with specific task ID splits for training, validation, and testing.", "size": "974 problems.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset of 974 crowd-sourced Python programming problems designed for entry-level programmers, each with a task description, code solution, and three test cases.", "tags": ["crowd-sourced", "QA", "atomic", "answer:verifiable", "question:crowd-sourced", "answer:human", "programming"]}, {"companion": "https://arxiv.org/abs/1801.07243", "date": "2018-01-22", "download": "https://github.com/facebookresearch/ParlAI/tree/main/parlai/tasks/personachat", "name": "PersonaChat", "notes": "Each participant is assigned a persona consisting of 4-5 profile sentences, resulting in 1,155 unique personas. The dataset is designed to facilitate personalized dialogue modeling, aiming to build agents that can naturally integrate persona information into conversations.", "size": "10,907 dialogues with 162,064 utterances.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dialogue dataset where paired crowdworkers engage in conversations, each adopting a given persona.", "tags": ["crowd-sourced", "dialog", "atomic", "question:crowd-sourced", "answer:crowd-sourced", "nlp"]}, {"companion": "https://arxiv.org/abs/2101.02235", "date": "2021-01-06", "download": "https://github.com/eladsegal/strategyqa", "name": "StrategyQA", "notes": "Questions are designed to require implicit reasoning steps, with decompositions and evidence paragraphs provided for each. For example, \u0027Could Brooke Shields have been friends with Marilyn Monroe?\u0027 requires understanding their birth and death dates to conclude they could not have been friends. The dataset assesses creative multi-hop reasoning and is often used to evaluate explanation generation.", "size": "2,780 questions (Train: 2,290; Test: 490).", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset of 2,780 yes/no questions requiring implicit multi-hop reasoning, each paired with a decomposition and evidence paragraphs.", "tags": ["crowd-sourced", "QA", "atomic", "with rationale", "answer:verifiable", "question:crowd-sourced", "answer:crowd-sourced", "rationale:human", "nlp"]}, {"companion": "https://arxiv.org/abs/2311.12022", "date": "2023-11-20", "download": "https://github.com/idavidrein/gpqa", "name": "GPQA Diamond", "notes": "The Diamond subset represents the most difficult questions within the GPQA benchmark, designed to be \u0027Google-proof\u0027 and requiring deep subject matter expertise. Expert validators achieved 65% accuracy, while highly skilled non-experts reached only 34%, despite unrestricted web access and extended time per question. This subset serves as a rigorous test for assessing advanced reasoning and knowledge in language models.", "size": "198 multiple-choice questions.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA subset of 198 exceptionally challenging, expert-crafted multiple-choice questions in biology, physics, and chemistry from the GPQA benchmark.", "tags": ["human", "QA", "benchmark", "answer:verifiable", "question:human", "answer:human", "biology", "physics", "chemistry"]}, {"companion": "https://huggingface.co/blog/cosmopedia", "date": "2024-03-20", "download": "https://huggingface.co/datasets/HuggingFaceTB/cosmopedia", "name": "Cosmopedia", "notes": "Cosmopedia is organized into eight subsets based on the source materials used as seed data for generation: web_samples_v1, web_samples_v2, stanford, stories, wikihow, openstax, khanacademy, and auto_math_text. The dataset was developed to facilitate language model pre-training by providing diverse synthetic data across a broad range of topics.", "size": "Over 30 million documents totaling approximately 25 billion tokens.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA synthetic dataset of over 30 million documents generated by Mixtral-8x7B-Instruct-v0.1, including textbooks, blog posts, stories, and WikiHow articles.", "tags": ["synthetic", "monolithic", "compilation", "nlp"]}, {"date": "2025-02", "download": "https://huggingface.co/datasets/AI-MO/NuminaMath-1.5", "name": "NuminaMath 1.5", "notes": "NuminaMath 1.5 enhances its predecessor by adding problem metadata for verifiability, manually curated data from various contests, and removing synthetic datasets like \u0027synthetic_amc\u0027 due to their negative impact on performance. The dataset sources include Art of Problem Solving community problems, American Mathematics Competitions, Chinese high school math exercises, and international mathematics olympiad problems.", "size": "Approximately 900,000 samples, totaling 531 MB.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset of over 900,000 competition-level math problems with Chain of Thought (CoT) solutions.", "tags": ["human", "QA", "compilation", "answer:verifiable", "with rationale", "question:human", "answer:human", "rationale:human", "mathematics"]}, {"companion": "https://arxiv.org/abs/1705.03551", "date": "2017-05-09", "download": "http://nlp.cs.washington.edu/triviaqa/", "name": "TriviaQA", "notes": "Questions are authored by trivia enthusiasts and paired with evidence documents from Wikipedia and the web. The dataset includes both \u0027unfiltered\u0027 and \u0027filtered\u0027 subsets, with the latter ensuring answers appear in the provided texts. It is designed to challenge models with complex, compositional questions requiring cross-sentence reasoning.", "size": "Over 650,000 question-answer-evidence triples.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA large-scale reading comprehension dataset with 95,000 trivia questions and corresponding evidence documents.", "tags": ["human", "QA", "compilation", "answer:verifiable", "question:human", "answer:human", "nlp"]}, {"companion": "https://arxiv.org/abs/2305.20050", "date": "2023-05-31", "download": "https://github.com/openai/prm800k", "name": "PRM800K", "notes": "Developed by OpenAI to support research in training reliable models for complex multi-step reasoning tasks. The dataset provides step-level human feedback, enabling the development of process reward models (PRMs) that evaluate intermediate steps in problem-solving. Some annotations have been reported to be incorrect, which may impact model training and evaluation.", "size": "800,000 annotated reasoning steps.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset of 800,000 step-level correctness labels for model-generated solutions to MATH problems.", "tags": ["human", "QA", "compilation", "answer:verifiable", "with rationale", "question:synthetic", "answer:synthetic", "rationale:human", "mathematics"]}, {"companion": "https://arxiv.org/abs/1808.07036", "date": "2018-08-21", "download": "https://quac.ai/", "name": "QuAC (Question Answering in Context)", "notes": "In each dialog, a student poses a sequence of freeform questions to learn about a hidden Wikipedia text, and a teacher answers by providing short excerpts from the text. Questions are often open-ended, unanswerable, or context-dependent. The dataset includes teacher\u0027s dialogue acts and answer summaries for some turns.", "size": "13,594 dialogs with 98,407 question-answer pairs.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dialogue-style QA dataset with 13,594 information-seeking dialogs (98,407 questions) based on Wikipedia articles.", "tags": ["crowd-sourced", "dialog", "atomic", "answer:verifiable", "question:crowd-sourced", "answer:human", "nlp"]}, {"companion": "https://arxiv.org/abs/1611.09268", "date": "2016-11-28", "download": "https://microsoft.github.io/msmarco/", "name": "MS MARCO QA (Microsoft MAchine Reading COmprehension Question Answering)", "notes": "MS MARCO QA includes queries with passages extracted from web documents retrieved by Bing. Answers are often segments from passages or human-written summaries. The dataset is utilized for tasks like passage ranking and reading comprehension, and includes yes/no and unanswerable questions.", "size": "Approximately 1,010,916 query-passage pairs; 182,669 queries with manually generated answers.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA large-scale machine reading comprehension dataset featuring real anonymized Bing search queries with corresponding passages and human-generated answers.", "tags": ["human", "QA", "compilation", "answer:verifiable", "question:human", "answer:human", "nlp"]}, {"companion": "https://arxiv.org/abs/1606.05250", "date": "2016-06-16", "download": "https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json", "name": "SQuAD1.1", "notes": "Primarily used for extractive QA evaluation with exact-match and F1 scoring. All answers are spans from the passage; no unanswerable questions are included in v1.1.", "size": "107,785 question-answer pairs on 536 articles.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA reading comprehension dataset of over 100,000 crowd-sourced questions on Wikipedia articles, with answers as text spans from the corresponding passages.", "tags": ["crowd-sourced", "QA", "atomic", "answer:verifiable", "question:crowd-sourced", "answer:human", "nlp"]}, {"companion": "https://arxiv.org/abs/2410.01560", "date": "2024-10-02", "download": "https://huggingface.co/datasets/nvidia/OpenMathInstruct-2", "name": "OpenMathInstruct-2", "notes": "The dataset was created to enhance the training of large language models in mathematical reasoning, with significant improvements in performance on benchmarks like MATH through augmentation using the Llama3.1 models.", "size": "14 million samples (~12 GB).", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA large-scale synthetic dataset of question-solution pairs, generated by augmenting MATH and GSM8K using Llama3.1 models.", "tags": ["synthetic", "QA", "compilation", "answer:verifiable", "with rationale", "question:synthetic", "answer:synthetic", "rationale:synthetic", "mathematics"]}, {"companion": "https://aclanthology.org/P18-1156", "date": "2018-04-21", "download": "https://github.com/duorc/duorc", "name": "DuoRC", "notes": "Comprises two subsets: SelfRC (~96k Q) where questions are answerable from a single plot, and ParaphraseRC (~90k Q) where questions require cross-document reasoning. Answers are free-form text, often phrases from the target plot.", "size": "186,089 unique QA pairs over 7,680 pairs of movie plots.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA reading comprehension dataset with ~186k QA pairs from parallel movie plot summaries.", "tags": ["crowd-sourced", "QA", "compilation", "answer:verifiable", "question:crowd-sourced", "answer:human", "nlp"]}, {"companion": "https://arxiv.org/abs/1905.09381", "date": "2019-05-21", "download": "https://github.com/princeton-vl/CoqGym", "name": "CoqGym", "notes": "CoqGym includes proofs from diverse domains such as mathematics, computer hardware, and programming languages. It provides structural information like abstract syntax trees (ASTs) and detailed proof trees, supporting the development of machine learning models for tactic prediction and proof generation. Additionally, it offers synthetic proofs generated from intermediate proof steps to augment training data.", "size": "70,856 proofs from 123 Coq projects.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset and learning environment comprising over 70,000 human-written Coq proofs from 123 projects.", "tags": ["human", "monolithic", "compilation", "programming"]}, {"date": "2025-03-27", "download": "https://huggingface.co/datasets/MatrixStudio/Codeforces-Python-Submissions", "name": "Codeforces Python Submissions", "notes": "The dataset includes problem statements, user submissions, and metadata. Many problems are accompanied by unit tests, facilitating automated assessment of solution correctness. It serves as a valuable resource for analyzing coding patterns, developing machine learning models for code generation and evaluation, and studying competitive programming methodologies.", "size": "Approximately 690,396 samples, totaling around 1.66 GB.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA comprehensive collection of Python solutions submitted to Codeforces, a competitive programming platform.", "tags": ["human", "monolithic", "filtration", "programming"]}, {"companion": "https://arxiv.org/abs/2502.01456", "date": "2025-02-03", "download": "https://huggingface.co/datasets/PRIME-RL/Eurus-2-RL-Data", "name": "Eurus-RL", "notes": "Eurus-RL aggregates problems from sources like NuminaMath-CoT, APPS, CodeContests, TACO, and Codeforces. The dataset underwent rigorous filtering to ensure problem solvability and answer correctness, including reformatting multiple-choice questions to open-ended formats and removing duplicates. It supports research in reinforcement learning for complex reasoning tasks.", "size": "Approximately 480,537 samples (~2 GB).", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset of mathematics and coding problems with verifiable outcomes for reinforcement learning training.", "tags": ["human", "compilation", "filtration", "mathematics", "programming"]}, {"companion": "Introduced in the data mix to the train Llema: https://arxiv.org/abs/2310.10631", "date": "2023-10-16", "download": "Available as part of the [Proof-Pile-2 mix](https://huggingface.co/datasets/EleutherAI/proof-pile-2).", "name": "Algebraic Stack", "notes": "The dataset is dominated by Python (65%), Isabelle (10%), and C++ (9%). Among others, the formal proofs are extracted from Lean mathlib library and Isabelle Archive of Formal Proofs.", "size": "Approximately 11 billion tokens (~40 MB).", "summary": "Code data coming from filtering the Stack, as well as extracting formal proofs.", "tags": ["human", "monolithic", "filtration", "programming"]}, {"companion": "https://arxiv.org/abs/1704.04683", "date": "2017-04-15", "download": "http://www.cs.cmu.edu/~glai1/data/race/", "name": "RACE (ReAding Comprehension from Examinations)", "notes": "Collected from English exams designed for middle and high school Chinese students, RACE includes questions generated by human experts to evaluate comprehensive understanding and reasoning abilities. Each question is multiple-choice with four options, only one of which is correct. The dataset is divided into RACE-M (middle school level) and RACE-H (high school level). Topics span various subjects, and answering many questions requires inference and world knowledge.", "size": "27,933 passages and 97,687 questions.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA large-scale reading comprehension dataset comprising 27,933 passages and 97,687 questions from English exams for Chinese students aged 12\u201318.", "tags": ["human", "QA", "compilation", "answer:verifiable", "question:human", "answer:human", "nlp"]}, {"companion": "https://arxiv.org/abs/1911.11641", "date": "2019-11-26", "download": "https://allenai.org/data/piqa", "name": "PIQA (Physical Interaction: Question Answering)", "notes": "Each question presents a goal and two possible solutions, focusing on naive physics and ergonomics knowledge. The task is to select the most plausible solution, evaluating models\u0027 grasp of physical commonsense.", "size": "21,000 questions (Train: 16k, Dev: 2k, Test: 3k).", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset for physical commonsense reasoning, featuring multiple-choice questions about everyday physical tasks.", "tags": ["crowd-sourced", "QA", "atomic", "question:crowd-sourced", "answer:crowd-sourced", "physics"]}, {"companion": "https://arxiv.org/abs/2402.14008", "date": "2024-02-21", "download": "https://huggingface.co/datasets/Hothan/OlympiadBench", "name": "OlympiadBench", "notes": "Problems are sourced from international and regional Olympiad competitions, including the Chinese College Entrance Exam (Gaokao). Each problem includes expert-level, step-by-step solutions. Notably, 57% of the problems contain visual elements, making it a multimodal benchmark. The dataset is bilingual, featuring problems in both Chinese and English, and spans a range of difficulties.", "size": "8,476 problems.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA benchmark of 8,476 Olympiad-level math and physics problems for evaluating reasoning in large models.", "tags": ["human", "QA", "atomic", "answer:verifiable", "with rationale", "question:human", "answer:human", "rationale:human", "mathematics", "physics"]}, {"companion": "https://arxiv.org/abs/2109.07958", "date": "2021-09-16", "download": "https://github.com/sylinrl/TruthfulQA", "name": "TruthfulQA", "notes": "Questions are crafted to test models on imitative falsehoods\u2014false answers that reflect widespread misconceptions. Evaluations indicate that larger language models tend to produce more such falsehoods, highlighting challenges in ensuring model truthfulness. TruthfulQA serves as a critical tool for measuring and improving the reliability of AI-generated information.", "size": "817 questions spanning 38 categories.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA benchmark of 817 questions across 38 categories designed to evaluate the truthfulness of language models by assessing their ability to avoid generating false answers that mimic common human misconceptions.", "tags": ["human", "QA", "benchmark", "answer:verifiable", "question:human", "answer:human", "nlp"]}, {"companion": "https://crfm.stanford.edu/2023/03/13/alpaca.html", "date": "2023-03-13", "download": "https://github.com/tatsu-lab/stanford_alpaca", "name": "Stanford Alpaca (52k Instruction Dataset)", "notes": "Created by prompting OpenAI\u0027s text-davinci-003 with 175 human-written seed tasks using Self-Instruct techniques. Covers tasks like writing, transformation, closed QA, and code. Widely used to fine-tune models to follow user instructions. Synthetic nature may introduce subtle biases from the base model.", "size": "52,000 instruction-response pairs (~80 MB JSON).", "summary": "A synthetic instruction-following dataset of 52,000 diverse instructions and GPT-generated responses.", "tags": ["synthetic", "monolithic", "compilation", "QA", "question:synthetic", "answer:synthetic", "nlp"]}, {"companion": "https://arxiv.org/abs/2310.10631", "date": "2023-10-16", "download": "https://huggingface.co/datasets/EleutherAI/proof-pile-2", "name": "ArXiv Subset of Proof-Pile-2", "notes": "The ArXiv subset was filtered and compiled from a 2023 snapshot by RedPajama. It supports training on research-level reasoning and was used in the development of the Llemma models for mathematical tasks.", "size": "Approximately 29 billion tokens.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA 29B-token subset of scientific and mathematical documents from arXiv, used in Proof-Pile-2 for training language models on formal and technical content.", "tags": ["human", "monolithic", "filtration", "mathematics"]}, {"companion": "https://arxiv.org/abs/2311.12022", "date": "2023-11-20", "download": "https://github.com/idavidrein/gpqa", "name": "GPQA (Graduate-Level Google-Proof Q\u0026A)", "notes": "GPQA\u0027s questions are designed to be exceptionally challenging and \u0027Google-proof,\u0027 with experts achieving 65% accuracy and highly skilled non-experts only 34%, despite unrestricted web access and extensive time per question. As of March 2024, Claude 3 Opus achieved approximately 60% accuracy on this benchmark, highlighting rapid advancements in AI capabilities.", "size": "448 multiple-choice questions.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA benchmark of 448 expert-crafted, graduate-level multiple-choice questions in biology, physics, and chemistry.", "tags": ["human", "QA", "benchmark", "answer:verifiable", "question:human", "answer:human", "biology", "physics", "chemistry"]}, {"companion": "https://aclanthology.org/N18-1023/", "date": "2018-06", "download": "https://cogcomp.seas.upenn.edu/multirc/", "name": "MultiRC (Multi-Sentence Reading Comprehension)", "notes": "Each question has multiple candidate answers, with one or more correct options. Models must evaluate each option independently. The dataset encourages compositional reasoning across sentence boundaries and is part of the SuperGLUE benchmark.", "size": "Approximately 10,000 questions on 1,000 paragraphs across 7 domains.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset of short paragraphs with multi-sentence questions requiring reasoning across sentences for answers.", "tags": ["crowd-sourced", "QA", "atomic", "answer:verifiable", "question:crowd-sourced", "answer:crowd-sourced", "nlp"]}, {"companion": "https://arxiv.org/abs/1808.07042", "date": "2018-08-21", "download": "https://stanfordnlp.github.io/coqa/", "name": "CoQA (Conversational Question Answering Challenge)", "notes": "Domains include children\u0027s stories, literature, and middle/high school textbooks. Answers can be yes/no or free text, not necessarily extractive. Each answer is accompanied by a highlighted evidence span from the passage. CoQA evaluates F1 of answer text and a conversational Exact Match.", "size": "8,000+ conversations, 127,000+ Q\u0026A pairs (Train: ~108k; Dev: 7.2k; Test: ~7.3k).", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset with 127k+ conversational Q\u0026A pairs across 8k+ dialogues spanning multiple domains.", "tags": ["crowd-sourced", "dialog", "atomic", "with rationale", "answer:verifiable", "question:crowd-sourced", "answer:crowd-sourced", "rationale:human", "nlp"]}, {"companion": "https://arxiv.org/abs/1803.05457", "date": "2018-03-14", "download": "https://huggingface.co/datasets/allenai/ai2_arc", "name": "AI2 ARC (Easy \u0026 Challenge)", "notes": "Each question has four answer choices and is accompanied by a 14M-sentence science corpus for retrieval. The Challenge set contains questions that simple retrieval or co-occurrence methods fail to answer correctly.", "size": "7,787 questions (5,228 Easy; 2,559 Challenge).", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset of 7,787 grade-school science multiple-choice questions, divided into Easy and Challenge sets.", "tags": ["compilation", "QA", "benchmark", "question:human", "answer:human", "nlp"]}, {"companion": "https://huggingface.co/blog/open-r1/update-2", "date": "2025-02-10", "download": "https://huggingface.co/datasets/open-r1/OpenR1-Math-220k", "name": "OpenR1-Math-220k", "notes": "Problems are sourced from NuminaMath 1.5, each accompanied by two to four reasoning traces generated by DeepSeek R1. The dataset includes two splits: \u0027default\u0027 with 94k problems achieving optimal performance after supervised fine-tuning, and \u0027extended\u0027 with 131k samples incorporating additional sources like \u0027cn_k12\u0027.", "size": "Approximately 220,000 samples, totaling around 8 GB.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset of ~220k math problems with reasoning traces generated by DeepSeek R1, sourced from NuminaMath 1.5.", "tags": ["synthetic", "QA", "compilation", "answer:verifiable", "with rationale", "question:human", "answer:synthetic", "rationale:synthetic", "mathematics"]}, {"companion": "https://arxiv.org/abs/2305.01210", "date": "2023-05-02", "download": "https://github.com/evalplus/evalplus", "name": "HumanEval+", "notes": "HumanEval+ increases the number of test cases approximately 80-fold compared to the original HumanEval dataset, aiming to reduce false positives in model evaluations and offer a more stringent assessment of functional correctness in generated code.", "size": "164 programming problems with over 1.3 million test cases.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eAn augmented version of the HumanEval dataset, enhancing test coverage to provide a more rigorous evaluation framework for code generation models.", "tags": ["human", "QA", "benchmark", "answer:verifiable", "question:human", "answer:human", "programming"]}, {"companion": "https://arxiv.org/abs/1904.09728", "date": "2019-04-22", "download": "https://leaderboard.allenai.org/socialiqa/submissions/get-started", "name": "SocialIQA", "notes": "Questions focus on understanding motivations, reactions, and emotions in everyday social scenarios, requiring models to infer beyond literal text.", "size": "Approximately 38,000 questions with three answer choices each.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA benchmark dataset with 38,000 multiple-choice questions assessing commonsense reasoning about social interactions.", "tags": ["crowd-sourced", "QA", "atomic", "question:crowd-sourced", "answer:crowd-sourced", "nlp"]}, {"companion": "https://arxiv.org/abs/1704.05179", "date": "2017-04-18", "download": "https://github.com/nyu-dl/SearchQA", "name": "SearchQA", "notes": "The dataset was constructed by collecting question-answer pairs from the J! Archive and augmenting them with text snippets retrieved using the questions as queries in Google Search. Each snippet includes metadata such as the snippet\u0027s URL and title. The dataset reflects a realistic information retrieval scenario, requiring models to extract answers from multiple, potentially noisy, text snippets. It has been used to benchmark machine comprehension and question-answering systems.", "size": "140,461 question-answer pairs, each with an average of 49.6 supporting snippets.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset of over 140,000 open-domain factoid question-answer pairs sourced from Jeopardy! clues, each augmented with approximately 50 text snippets retrieved via Google Search.", "tags": ["human", "QA", "compilation", "answer:verifiable", "question:human", "answer:human", "nlp"]}, {"companion": "https://arxiv.org/abs/1705.04146", "date": "2017-05-11", "download": "https://huggingface.co/datasets/deepmind/aqua_rat", "name": "AQuA (Algebra Question Answering with Rationales)", "notes": "Developed by DeepMind, the dataset includes problems sourced from exams like GMAT and GRE, supplemented with crowdsourced questions. Each problem features a question, five answer choices, and a detailed rationale explaining the solution.", "size": "Approximately 100,000 problems.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset of approximately 100,000 algebraic word problems, each with multiple-choice answers and detailed rationales.", "tags": ["crowd-sourced", "QA", "benchmark", "with rationale", "question:crowd-sourced", "answer:crowd-sourced", "rationale:crowd-sourced", "mathematics"]}, {"download": "Various formatted datasets based on historical AIME questions are available, e.g., https://huggingface.co/datasets/di-zhang-fdu/AIME_1983_2024.", "name": "AIME (American Invitational Mathematics Examination)", "notes": "The competition lasts 3 hours. The answer is a number between 0 and 999.", "size": "15 questions per year.", "summary": "Annual US high-school competition, harder than the AMC and easier than the USAMO.", "tags": ["human", "QA", "dialog", "atomic", "benchmark", "answer:human", "answer:verifiable", "question:human", "mathematics"]}, {"companion": "https://arxiv.org/abs/2402.19173", "date": "2024-02-29", "download": "https://huggingface.co/datasets/bigcode/the-stack-v2", "name": "The Stack v2", "notes": "Developed by the BigCode project in collaboration with Software Heritage, this dataset serves as a substantial resource for training and evaluating large language models for code generation and understanding. Access requires an agreement with Software Heritage and INRIA; contact datasets@softwareheritage.org for more information.", "size": "Approximately 5.5 billion samples, totaling around 67.5 terabytes.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA comprehensive dataset of over 3 billion source code files across 600+ programming languages, compiled from the Software Heritage archive as of September 2023.", "tags": ["human", "monolithic", "compilation", "programming"]}, {"companion": "https://arxiv.org/abs/1806.03822", "date": "2018-06-11", "download": "https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json", "name": "SQuAD 2.0", "notes": "Includes unanswerable questions requiring models to predict a special null response. The leaderboard evaluates combined accuracy on answerable and unanswerable questions.", "size": "Approximately 152,500 Q\u0026A pairs (including unanswerable); ~54 MB JSON.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eAn expanded version of SQuAD featuring over 150k questions, including 50k unanswerable ones crafted to resemble answerable questions.", "tags": ["crowd-sourced", "QA", "atomic", "answer:verifiable", "question:crowd-sourced", "answer:human", "nlp"]}, {"companion": "https://arxiv.org/abs/1707.06209", "date": "2017-07-19", "download": "https://allenai.org/data/sciq", "name": "SciQ", "notes": "Questions were created by crowdworkers who, after reviewing a science fact or paragraph, formulated a question along with answer options. Each question includes a correct answer and three distractors, accompanied by a supporting explanation. The dataset is useful for studying the impact of explanations on model performance and is often employed in data augmentation for scientific QA tasks.", "size": "13,679 questions, each with 4 answer choices and a supporting explanation.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset of 13,679 crowd-sourced multiple-choice science questions covering subjects like physics, chemistry, and biology.", "tags": ["crowd-sourced", "QA", "atomic", "with rationale", "question:crowd-sourced", "answer:crowd-sourced", "rationale:crowd-sourced", "physics", "chemistry", "biology"]}, {"companion": "https://arxiv.org/abs/2403.07974", "date": "2024-03-12", "download": "https://github.com/LiveCodeBench/LiveCodeBench", "name": "LiveCodeBench", "notes": "Continuously updated to prevent test set contamination, assessing capabilities beyond code generation, including self-repair, code execution, and test output prediction.", "size": "400 coding problems.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA benchmark of 400 coding problems from LeetCode, AtCoder, and CodeForces, designed to evaluate large language models\u0027 coding capabilities.", "tags": ["human", "QA", "benchmark", "answer:verifiable", "question:human", "answer:human", "programming"]}, {"companion": "https://arxiv.org/abs/2312.14852", "date": "2023-12-22", "download": "https://huggingface.co/datasets/BAAI/TACO", "name": "TACO: Topics in Algorithmic Code Generation", "notes": "Problems are sourced from platforms like CodeChef, CodeForces, HackerRank, GeeksforGeeks, and datasets such as APPS, CodeContest, and Description2code. Each problem includes fine-grained labels like task topics, algorithms, programming skills, and difficulty levels, along with unit tests for solution verification. The dataset emphasizes Python 3 solutions for cleaner code structures.", "size": "26,443 problems with 1,539,152 verified Python 3 solutions.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA large-scale dataset of 26,443 algorithmic programming problems with 1.5 million verified Python 3 solutions, sourced from various platforms.", "tags": ["human", "QA", "compilation", "answer:verifiable", "with rationale", "question:human", "answer:human", "rationale:human", "programming"]}, {"companion": "https://arxiv.org/abs/2109.00110", "date": "2021-08-31", "download": "https://github.com/openai/miniF2F", "name": "MiniF2F", "notes": "MiniF2F includes problems from competitions like AIME, AMC, and IMO, formalized in systems such as Lean, Metamath, Isabelle, and HOL Light. It serves as a unified cross-system benchmark for neural theorem proving.", "size": "488 problems with formal and informal statements.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA benchmark of 488 formalized Olympiad-level mathematics problems for evaluating neural theorem provers.", "tags": ["human", "QA", "benchmark", "answer:verifiable", "question:human", "answer:human", "mathematics"]}, {"companion": "https://arxiv.org/abs/1809.02789", "date": "2018-09-08", "download": "https://leaderboard.allenai.org/open_book_qa/submissions/get-started", "name": "OpenBookQA", "notes": "Includes an \u0027open book\u0027 of 1,326 science facts from elementary textbooks. Answering requires combining these facts with additional commonsense knowledge, testing multi-hop reasoning. Part of the ARC/OpenBookQA leaderboard challenges.", "size": "5,957 questions (4,957 train, 500 dev, 500 test); each with 4 answer choices.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA multiple-choice QA dataset of elementary science questions requiring reasoning with core science facts and common knowledge.", "tags": ["crowd-sourced", "QA", "benchmark", "question:crowd-sourced", "answer:crowd-sourced", "physics"]}, {"companion": "https://arxiv.org/abs/2203.07814", "date": "2022-03-15", "download": "https://huggingface.co/datasets/deepmind/code_contests", "name": "CodeContests", "notes": "Includes both correct and incorrect solutions in various programming languages, providing rich data for training and debugging models.", "size": "Approximately 13,000 problems, totaling around 2 GB.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset of competitive programming problems with test cases and human solutions from platforms like Aizu, AtCoder, CodeChef, Codeforces, and HackerEarth.", "tags": ["crowd-sourced", "QA", "compilation", "benchmark", "question:crowd-sourced", "answer:crowd-sourced", "programming"]}, {"companion": "https://arxiv.org/abs/2405.14333", "date": "2024-05-23", "download": "https://huggingface.co/datasets/deepseek-ai/DeepSeek-Prover-V1", "name": "DeepSeek-Prover-V1", "notes": "Developed to enhance LLM capabilities in formal theorem proving, this dataset includes formal proofs constructed in Lean 4, targeting problems from benchmarks like miniF2F. Fine-tuning DeepSeekMath 7B on this dataset achieved a 50.0% pass rate on the miniF2F test set, surpassing previous methods.", "size": "27,503 samples, 19.6 MB.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA synthetic dataset of 27,503 Lean 4 formal proofs generated by DeepSeek, focusing on high-school and undergraduate-level mathematical competition problems.", "tags": ["synthetic", "monolithic", "atomic", "mathematics"]}, {"date": "2025-03-18", "download": "https://huggingface.co/datasets/glaiveai/reasoning-v1-20m", "name": "GlaiveAI Reasoning v1-20M", "notes": "The dataset includes reasoning traces from DeepSeek-R1 across domains such as mathematics, coding, and science. Notably, it does not include verification of correctness for the reasoning traces, which may impact its applicability in certain contexts.", "size": "Approximately 87 GB.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset of 20 million reasoning traces across various domains, distilled from the DeepSeek-R1 model.", "tags": ["synthetic", "monolithic", "compilation", "mathematics", "programming", "science"]}, {"companion": "https://arxiv.org/abs/2309.05653", "date": "2023-09-11", "download": "https://huggingface.co/datasets/TIGER-Lab/MathInstruct", "name": "MathInstruct", "notes": "Compiled from 13 datasets, including GSM8K, AQuA, and Camel-Math, to train the MAmmoTH models for enhanced mathematical reasoning.", "size": "262,039 samples (~200 MB).", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA curated instruction tuning dataset of 262,039 mathematical problems with chain-of-thought and program-of-thought rationales.", "tags": ["synthetic", "QA", "compilation", "answer:verifiable", "question:synthetic", "answer:synthetic", "mathematics"]}, {"companion": "https://aclanthology.org/Q19-1026/", "date": "2019-06", "download": "https://ai.google.com/research/NaturalQuestions/download", "name": "NQ (Natural Questions)", "notes": "Each example includes a Google search query and a corresponding Wikipedia page. Annotators provide a long answer (a paragraph or table) and, if present, a short answer (one or more entities or a \u0027yes\u0027/\u0027no\u0027 response). The dataset is designed to evaluate open-domain question answering systems and reading comprehension models.", "size": "307,373 training examples; 7,830 development examples; 7,842 test examples.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA benchmark dataset of real anonymized Google search queries paired with Wikipedia articles and human-annotated answers.", "tags": ["human", "QA", "atomic", "answer:verifiable", "question:human", "answer:human", "nlp"]}, {"companion": "https://arxiv.org/abs/2402.03300", "date": "2024-04-27", "download": "Not publicly available.", "name": "DeepSeek Math Corpus", "notes": "The dataset was filtered using a classifier trained on OpenWebMath, with manual annotation and heuristic refinement. It was used to train DeepSeekMath 7B, which scored 51.7% on the MATH benchmark without external tools. The data includes math problems in multiple languages from Common Crawl.", "size": "Approximately 120 billion tokens.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA 120B-token dataset of mathematical content filtered from Common Crawl using a fastText classifier trained on OpenWebMath.", "tags": ["synthetic", "monolithic", "filtration", "mathematics"]}, {"companion": "https://arxiv.org/abs/2411.18872", "date": "2024-11-28", "download": "https://huggingface.co/datasets/roozbeh-yz/IMO-Steps", "name": "IMO-Steps", "notes": "Includes formal proofs for 14 IMO problems from the miniF2F benchmark and 3 additional problems from IMO 2022 and 2023. The dataset also provides a decomposition into 1,329 lemmas, totaling over 40,000 lines of Lean code, serving as building blocks for the formal proofs.", "size": "Approximately 17 formalized IMO problems, totaling around 5,880 lines of Lean code.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset of formal proofs for International Mathematical Olympiad (IMO) problems, formalized in Lean 4.", "tags": ["human", "atomic", "benchmark", "answer:verifiable", "question:human", "answer:human", "mathematics"]}, {"companion": "https://arxiv.org/abs/1905.10044", "date": "2019-05-23", "download": "https://huggingface.co/datasets/boolq", "name": "BoolQ (Boolean Questions)", "notes": "Each example consists of a real user question (from search logs), a relevant Wikipedia passage, and a yes/no answer. Annotators labeled answers based on the passage. Questions often require inference or commonsense reasoning. Included in SuperGLUE.", "size": "15,942 question\u2013passage pairs.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset of 15,942 naturally occurring yes/no questions paired with Wikipedia passages.", "tags": ["human", "QA", "atomic", "benchmark", "answer:verifiable", "question:human", "answer:human", "nlp"]}, {"companion": "https://arxiv.org/abs/2007.00398", "date": "2020-07-01", "download": "https://rrc.cvc.uab.es/?ch=17\u0026com=downloads", "name": "DocVQA", "notes": "The dataset includes diverse document types such as forms, tables, and figures, requiring models to interpret text within complex layouts. A significant performance gap exists between current models and human accuracy, underscoring the need for improved document understanding in VQA systems.", "size": "50,000 questions on 12,767 document images.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset of 50,000 questions on 12,767 document images, designed to evaluate Visual Question Answering models\u0027 ability to understand and extract information from various document types.", "tags": ["human", "QA", "benchmark", "answer:verifiable", "question:human", "answer:human", "nlp"]}, {"companion": "https://arxiv.org/abs/2310.06770", "date": "2023-10-10", "download": "https://github.com/swe-bench/SWE-bench", "name": "SWE-bench", "notes": "Each issue is paired with the codebase state at the time of the issue and the associated pull request that resolved it. Evaluation is performed by unit test verification, with models tasked to generate patches that address the described issues. Subsets include SWE-bench Lite (300 curated instances for accessible evaluation) and SWE-bench Verified (500 human-validated problems confirmed to be solvable).", "size": "2,294 issue-pull request pairs.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA benchmark of 2,294 real-world GitHub issues and corresponding pull requests from 12 popular Python repositories, designed to evaluate language models\u0027 capabilities in resolving software engineering problems.", "tags": ["human", "QA", "compilation", "answer:verifiable", "question:human", "answer:human", "programming"]}, {"companion": "https://arxiv.org/abs/2305.12524", "date": "2023-05-21", "download": "https://huggingface.co/datasets/TIGER-Lab/TheoremQA", "name": "TheoremQA", "notes": "Curated by domain experts, TheoremQA includes a diverse range of theorems such as Taylor\u0027s theorem, Lagrange\u0027s theorem, Huffman coding, and Quantum Theorem. The dataset serves as a rigorous benchmark to assess large language models\u0027 abilities to apply theoretical knowledge to solve complex science problems. Evaluations indicate that models like GPT-4 achieve up to 51% accuracy with Program-of-Thoughts Prompting, while other open-source models perform below 15%, highlighting the dataset\u0027s challenging nature.", "size": "800 questions covering 350+ theorems.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA benchmark dataset of 800 university-level questions requiring the application of over 350 STEM theorems across mathematics, physics, electrical engineering, computer science, and finance.", "tags": ["human", "QA", "benchmark", "answer:verifiable", "question:human", "answer:human", "mathematics", "physics", "programming"]}, {"companion": "https://arxiv.org/abs/1712.07040", "date": "2017-12-19", "download": "https://github.com/deepmind/narrativeqa", "name": "NarrativeQA", "notes": "NarrativeQA presents questions that necessitate reasoning across entire narratives, with answers being free-form summaries rather than exact spans. The dataset includes both summaries and full texts of stories, allowing for tasks that assess comprehension from summaries or full narratives.", "size": "1,567 stories with 46,765 question\u2013answer pairs.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA reading comprehension dataset focusing on long narratives, requiring understanding of entire books or movie scripts to answer questions.", "tags": ["crowd-sourced", "QA", "atomic", "question:crowd-sourced", "answer:crowd-sourced", "nlp"]}, {"companion": "https://arxiv.org/abs/2406.03847", "date": "2024-06-06", "download": "https://huggingface.co/datasets/internlm/Lean-Workbook", "name": "Lean-Workbook", "notes": "Developed using an iterative autoformalization pipeline, translating natural language problems into Lean 4 statements. The dataset includes formal-informal question pairs and new IMO questions.", "size": "Approximately 57,000 problems.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset of approximately 57,000 mathematical problems formalized in Lean 4.", "tags": ["human", "compilation", "mathematics"]}, {"companion": "https://arxiv.org/abs/1811.01241", "date": "2018-12-21", "download": "https://huggingface.co/datasets/wizard_of_wikipedia", "name": "Wizard of Wikipedia", "notes": "The dataset comprises approximately 22,311 dialogues totaling around 201,000 utterances, with each turn by the Wizard annotated with the corresponding Wikipedia sentence used. It facilitates training conversational agents capable of seamlessly integrating factual knowledge into dialogues. Evaluations often focus on knowledge accuracy and conversational fluency.", "size": "Approximately 22,311 dialogues with around 201,000 utterances.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA knowledge-grounded open-domain dialogue dataset where one participant (\u0027Wizard\u0027) utilizes Wikipedia knowledge to inform and engage with another participant (\u0027Apprentice\u0027).", "tags": ["crowd-sourced", "dialog", "tool use", "question:crowd-sourced", "answer:crowd-sourced", "nlp"]}, {"companion": "https://arxiv.org/abs/2310.06786", "date": "2023-10-10", "download": "https://huggingface.co/datasets/open-web-math/open-web-math", "name": "OpenWebMath", "notes": "OpenWebMath was curated by filtering over 200 billion HTML files from Common Crawl to include only English documents with substantial mathematical content. The extraction process preserved LaTeX expressions and removed boilerplate text. The dataset encompasses approximately 14.7 billion tokens and has been utilized for pretraining and fine-tuning large language models, demonstrating enhanced performance on mathematical reasoning tasks.", "size": "6.3 million documents (~27 GB).", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset of 6.3 million high-quality mathematical documents extracted from Common Crawl.", "tags": ["human", "monolithic", "filtration", "mathematics"]}, {"companion": "https://aclanthology.org/P19-1534/", "date": "2019-06-01", "download": "https://github.com/facebookresearch/EmpatheticDialogues", "name": "EmpatheticDialogues", "notes": "Each dialogue features a speaker describing a personal emotional experience, paired with an empathetic response from a listener. The dataset spans 32 emotion categories and supports training and evaluation of models on emotional understanding and response generation.", "size": "24,850 dialogues with 32 distinct emotion labels.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset of 24,850 crowd-sourced one-on-one dialogues grounded in emotional situations, designed to train empathetic conversational models.", "tags": ["crowd-sourced", "dialog", "atomic", "benchmark", "nlp"]}, {"companion": "https://arxiv.org/abs/2312.17120", "date": "2023-12-28", "download": "https://huggingface.co/datasets/GAIR/MathPile", "name": "MathPile", "notes": "Developed by the Generative AI Research Lab (GAIR), MathPile emphasizes data quality over quantity. It includes content from textbooks, arXiv, Wikipedia, ProofWiki, StackExchange, and filtered Common Crawl data. Extensive preprocessing ensures high-quality data, with measures taken to eliminate duplicates from benchmark test sets like MATH and MMLU-STEM.", "size": "Approximately 9.5 billion tokens.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA diverse, high-quality math-centric corpus of approximately 9.5 billion tokens, compiled from sources like textbooks, arXiv papers, and web pages.", "tags": ["human", "compilation", "filtration", "mathematics"]}, {"companion": "https://arxiv.org/abs/2304.08994", "date": "2023-03-31", "download": "https://huggingface.co/datasets/RyokoAI/ShareGPT52K", "name": "ShareGPT Conversations", "notes": "Conversations include user prompts and ChatGPT responses, reflecting real-world interactions. The dataset has been utilized to fine-tune open-source chat models like Vicuna. Legal and terms of service considerations are pertinent, as the data originates from user-submitted logs.", "size": "Approximately 90,000 conversations, each averaging 10\u201315 turns.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA crowdsourced dataset of dialogues between users and ChatGPT, contributed via ShareGPT, encompassing a wide range of topics and user queries.", "tags": ["crowd-sourced", "dialog", "compilation", "question:human", "answer:synthetic", "nlp"]}, {"companion": "https://aclanthology.org/D13-1020/", "date": "2013-10-18", "download": "http://research.microsoft.com/mctest", "name": "MCTest", "notes": "Each story is accompanied by four multiple-choice questions, each with four answer options. The dataset targets elementary-level text understanding and reasoning, with content suitable for readers aged 7. It was influential as a controlled reading comprehension test with broad research usage.", "size": "660 stories, 2,640 questions.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset of 660 crowd-sourced fictional stories with 2,640 multiple-choice questions designed to evaluate machine comprehension.", "tags": ["crowd-sourced", "QA", "atomic", "answer:verifiable", "question:crowd-sourced", "answer:crowd-sourced", "nlp"]}, {"companion": "https://arxiv.org/abs/1809.09600", "date": "2018-09-25", "download": "https://hotpotqa.github.io", "name": "HotpotQA", "notes": "HotpotQA includes sentence-level supporting facts annotations, enabling evaluation of explainability. It features two settings: (1) Distractor: relevant paragraphs mixed with 8 distractors, (2) Fullwiki: requiring a full Wikipedia search. The dataset supports evaluation of multi-hop reasoning and evidence retrieval.", "size": "113,000 question-answer pairs.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset of 113k crowd-sourced questions requiring multi-hop reasoning over multiple Wikipedia paragraphs.", "tags": ["crowd-sourced", "QA", "benchmark", "answer:verifiable", "with rationale", "question:crowd-sourced", "answer:human", "rationale:human", "nlp"]}, {"companion": "https://people.ict.usc.edu/~gordon/publications/Chambers-etal-IJCAI2011.pdf", "date": "2011-03-01", "download": "https://people.ict.usc.edu/~gordon/copa.html", "name": "COPA (Choice of Plausible Alternatives)", "notes": "Each COPA item presents a premise and asks for either a cause or an effect, with two plausible alternatives. The dataset is balanced to avoid superficial cues and has been included in the SuperGLUE benchmark. Human performance is approximately 100%.", "size": "1,000 questions (500 development, 500 test); each with 2 answer options.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset for evaluating commonsense causal reasoning through multiple-choice questions.", "tags": ["human", "QA", "atomic", "question:human", "answer:human", "nlp"]}, {"companion": "https://arxiv.org/abs/2406.17557", "date": "2024-06-17", "download": "https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu", "name": "FineWeb-Edu", "notes": "Filtered using a classifier trained on 450,000 annotated web samples by Llama 3 to emphasize high educational value.", "size": "Approximately 1.3 trillion tokens.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eCurated educational texts extracted from FineWeb using an educational quality classifier.", "tags": ["human", "monolithic", "filtration"]}, {"companion": "https://arxiv.org/abs/2107.03374", "date": "2021-07-07", "download": "https://github.com/openai/human-eval", "name": "HumanEval", "notes": "Each problem includes a function signature, docstring, and unit tests to assess functional correctness. The dataset emphasizes functional correctness over syntactic accuracy, providing a robust benchmark for code generation models. Notably, OpenAI\u0027s Codex achieved a 28.8% pass rate on these problems.", "size": "164 programming problems.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset of 164 hand-crafted Python programming problems designed to evaluate functional correctness in code generation models.", "tags": ["human", "QA", "benchmark", "answer:verifiable", "question:human", "answer:human", "programming"]}, {"companion": "https://arxiv.org/abs/2310.10631", "date": "2023-10-17", "download": "https://huggingface.co/datasets/EleutherAI/proof-pile-2", "name": "Proof-Pile-2", "notes": "Proof-Pile-2 comprises three subsets: 29B tokens from the ArXiv subset of RedPajama, 15B tokens from OpenWebMath containing high-quality mathematical web texts, and 11B tokens from AlgebraicStack, which includes mathematical code across 17 programming languages. This dataset was utilized to train models like Llemma 7B and Llemma 34B, enhancing mathematical reasoning capabilities.", "size": "Approximately 55 billion tokens.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA 55-billion-token dataset of mathematical and scientific documents for training language models.", "tags": ["human", "monolithic", "compilation", "mathematics"]}, {"companion": "https://arxiv.org/abs/2103.03874", "date": "2021-03-05", "download": "https://github.com/hendrycks/math", "name": "AMPS (Auxiliary Mathematics Problems and Solutions)", "notes": "Developed to support mathematical problem-solving model training, AMPS includes problems from Khan Academy covering topics from basic addition to multivariable calculus, and Mathematica-generated problems based on 100 hand-designed modules across various mathematical subjects.", "size": "Over 5 million problems with solutions.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA comprehensive dataset of over 100,000 Khan Academy problems and approximately 5 million Mathematica-generated problems, each with step-by-step solutions.", "tags": ["crowd-sourced", "synthetic", "monolithic", "compilation", "with rationale", "question:crowd-sourced", "question:synthetic", "answer:synthetic", "rationale:synthetic", "mathematics"]}, {"companion": "https://arxiv.org/abs/2502.13124", "date": "2025-02-18", "download": "https://huggingface.co/datasets/facebook/natural_reasoning", "name": "NaturalReasoning", "notes": "Developed by Meta\u0027s FAIR group, NaturalReasoning was constructed by back-translating content from pretraining corpora such as DCLM and FineMath. The dataset was deduplicated and decontaminated from popular reasoning benchmarks, including MATH, GPQA, MMLU-Pro, and MMLU-STEM. Each question includes a reference answer extracted from the original document when available, and a model-generated response from Llama3.3-70B-Instruct. A 1.1 million subset has been released to the research community to foster research on training robust large language model reasoners.", "size": "Approximately 2.8 million questions.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA large-scale dataset comprising 2.8 million challenging reasoning questions across diverse domains, including STEM fields, economics, and social sciences.", "tags": ["synthetic", "QA", "compilation", "answer:verifiable", "question:synthetic", "answer:synthetic", "mathematics", "physics", "economics", "social sciences"]}, {"companion": "https://arxiv.org/abs/2103.03874", "date": "2021-03-05", "download": "https://github.com/hendrycks/math", "name": "MATH (Mathematics Aptitude Test of Heuristics)", "notes": "Problems are sourced from competitions like AMC 10, AMC 12, and AIME, covering topics such as algebra, geometry, number theory, and calculus. Each problem includes a detailed solution, facilitating training and evaluation of mathematical reasoning models.", "size": "12,500 problems with solutions.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset of 12,500 challenging high school competition math problems with step-by-step solutions.", "tags": ["human", "QA", "benchmark", "answer:verifiable", "with rationale", "question:human", "answer:human", "rationale:human", "mathematics"]}, {"companion": "https://aaai.org/ojs/index.php/AAAI/article/view/6399", "date": "2020-04-03", "download": "https://huggingface.co/datasets/allenai/winogrande", "name": "WinoGrande", "notes": "Each problem presents a sentence with an ambiguous pronoun and two possible referents; the task is to choose the correct one. Example: \u0027The trophy didn\u0027t fit in the suitcase because ___ was too big.\u0027 (Options: trophy or suitcase). WinoGrande was created through crowdsourcing with adversarial filtering to reduce biases, making it more challenging than the original Winograd Schema Challenge.", "size": "44,000 problems (train: 40k, dev: 1.2k, test: 1.8k).", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA large-scale dataset of 44,000 pronoun resolution problems designed to test commonsense reasoning.", "tags": ["crowd-sourced", "QA", "atomic", "question:crowd-sourced", "answer:crowd-sourced", "nlp"]}, {"companion": "https://arxiv.org/abs/2309.12284", "date": "2023-09-21", "download": "https://huggingface.co/datasets/meta-math/MetaMathQA", "name": "MetaMathQA", "notes": "Developed to enhance the mathematical reasoning capabilities of large language models, MetaMathQA has been utilized to fine-tune models like MetaMath-7B, which demonstrated significant performance improvements on benchmarks such as GSM8K and MATH.", "size": "Approximately 395,000 samples, totaling around 200 MB.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset of approximately 395,000 mathematical problems with detailed solutions, generated through question bootstrapping from GSM8K and MATH training sets.", "tags": ["synthetic", "QA", "compilation", "answer:verifiable", "question:synthetic", "answer:synthetic", "mathematics"]}, {"companion": "https://arxiv.org/abs/1809.01494", "date": "2018-10-16", "download": "https://sharc-data.github.io/data.html", "name": "ShARC (Shaping Answers with Rules through Conversation)", "notes": "Involves an initial question followed by potential follow-up Q\u0026A turns to clarify conditions before providing a final yes/no/inquire answer. Unique for its conversational logic, requiring systems to infer if the user\u2019s scenario satisfies conditions in the rule text. Dialogues are tree-structured, and annotations include the underlying rule text and user scenario.", "size": "~32,000 QA instances from 948 distinct scenarios.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA conversational QA dataset with ~32,000 instances focusing on understanding rule texts, such as government policies, through multi-turn interactions.", "tags": ["crowd-sourced", "dialog", "atomic", "with rationale", "question:crowd-sourced", "answer:crowd-sourced", "rationale:human", "nlp"]}, {"companion": "https://arxiv.org/abs/2303.04488", "date": "2023-03-08", "download": "https://huggingface.co/datasets/Simontwice/premise_selection_in_isabelle", "name": "Isabelle Premise Selection", "notes": "Includes premises from original proofs and those generated using Isabelle\u0027s Sledgehammer tool, enhancing diversity for training models in premise selection tasks.", "size": "Over 4 million samples.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset of over 4 million proof context and premise pairs from Isabelle\u0027s Archive of Formal Proofs.", "tags": ["human", "compilation", "programming"]}, {"companion": "https://arxiv.org/abs/2502.02737", "date": "2025-02-12", "download": "https://huggingface.co/datasets/HuggingFaceTB/finemath", "name": "FineMath", "notes": "Developed using a Llama-3.1-70B-Instruct based classifier to retain educational math content with clear explanations and step-by-step solutions.", "size": "Approximately 34B tokens (FineMath-3+) and 9.6B tokens (FineMath-4+).", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eCurated mathematical content filtered from CommonCrawl for training language models.", "tags": ["human", "monolithic", "filtration", "mathematics"]}, {"date": "2023-08-06", "download": "https://huggingface.co/datasets/greengerong/leetcode", "name": "LeetCode (greengerong version)", "notes": "The dataset includes problem statements and example solutions but lacks unit tests. The data collection methodology is not specified, and the dataset may not cover the full range of LeetCode problems.", "size": "Approximately 2,000 samples (~7 MB).", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eA dataset of approximately 2,000 programming problems sourced from LeetCode, lacking unit tests and with unspecified collection methods.", "tags": ["human", "QA", "compilation", "answer:verifiable", "question:human", "answer:human", "programming"]}, {"companion": "https://arxiv.org/abs/2504.02807", "date": "2025-04-03", "download": "https://huggingface.co/datasets/LLM360/MegaMath", "name": "MegaMath", "notes": "MegaMath surpasses previous open math pretraining datasets, such as DeepSeekMath, by over 30% in token count. Extensive experiments during development led to optimized practices for text extraction, deduplication, and fastText training, ensuring high data quality. Training language models on MegaMath has demonstrated a 15% to 20% performance boost on ten downstream benchmarks, underscoring its efficacy.", "size": "Approximately 215 million samples totaling 371.6 billion tokens.", "summary": "_\u26a0 Use card with caution_ \u003cbr\u003eAn extensive open math pretraining dataset with over 300 billion tokens, curated to enhance mathematical reasoning in language models.", "tags": ["synthetic", "compilation", "answer:verifiable", "question:synthetic", "answer:synthetic", "mathematics", "programming"]}]</script>

  <script>
    // Markdown parser and data setup.
    let md = new markdownit({ linkify: true, html: true });
    const datasets = JSON.parse(document.getElementById("data").textContent);
    const tagData = JSON.parse('[{"group": "Source", "tags": {"crowd-sourced": "data collected via human crowd-sourcing platforms to train AI models", "human": "data generated by humans without AI in mind", "synthetic": "data generated by machines"}}, {"group": "Type", "tags": {"QA": "question\u2013answer data", "dialog": "data that can be segmented into individual turns (e.g., `user, llm, python-tool, llm, user, llm, user, llm`)", "monolithic": "continuous, unstructured blocks of text useful for pretraining", "tool use": "data illustrating the integration of external tools"}}, {"group": "Collection process", "tags": {"atomic": "data from a single, consistent source (e.g., standardized exam sets)", "benchmark": "datasets originally created for evaluation or benchmarking", "compilation": "a mixture of sources compiled for training purposes", "filtration": "data obtained by filtering a large chunk of the internet"}}, {"group": "QA specific", "tags": {"answer:crowd-sourced": "answers collected via crowd-sourcing", "answer:human": "answers written by humans", "answer:synthetic": "answers generated by machines", "answer:verifiable": "answers can be independently verified (e.g., factual or numerical)", "question:crowd-sourced": "questions collected via crowd-sourcing", "question:human": "questions written by humans", "question:synthetic": "questions generated by machines", "rationale:crowd-sourced": "rationales collected via crowd-sourcing", "rationale:human": "rationales written by humans", "rationale:synthetic": "rationales generated by machines", "with rationale": "answers that include supporting justifications"}}, {"group": "Domain", "tags": {"mathematics": "focused on math problems and reasoning", "nlp": "focused on natural language processing", "physics": "focused on physics content", "programming": "focused on coding and software tasks"}}]');
    let searchTerm = '';
    let selectedTags = [];
    let expandedStates = {};
  
    // Function to attach hover event listeners to tag elements within a given container.
    function attachHoverToTags(container) {
      container.querySelectorAll('.tag').forEach(tag => {
        let tooltipDiv;
        let hoverTimer;
  
        tag.addEventListener('mouseenter', () => {
          const text = tag.getAttribute('data-tooltip');
          if (!text) return;
          hoverTimer = setTimeout(() => {
            tooltipDiv = document.createElement('div');
            tooltipDiv.className = 'custom-tooltip';
            tooltipDiv.innerText = text;
            document.body.appendChild(tooltipDiv);
            const rect = tag.getBoundingClientRect();
            const tooltipRect = tooltipDiv.getBoundingClientRect();
            let top = rect.bottom + window.scrollY + 5;
            let left = rect.left + window.scrollX + (rect.width - tooltipRect.width) / 2;
            if (left + tooltipRect.width > window.innerWidth) {
              left = window.innerWidth - tooltipRect.width - 5;
            }
            if (left < 0) {
              left = 5;
            }
            if (top + tooltipRect.height > window.innerHeight + window.scrollY) {
              top = rect.top + window.scrollY - tooltipRect.height - 5;
            }
            tooltipDiv.style.top = `${top}px`;
            tooltipDiv.style.left = `${left}px`;
          }, 500);
        });
  
        tag.addEventListener('mouseleave', () => {
          clearTimeout(hoverTimer);
          if (tooltipDiv) {
            tooltipDiv.remove();
            tooltipDiv = null;
          }
        });
      });
    }
  
    // Render dataset cards. Note that after rendering the cards,
    // we call attachHoverToTags to bind hover logic to the newly created tag elements.
    function renderDatasets(list) {
      const container = document.getElementById("datasets-container");
      container.innerHTML = "";
      list.forEach(dataset => {
        const summaryHtml = md.renderInline(dataset.summary || "");
        const sizeHtml = md.renderInline(dataset.size || "");
        const dateHtml = md.renderInline(dataset.date || "N/A");
        // Only include full details if present
        let optionalDetails = "";
        if (dataset.download) {
          optionalDetails += `<div class="dataset-detail"><strong>Download Location:</strong> ${md.renderInline(dataset.download)}</div>`;
        }
        if (dataset.companion) {
          optionalDetails += `<div class="dataset-detail"><strong>Companion Paper:</strong> ${md.renderInline(dataset.companion)}</div>`;
        }
        if (dataset.notes) {
          optionalDetails += `<div class="dataset-detail">${md.renderInline(dataset.notes)}</div>`;
        }
        // Check the global expanded state for this dataset.
        const isExpanded = expandedStates[dataset.name];
        const elem = document.createElement("div");
        elem.className = "dataset";
        elem.innerHTML = `
          <div class="dataset-minimal">
            <div class="dataset-title">${dataset.name}</div>
            <div class="dataset-detail">${summaryHtml}</div>
            <div class="dataset-detail"><strong>Size:</strong> ${sizeHtml}</div>
            <div class="dataset-detail"><strong>Date Created:</strong> ${dateHtml}</div>
          </div>
          <div class="dataset-full" style="display: ${isExpanded ? 'block' : 'none'};">
            ${optionalDetails}
            <div class="dataset-detail"><strong>Tags:</strong> ${renderDatasetTags(dataset.tags)}</div>
          </div>
        `;
        elem.addEventListener("click", () => {
          const details = elem.querySelector(".dataset-full");
          if (expandedStates[dataset.name]) {
            expandedStates[dataset.name] = false;
            details.style.display = "none";
          } else {
            expandedStates[dataset.name] = true;
            details.style.display = "block";
          }
        });
        container.appendChild(elem);
  
        // Ensure links inside the card donâ€™t trigger the parent click event.
        const links = elem.querySelectorAll("a");
        links.forEach(link => {
          link.setAttribute("target", "_blank");
          link.setAttribute("rel", "noopener noreferrer");
          link.setAttribute("onclick", "event.stopPropagation()");
        });
      });
      // Re-attach hover handlers to tag elements in the newly rendered datasets.
      attachHoverToTags(container);
    }
  
    // Render dataset tags (inside each dataset card).
    function renderDatasetTags(tags) {
      if (!tags) return "";
      return tags.map(tag => {
        let tooltip = "";
        for (let group of tagData) {
          if (group.tags && group.tags[tag]) {
            tooltip = group.tags[tag];
            break;
          }
        }
        return `<span class="tag" data-tooltip="${tooltip}">${tag}</span>`;
      }).join(" ");
    }
  
    // Update view based on search and tag filters.
    const updateView = () => {
      const filtered = datasets.filter(dataset => {
        const matchesSearch = !searchTerm ||
          (dataset.name && dataset.name.toLowerCase().includes(searchTerm.toLowerCase())) ||
          (dataset.summary && dataset.summary.toLowerCase().includes(searchTerm.toLowerCase())) ||
          (dataset.notes && dataset.notes.toLowerCase().includes(searchTerm.toLowerCase()));
        const matchesTags = selectedTags.length === 0 ||
          (Array.isArray(dataset.tags) && selectedTags.every(tag => dataset.tags.includes(tag)));
        return matchesSearch && matchesTags;
      });
      renderDatasets(filtered);
    };
  
    // Sort datasets.
    function updateOrder() {
      const sortOption = document.getElementById("sort-select").value;
      datasets.sort((a, b) => {
        if (sortOption === "name") return a.name.localeCompare(b.name);
        const dateA = new Date(a.date || "1970-01-01");
        const dateB = new Date(b.date || "1970-01-01");
        if (sortOption === "date_latest") return dateB - dateA;
        if (sortOption === "date_earliest") return dateA - dateB;
      });
      updateView();
    }
  
    // Render the top-level tag filters.
    function renderTags() {
      const container = document.getElementById("tags-container");
      container.innerHTML = "";
      tagData.forEach(groupObj => {
        const groupContainer = document.createElement("div");
        groupContainer.className = "tag-group-line";
        const header = document.createElement("span");
        header.className = "tag-group-header";
        header.textContent = `${groupObj.group}: `;
        groupContainer.appendChild(header);
        Object.keys(groupObj.tags).forEach(tag => {
          const tagElem = document.createElement("span");
          tagElem.className = "tag";
          tagElem.textContent = tag;
          tagElem.setAttribute("data-tooltip", groupObj.tags[tag]);
          tagElem.style.cursor = "pointer";
          tagElem.addEventListener("click", e => {
            e.stopPropagation();
            tagElem.classList.toggle("selected");
            const tagText = tagElem.textContent;
            selectedTags = tagElem.classList.contains("selected")
              ? [...selectedTags, tagText]
              : selectedTags.filter(t => t !== tagText);
            updateView();
          });
          groupContainer.appendChild(tagElem);
          groupContainer.appendChild(document.createTextNode(" "));
        });
        container.appendChild(groupContainer);
      });
      // Attach hover handlers for the top-level tags.
      attachHoverToTags(container);
    }
  
    // Event listeners for search and sorting.
    document.getElementById("search-input").addEventListener("input", e => {
      searchTerm = e.target.value;
      updateView();
    });
    document.getElementById("sort-select").addEventListener("change", updateOrder);
    document.getElementById("help-button").addEventListener("click", () => {
      const helpText = document.getElementById("help-text");
      helpText.style.display = helpText.style.display === "none" ? "block" : "none";
    });
    document.getElementById("theme-toggle-button").addEventListener("click", () => {
      document.body.classList.toggle("dark-mode");
      const button = document.getElementById("theme-toggle-button");
      button.textContent = document.body.classList.contains("dark-mode") ? "â˜€ï¸" : "ðŸŒ™";
    });
  
    // Initialization.
    renderTags();
    updateOrder();
  </script>
</body>
</html>