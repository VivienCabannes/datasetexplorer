<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>Dataset Explorer</title>
  <link rel="stylesheet" type="text/css" href="static/style.css">

  <!-- Use Katex to render math if needed -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.css" integrity="sha384-zh0CIslj+VczCZtlzBcjt5ppRcsAmDnRem7ESsYwWwg3m/OaJ2l4x7YBZl9Kxxib" crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.js" integrity="sha384-Rma6DA2IPUwhNxmrB/7S3Tno0YY7sFu9WSYMCuulLhIqYSGZ2gKCJWIqhBWqMQfh" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/contrib/auto-render.min.js" integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh" crossorigin="anonymous"
      onload="renderMathInElement(document.body);"></script>
</head>
<body>
  <div class="header">
    <h1>Dataset Explorer</h1>
    <div class="top-buttons">
      <button id="help-button" class="round-button" title="click for help">?</button>
      <button id="theme-toggle-button" class="round-button" title="toggle theme">üåô</button>
    </div>
  </div>
  
  <!-- Help text block, initially hidden -->
  <div id="help-text" style="display:none; margin: 10px 0;">
    <p>
      ‚ö†Ô∏è <strong>Under Construction:</strong> the dataset card needs to be completed.<br/><br/>

      <strong>Welcome to the Dataset Explorer!</strong><br/>
      Use the filters to narrow your dataset choices, type in the search bar to find datasets by name or summary, and sort them as needed. 
      Click on a dataset to see further details.
    </p>
  </div>
  
  <!-- Tags rendered below the help button -->
  <div id="tags-container">
    <!-- Tags will be dynamically inserted here -->
  </div>
  
  <!-- Controls row: Search bar (left) & Order menu (right) -->
  <div class="controls-row">
    <div class="search-bar">
      <input type="text" id="search-input" placeholder="Search datasets...">
    </div>
    <div class="sort-bar">
      <label for="sort-select">Sort by:</label>
      <select id="sort-select">
        <option value="name">Name (A-Z)</option>
        <option value="date_latest">Date (Latest)</option>
        <option value="date_earliest">Date (Earliest)</option>
      </select>
    </div>
  </div>
  
  <!-- Container for displaying datasets -->
  <div id="datasets-container">
    <!-- Matching datasets will be displayed here -->
  </div>

  <!-- Render mardown -->
  <script id="markdown-it" src="static/markdownit.js"></script>
  <script id="data" type="application/json">[{"name": "Various Websites", "summary": "- The art of problem solving (https://artofproblemsolving.com/): a website that prepare students to various STEM competition, and is often used by researchers to craft datasets with rationale. - ProofWiki (https://proofwiki.org/):  a website that aim at collecting math proofs. - StackExchange - Wikipedia # Semi-Native Sources Data that was built by leveraging existing assets with substantial data scrapping work."}, {"companion": "https://arxiv.org/abs/2310.10631", "download": "https://huggingface.co/datasets/EleutherAI/proof-pile-2", "name": "Proof-Pile-2", "size": "60B tokens,", "summary": "Massive pretraining corpus of formal mathematics and related documents (Lean, Coq, math papers)."}, {"companion": "https://arxiv.org/abs/2411.18872", "download": "https://huggingface.co/datasets/roozbeh-yz/IMO-Steps", "name": "IMO-Steps", "size": "20 samples, 6 kB", "summary": "DOWNLOADED IN `/checkpoint/amaia/explore/datasets/reasoning/raw` 20 Lean proofs of IMO problems"}, {"companion": "https://arxiv.org/abs/2502.02737", "download": "https://huggingface.co/datasets/HuggingFaceTB/stack-edu", "name": "Stack-Edu", "notes": "Need to be download with S3, still really big in terms of number of tokens.", "size": "167,000,000 samples, ~1 TB", "summary": "Filtering version of the Stack V2. # LLM Augmented Source Data being synthesized with LLMs. The LLM can be used to synthesize texts or questions (usually providing example of questions from existing datasets). It can also be used to synthesize a rationale to answer questions. The answer may be verified, either with parser checking for numerical equality (i.e. `\\frac13` = `0.333`), or using LLM as a judge. Synthesized questions (bootstrapped from existing one), synthesized, eventually verified, answers."}, {"companion": "none, but the datasets was key to https://arxiv.org/abs/2303.04488", "download": "https://huggingface.co/datasets/Simontwice/premise_selection_in_isabelle", "name": "Isabelle Premise Selection", "size": "4,000,000 samples", "summary": "Datasets of proofs in Isabelle collected from the Archive of Formal Proofs (https://www.isa-afp.org/). Useful to study premise selection (i.e. selecting potential lemmas to apply mid-proof)."}, {"download": "https://huggingface.co/datasets/open-r1/OpenR1-Math-220k", "name": "Open-R1-220k", "notes": "They have other datasets, see https://huggingface.co/open-r1", "size": "225,000 samples, 8 GB.", "summary": "This part of an ongoing effort by HuggingFace to reproduce DeepSeek R1. Consists of DeepSeek R1 traces answering problems from NuminaMath, verified with Math Verify, a HuggingFace parser to check numerical equality."}, {"companion": "https://arxiv.org/abs/2410.07985", "download": "https://huggingface.co/datasets/KbsdJames/Omni-MATH", "name": "OmniMath", "size": "4,000, 7 MB.", "summary": "Data obtained from regional to international Olympiads, from the art-of-problem solving."}, {"companion": "https://arxiv.org/abs/2406.17557", "download": "https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu", "name": "FineWeb-Edu", "summary": "Filtered from FineWeb to focus on sample with educational value"}, {"companion": "https://arxiv.org/abs/2310.06786", "download": "https://huggingface.co/datasets/open-web-math/open-web-math", "name": "OpenWebMath", "size": "6,300,000 samples, 27 GB.", "summary": "Filtering of CommonCrawl done in 2023, that is considered good quality. Not donwloaded, as FineMath is more recent."}, {"companion": "https://arxiv.org/abs/2502.13124", "download": "https://huggingface.co/datasets/facebook/natural_reasoning", "name": "Natural Reasoning", "summary": "From Fair RAM group, questions filtered from DCLM and FineMath, with answers provided by Llama."}, {"companion": "https://arxiv.org/abs/2103.03874", "download": "https://github.com/hendrycks/math (was taken down from HuggingFace due to copyright issue filled by the art of problem solving).", "name": "MATH", "size": "12,500 samples", "summary": "Classical evaluation datasets, created by Dan Hendrycks, by collecting various US high-school competition problems with solution."}, {"companion": "https://arxiv.org/pdf/2309.05653", "download": "https://huggingface.co/datasets/TIGER-Lab/MathInstruct", "name": "MathInstruct", "size": "262,000 samples, 200 MB", "summary": "A compilation of datasets (gsm8k, aqua,camel) with rationale (some of them being generated by LLMs) used to train MAmmoTH"}, {"download": "https://huggingface.co/datasets/EleutherAI/proof-pile-2 contains data scrapped from the ArXiv website, according to a snapshot taken in 2023 by RedPajama.", "name": "ArXiv", "notes": "The dataset is valuable for training models on advanced mathematical concepts and research-level problems.", "size": "29B tokens", "summary": ""}, {"companion": "https://arxiv.org/abs/2410.01560", "download": "https://huggingface.co/datasets/nvidia/OpenMathInstruct-2", "name": "OpenMathInstruct-v2", "size": "22,000,000 samples, 12 GB", "summary": "Augmentation of MATH and GSM8K from LLM (rephrase questions, provide answers)."}, {"name": "Stack V2", "size": "5,500,000,000 samples, 67 TB", "summary": "Scrapping of the Software Heritage archive, which contains software source code, in September 2023."}, {"companion": "https://arxiv.org/pdf/1705.04146", "download": "https://huggingface.co/datasets/deepmind/aqua_rat", "name": "AQuA (Algebra Question Answering)", "size": "98,000 samples; 52 MB", "summary": "DeepMind Dataset built by extracting 34,000 questions from undergrad, and grad student admission test (GMAT and GRE), with answer and rational scrapped on the web. Plus crowdsourcing to provide similar questions."}, {"companion": "https://arxiv.org/abs/2405.14333", "download": "https://huggingface.co/datasets/deepseek-ai/DeepSeek-Prover-V1", "name": "DeepSeek-Prover-V1", "size": "27,000 samples, 6 MB", "summary": "Synthetic dataset of Lean proofs generated by DeepSeek, solving half of miniF2F."}, {"companion": "https://arxiv.org/abs/2310.06770", "download": "https://github.com/swe-bench/SWE-bench", "name": "SWE-bench", "size": "2,300 samples", "summary": "Datasets of codebase, issues and unit tests. The goal is to fix the codebase that currently yield the issue resulting in failing unit tests. The data was collected from popular Github repository."}, {"download": "Various formatted datasets based on historical AMC questions are available, e.g. https://huggingface.co/datasets/AI-MO/aimo-validation-amc/", "name": "AMC", "size": "3 (levels) times 25 multi-choice questions per year.", "summary": "American Math competition for various high-school students, which acts as a pre-selection to AIME."}, {"companion": "https://arxiv.org/abs/2412.09413", "download": "https://huggingface.co/datasets/RUC-AIBOX/long_form_thought_data_5k", "name": "Still long format", "size": "5,000 samples, 20 MB.", "summary": "Similar to Still, with a focus on long answer, questions come from NuminaMath, Aime, Leetcode, OpenCoder, Camel, Gaokao (Chinese A-level) and RiddleSense. # Formal Math"}, {"companion": "https://arxiv.org/abs/2502.00203", "download": "https://huggingface.co/datasets/nvidia/Llama-Nemotron-Post-Training-Dataset-v1", "name": "Nemotron Post-Training Dataset v1", "notes": "Developed to fine-tune models for improved accuracy and reliability.", "size": "15,000,000 samples, 6 GB.", "summary": "We do not have too many details on this datasets, it seems to have been curated from Llama, Qwen and DeepSeek answers."}, {"companion": "https://arxiv.org/abs/1905.09381", "download": "https://github.com/princeton-vl/CoqGym", "name": "CoqGym", "size": "70,000 proof steps", "summary": "Large-scale dataset compiled from various 71,000 Coq projects."}, {"companion": "https://arxiv.org/abs/2312.14852", "download": "https://huggingface.co/datasets/BAAI/TACO", "name": "TACO", "size": "26,000 problems with 1.5M solutions, .", "summary": "Algorithmic problems collected from various platforms such as CodeChef, CodeForces, HackerRank, and GeeksforGeeks, as well as existing datasets such as APPS, CodeContest, and Description2code. Each problems come with unit tests that allows to test for correctness. # Filtered Sources Data that come from filtering a bigger dataset."}, {"companion": "https://arxiv.org/abs/2203.07814", "download": "https://huggingface.co/datasets/deepmind/code_contests", "name": "CodeContests", "notes": "Includes correct and incorrect solutions. Rich data for training/debugging models.", "size": "13,000 samples, 2GB", "summary": "Dataset used for AlphaCode. Competitive programming problems from Aizu, AtCoder, CodeChef, Codeforces and HackerEarth."}, {"companion": "https://arxiv.org/abs/2310.10631", "download": "https://huggingface.co/datasets/EleutherAI/proof-pile-2", "name": "Algebraic Stack", "size": "3,000,000, 11MB.", "summary": "Subset of ProofPile-2. Obtained by filtering GitHub for Coq, Isabelle, Lean and Matlab, extracting data from Mathlib 4 (the Lean library), building a dataset of Isabelle proofs, and filtering the Stack."}, {"companion": "https://arxiv.org/abs/2109.00110", "download": "https://github.com/facebookresearch/miniF2F", "name": "MiniF2F", "size": "500 examples with Lean formal statements and informal statements and proofs.", "summary": "MiniF2F is a benchmark for formal mathematics, created by Kunhao, consisting of 500 Olympiad-level mathematics problems from competitions like AIME, AMC, and IMO. Each problem is provided with both informal and formal statements. # Compilation"}, {"companion": "https://arxiv.org/abs/2309.12284", "download": "https://huggingface.co/datasets/meta-math/MetaMathQA", "name": "MetaMathQA", "notes": "OpenMathInstruct is a more recent iteration of the same idea.", "size": "400,000 samples, 200 MB.", "summary": "Datasets collected by \u0027bootstrapping\u0027 gsm8k and Math"}, {"download": "https://huggingface.co/datasets/PRIME-RL/Eurus-2-RL-Data", "name": "Eurus-RL", "notes": "Includes multiple datasets with corresponding training recipes.", "size": "500,000 samples, 2 GB", "summary": "Collection of question with verifiable answer extracted from Numina, Apps, CodeContests, Taco and Codeforces. https://arxiv.org/abs/2502.01456"}, {"companion": "https://arxiv.org/abs/2210.17517", "download": "https://huggingface.co/datasets/allenai/lila", "name": "Lila", "notes": "The datasets were collected for evaluation, it seems small and outdated.", "summary": "Compilation of many datasets including addsub, amps, apps, asdiv, conala, mathematics, dolphin, draw, gsm8k, math, mathqa, mbpp, mctaco, multiarith, numersense, numglus, simuleq, singleop, singleq, svamp."}, {"name": "DeepScaleR", "summary": "Compiled from Aime, AMC, Omni-Math and Still Compilation of data from AIME, AMC, Omni-Math, and Still. Used to reproduce R1."}, {"companion": "https://arxiv.org/abs/2305.20050", "download": "https://github.com/openai/prm800k", "name": "PRM800K", "notes": "Some annotations were reported to be incorrect", "size": "800,000 samples,", "summary": "Datasets created by OpenAI by using LLMs to answer question from the MATH datasets, with rationale graded by humnas."}, {"companion": "https://arxiv.org/abs/2110.14168", "download": "https://github.com/openai/grade-school-math", "name": "GSM8k", "size": "8,500 problems, 5 MB", "summary": "Grade School Math (GSM) benchmark, created by human annotator for OpenAI."}, {"download": "Various formatted datasets based on historical AIME questions are available, e.g., [on HuggingFace](https://huggingface.co/datasets/di-zhang-fdu/AIME_1983_2024)", "name": "AIME", "notes": "Since AIME is an annual examination, new data becomes available each year. However, designers may select questions already present on the internet, which could be included in pretraining corpora.", "size": "15 questions per year", "summary": "US high-school competition, American Invitational Mathematics Examination.", "tags": ["atomic", "benchmark", "QA", "question:human", "answer:human"]}, {"companion": "https://arxiv.org/abs/2108.07732", "download": "https://github.com/google-research/google-research/tree/master/mbpp", "name": "MBPP (Mostly Basic Python Problems)", "size": "1,000 samples", "summary": "Crowd-sourced datasets of small Python problems A dataset consisting of 1,000 Python programming problems aimed at entry-level programmers."}, {"download": "https://huggingface.co/datasets/AI-MO/NuminaMath-1.5", "name": "NuminaMath", "size": "Approximately 900,000 samples, 531 MB.", "summary": "Mix of math problems solved with rationale. Sources: aops_forum (https://artofproblemsolving.com/), amc_aime, Chinese k12, gsm8k, math, Olympiads, Orca-math (https://arxiv.org/abs/2402.14830), synthetic_amc, synthetic_math"}, {"companion": "https://arxiv.org/abs/2411.18872", "download": "https://huggingface.co/datasets/internlm/Lean-Workbook", "name": "Lean-Workbook", "size": "25,000 samples, 5MB", "summary": "DOWNLOADED IN `/checkpoint/amaia/explore/datasets/reasoning/raw` Tens of thousands of math problems formalized in Lean4"}, {"companion": "https://arxiv.org/abs/2303.17760", "name": "Camel", "notes": "Comes with no verification of correctness", "summary": "Exercise textbooks synthetically generated by GPT-4"}, {"companion": "https://arxiv.org/pdf/2402.14008", "download": "https://huggingface.co/datasets/Hothan/OlympiadBench", "name": "OlympiadBench", "size": "8,000 questions.", "summary": "Datasets collected from Olympiads with figures (multi-modal), rationale (derived by humans), various level of difficulty."}, {"companion": "https://arxiv.org/abs/2412.09413", "download": "https://huggingface.co/datasets/RUC-AIBOX/STILL-3-Preview-RL-Data", "name": "Still", "size": "30,000 samples, 10 MB", "summary": "Collection of question and verifiable answer extracted from Math, Numina, and Aime. # Additional notes (for additional assets) MIT-8 benchmark Proof-Pile - ArXiv.math (10GB) - Open-source math textbooks (50MB) - Formal mathematics libraries (500MB) - Lean mathlib and other Lean repositories - Isabelle AFP - Coq mathematical components and other Coq repositories - HOL Light - set.mm - Mizar Mathematical Library - Math Overflow and Math Stack Exchange (2.5GB) - Wiki-style sources (50MB) - ProofWiki - Wikipedia math articles - MATH dataset (6MB) https://huggingface.co/datasets/HuggingFaceTB/cosmopedia https://huggingface.co/datasets/open-web-math/open-web-math https://github.com/LiveCodeBench/LiveCodeBench https://huggingface.co/datasets/LLM360/MegaMath https://huggingface.co/datasets/math-ai/AutoMathText https://arxiv.org/abs/2402.07625 https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k AMPS (Khan + Mathematica) Dataset OCR: https://arxiv.org/pdf/2502.18443"}, {"download": "https://huggingface.co/datasets/glaiveai/reasoning-v1-20m", "name": "GlaiveAI", "notes": "Comes with no verification of correctness", "size": "20 million examples, 87 GB", "summary": "Traces from DeepSeek R1"}, {"companion": "https://arxiv.org/abs/2312.17120", "download": "https://huggingface.co/datasets/GAIR/MathPile", "name": "MathPile", "size": "Approximately 9.5 billion tokens.", "summary": "Dataset collected by GAIR (the lab behind LIMO) by compiling textbook, arXiv, ProofWiki, and filtering common crawl."}, {"companion": "https://arxiv.org/pdf/2105.09938", "download": "https://huggingface.co/datasets/codeparrot/apps", "name": "APPS", "notes": "AlphaCode mentions that test coverage was insufficient, leading to false positive.", "size": "10,000 problems", "summary": "10,000 programming problems with python solutions and test cases for correctness.  Curated from Codewars, AtCoder, Kattis, and Codeforces."}, {"download": "https://huggingface.co/datasets/greengerong/leetcode", "name": "LeetCode", "notes": "Unclear how the dataset was collected, comes with no unit tests", "size": "2,000 samples, 7 MB (for the small version I found on HuggingFace)", "summary": "Website with programming puzzles, typically used to prepare coding interviews."}, {"companion": "https://arxiv.org/abs/2502.02737", "download": "https://huggingface.co/datasets/HuggingFaceTB/finemath", "name": "FineMath", "summary": "Filtered from CommonCrawl by HuggingFace for SmolLM focused on Math domain."}, {"companion": "https://arxiv.org/abs/2406.11794", "download": "https://huggingface.co/datasets/mlfoundations/dclm-baseline-1.0", "name": "DataComp-LM (DCLM)", "size": "4T token", "summary": "Filtering of Common Crawl based on heuristic cleaning and filtering (see RefinedWeb), deduplication (through Bloom), filtering with fastText classifier to match the reddit channel ExplainLikeImFive, and an instruct model."}, {"companion": "https://arxiv.org/pdf/2402.03300", "name": "DeepSeek Math Corpus", "summary": "Filtering of Common Crawl based on a FastText classifier based on OpenWebMath as initial positive examples, and additional heuristics. Datasets not available."}, {"companion": "https://arxiv.org/abs/2406.17557", "download": "https://huggingface.co/datasets/HuggingFaceFW/fineweb", "name": "FineWeb", "summary": "Filtered from CommonCrawl by HuggingFace, seems to be slightly worse than DCLM."}, {"companion": "https://arxiv.org/pdf/2309.17452", "download": "https://huggingface.co/datasets/AI-MO/NuminaMath-TIR", "name": "Numina-Tool", "size": "70,000 samples, 150 MB", "summary": "Subset numina problems solved with tool-use."}, {"download": "https://huggingface.co/datasets/MatrixStudio/Codeforces-Python-Submissions", "name": "CodeForces", "notes": "Quite extensive datasets with unit test", "size": "700,000 samples, 1.7 GB", "summary": "Website with competitive programming puzzles."}]</script>

  <script>
    // Markdown parser
    let md = new markdownit({linkify: true,});

    // Data from flask server
    const datasets = JSON.parse(document.getElementById("data").textContent);
    const tagData = JSON.parse('[{"group": "Source", "tags": {"crowd-sourced": "data collected via human crowd-sourcing platforms to train AI models", "human": "data generated by humans without AI in mind", "synthetic": "data generated by machines"}}, {"group": "Type", "tags": {"QA": "question\u2013answer data", "dialog": "data that can be segmented into individual turns (e.g., `user, llm, python-tool, llm, user, llm, user, llm`)", "monolithic": "continuous, unstructured blocks of text useful for pretraining", "tool use": "data illustrating the integration of external tools"}}, {"group": "Collection process", "tags": {"atomic": "data sourced uniformly from a single, consistent origin (e.g., standardized exam sets)", "benchmark": "datasets originally curated for benchmarking purposes", "compilation": "data mix that was used for training purposes", "filtration": "data obtained by filtering to a large chunk of the internet"}}, {"group": "QA specific", "tags": {"answer:crowd-sourced": "answers collected via crowd-sourcing", "answer:human": "answers generated by humans", "answer:synthetic": "answers generated by machineis", "answer:verifiable": "answers can be verified (e.g. numerical, factual)", "question:crowd-sourced": "questions collected via crowd-sourcing", "question:human": "questions generated by humans", "question:synthetic": "questions generated by machineis", "rationale:crowd-sourced": "rationales collected via crowd-sourcing", "rationale:human": "rationales generated by humans", "rationale:synthetic": "rationales generated by machineis", "with rationale": "answers including justifications"}}, {"group": "Domain", "tags": {"mathematics": "data centered on mathematics", "nlp": "natural language processing data", "physics": "data centered on physics", "programming": "data centered on programming"}}]');
    let searchTerm = '';
    let selectedTags = [];

    // Render datasets with markdown and LaTeX support
    function renderDatasets(list) {
      const container = document.getElementById("datasets-container");
      container.innerHTML = "";
      list.forEach(dataset => {
        const summaryHtml = md.renderInline(dataset.summary || "");
        const sizeHtml = md.renderInline(dataset.size || "");
        const dateHtml = md.renderInline(dataset.date || "N/A");

        // Only include full details if present
        let optionalDetails = "";
        if (dataset.download) {
          optionalDetails += `<div class="dataset-detail"><strong>Download Location:</strong> ${md.renderInline(dataset.download)}</div>`;
        }
        if (dataset.companion) {
          optionalDetails += `<div class="dataset-detail"><strong>Companion Paper:</strong> ${md.renderInline(dataset.companion)}</div>`;
        }
        if (dataset.notes) {
          optionalDetails += `<div class="dataset-detail"> ${md.renderInline(dataset.notes)}</div>`;
        }

        const elem = document.createElement("div");
        elem.className = "dataset";
        elem.innerHTML = `
          <div class="dataset-minimal">
            <div class="dataset-title">${dataset.name}</div>
            <div class="dataset-detail">${summaryHtml}</div>
            <div class="dataset-detail"><strong>Size:</strong> ${sizeHtml}</div>
            <div class="dataset-detail"><strong>Date Created:</strong> ${dateHtml}</div>
          </div>
          <div class="dataset-full" style="display: none;">
            ${optionalDetails}
            <div class="dataset-detail"><strong>Tags:</strong> ${renderDatasetTags(dataset.tags)}</div>
          </div>
        `;
        // Toggle details on click.
        elem.addEventListener("click", () => {
          const details = elem.querySelector(".dataset-full");
          details.style.display = details.style.display === "block" ? "none" : "block";
        });
        container.appendChild(elem);

        // Add hyperlink logic
        const links = elem.querySelectorAll("a");
        links.forEach(link => {
          link.setAttribute("target", "_blank");
          link.setAttribute("rel", "noopener noreferrer");
          link.setAttribute("onclick", "event.stopPropagation()");
        });
      });
    }

    // Filter datasets based on search term and selected tags.
    const updateView = () => {
      const filtered = datasets.filter(dataset => {
        const matchesSearch = !searchTerm ||
          (dataset.name && dataset.name.toLowerCase().includes(searchTerm.toLowerCase())) ||
          (dataset.summary && dataset.summary.toLowerCase().includes(searchTerm.toLowerCase())) ||
          (dataset.notes && dataset.notes.toLowerCase().includes(searchTerm.toLowerCase()));
        const matchesTags = selectedTags.length === 0 ||
          (Array.isArray(dataset.tags) && selectedTags.every(tag => dataset.tags.includes(tag)));
        return matchesSearch && matchesTags;
      });
      renderDatasets(filtered);
    };

    // Sort datasets
    function updateOrder() {
      const sortOption = document.getElementById("sort-select").value;
      datasets.sort((a, b) => {
        if (sortOption === "name") return a.name.localeCompare(b.name);
        if (sortOption === "date_latest") return new Date(b.date) - new Date(a.date);
        if (sortOption === "date_earliest") return new Date(a.date) - new Date(b.date);
      });
      updateView();
    }

    // Render and handle tag buttons with tooltips.
    function renderTags() {
      const container = document.getElementById("tags-container");
      container.innerHTML = "";
      // iterate over the array to preserve order.
      tagData.forEach(groupObj => {
        const groupContainer = document.createElement("div");
        groupContainer.className = "tag-group-line";
        const header = document.createElement("span");
        header.className = "tag-group-header";
        header.textContent = `${groupObj.group}: `;
        groupContainer.appendChild(header);
        // iterate over tags within the group.
        Object.keys(groupObj.tags).forEach(tag => {
          const tagElem = document.createElement("span");
          tagElem.className = "tag";
          tagElem.textContent = tag;
          tagElem.setAttribute("data-tooltip", groupObj.tags[tag]);
          tagElem.style.cursor = "pointer";
          tagElem.addEventListener("click", e => {
            e.stopPropagation();
            tagElem.classList.toggle("selected");
            const tagText = tagElem.textContent;
            selectedTags = tagElem.classList.contains("selected")
              ? [...selectedTags, tagText]
              : selectedTags.filter(t => t !== tagText);
            updateView();
          });
          groupContainer.appendChild(tagElem);
          groupContainer.appendChild(document.createTextNode(" "));
        });
        container.appendChild(groupContainer);
      });
    }

    // Render tags for a dataset's tag list.
    function renderDatasetTags(tags) {
      if (!tags) return "";
      return tags.map(tag => {
        let tooltip = "";
        for (let group in tagData) {
          if (tagData[group][tag]) {
            tooltip = tagData[group][tag];
            break;
          }
        }
        return `<span class="tag" data-tooltip="${tooltip}">${tag}</span>`;
      }).join(" ");
    }

    // Event listeners for search and sorting.
    document.getElementById("search-input").addEventListener("input", e => {
      searchTerm = e.target.value;
      updateView();
    });
    document.getElementById("sort-select").addEventListener("change", updateOrder);

    // help button toggles the display of help text.
    document.getElementById("help-button").addEventListener("click", () => {
      const helpText = document.getElementById("help-text");
      helpText.style.display = helpText.style.display === "none" ? "block" : "none";
    });

    // theme toggle button: toggles dark mode and updates its icon.
    document.getElementById("theme-toggle-button").addEventListener("click", () => {
      document.body.classList.toggle("dark-mode");
      const button = document.getElementById("theme-toggle-button");
      if(document.body.classList.contains("dark-mode")){
        button.textContent = "‚òÄÔ∏è";
      } else {
        button.textContent = "üåô";
      }
    });

    // Initialiation
    updateOrder();
    updateView();
    renderTags();

    // Add hovering logic over tags
    document.querySelectorAll('.tag').forEach(tag => {
      let tooltipDiv;
      let hoverTimer; // Timer for the delay

      tag.addEventListener('mouseenter', () => {
        const text = tag.getAttribute('data-tooltip');
        if (!text) return;

        // Start the timer for 1 second delay
        hoverTimer = setTimeout(() => {
          tooltipDiv = document.createElement('div');
          tooltipDiv.className = 'custom-tooltip';
          tooltipDiv.innerText = text;
          document.body.appendChild(tooltipDiv);

          // Position the tooltip
          const rect = tag.getBoundingClientRect();
          const tooltipRect = tooltipDiv.getBoundingClientRect();

          let top = rect.bottom + window.scrollY + 5;
          let left = rect.left + window.scrollX + (rect.width - tooltipRect.width) / 2;

          if (left + tooltipRect.width > window.innerWidth) {
            left = window.innerWidth - tooltipRect.width - 5;
          }
          if (left < 0) {
            left = 5;
          }
          if (top + tooltipRect.height > window.innerHeight + window.scrollY) {
            top = rect.top + window.scrollY - tooltipRect.height - 5;
          }

          tooltipDiv.style.top = `${top}px`;
          tooltipDiv.style.left = `${left}px`;
        }, 500); // 1 second delay
      });

      tag.addEventListener('mouseleave', () => {
        // Clear the timer if the user leaves early
        clearTimeout(hoverTimer);
        if (tooltipDiv) {
          tooltipDiv.remove();
          tooltipDiv = null;
        }
      });
    });

  </script>
</body>
</html>